# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰çˆ¬è™« Â· ä¸‰èŒ…æ—¥æŠ¥ä¸“æŠ“ç‰ˆï¼ˆ24å°æ—¶ + å…³é”®è¯ç™½åå• + å¼ºåŠ›å»å™ª + åˆå¹¶å»é‡ï¼‰
ä»…æŠ“å–æ ‡é¢˜ä¸­åŒ…å«ã€ä¸‰èŒ…æ—¥æŠ¥ã€‘çš„æ–°é—»ï¼Œå¿½ç•¥å…¶å®ƒæ‰€æœ‰èµ„è®¯ã€‚
"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ====== æ—¶åŒº ======
try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo

# ========= å°å·¥å…· =========
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]
def now_tz(): return datetime.now(ZoneInfo("Asia/Shanghai"))
def within_24h(dt): return (now_tz() - dt).total_seconds() <= 86400 if dt else False

# ========= é’‰é’‰ =========
def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    h = hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(h))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def _mask(v: str, head=6, tail=6):
    if not v: return ""
    if len(v) <= head + tail: return v
    return v[:head] + "..." + v[-tail:]

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASE")      # å¿…å¡«ï¼šæ— åŠ ç­¾ä¹Ÿå¯
    secret = os.getenv("DINGTALK_SECRET")  # é€‰å¡«ï¼šå¼€å¯â€œåŠ ç­¾â€æ‰éœ€è¦
    if not base:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASEï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    webhook = _sign_webhook(base, secret)
    try:
        r = requests.post(
            webhook,
            json={"msgtype": "markdown", "markdown": {"title": title, "text": md}},
            timeout=20,
        )
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} base={_mask(base)} http={r.status_code}")
        if not ok: print("DingTalk resp:", r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

# ========= ç½‘ç»œ =========
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/123.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    })
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

# ========= ä¸»çˆ¬è™« =========
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = 15
        self.detail_timeout = (6, 20)
        self.detail_sleep = 0.6

        # â€”â€” åªå…è®¸â€œä¸‰èŒ…æ—¥æŠ¥â€çš„æ ‡é¢˜ â€”â€” #
        # æ”¯æŒï¼šä¸‰èŒ…æ—¥æŠ¥ï½œä¸‰èŒ…æ—¥æŠ¥ | ä¸‰èŒ…æ—¥å ±ï¼ˆç¹ä½“ï¼‰ç­‰
        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")

        # â€”â€” å¼ºåŠ›å»å™ªè¯ â€”â€” #
        self.noise_words = [
            "æ‰‹æœº","çŸ­ä¿¡","éªŒè¯ç ","è¯ˆéª—","ä¸¾æŠ¥","è¿è¥å•†","é»‘åå•","å®‰å…¨",
            "å®¢æœ","å……å€¼","å¯†ç ","å°å·","ä¿¡å·","æ³¨é”€","æ³¨å†Œ","è´¦å·",
            "å¹¿å‘Š","ä¸‹è½½","æ‰«ç ","äºŒç»´ç ","å…³æ³¨","è½¬å‘","æŠ½å¥–","ç¦åˆ©",
            "ç›´æ’­","è§†é¢‘","è¯„è®º","ç‚¹èµ","ç§ä¿¡","ç¤¼åŒ…","ä¼˜æƒ åˆ¸"
        ]

        # â€”â€” è¦ç‚¹ç™½åå• â€”â€” #
        self.keep_words = [
            "å¯¹è±¡","é€‚ç”¨","èŒƒå›´","åŸå¸‚","åœ°åŒº","åœ°åŸŸ","æˆ·ç±","å¹´é¾„","èº«ä»½","æ¡ä»¶","èµ„æ ¼",
            "é‡‘é¢","è¡¥è´´","æ ‡å‡†","æ¯”ä¾‹","ä¸Šé™","ä¸‹é™","é¢åº¦","äº«å—","å¾…é‡",
            "ææ–™","è¯æ˜","æ‰€éœ€","æäº¤","å‡†å¤‡","æ¸…å•",
            "æµç¨‹","æ­¥éª¤","æ–¹å¼","æ¸ é“","å…¥å£","å¹³å°","åŠç†","ç”³é¢†","ç”³è¯·","ç™»è®°","æ³¨å†Œ",
            "æ—¶é—´","æœŸé™","æˆªè‡³","èµ·æ­¢","æ‰§è¡Œæ—¶é—´",
            "ä¾æ®","æ”¿ç­–","æ–‡ä»¶","é€šçŸ¥","æ¡æ¬¾","è§£è¯»",
            "å’¨è¯¢","ç”µè¯","çª—å£","åœ°ç‚¹","åœ°å€"
        ]

    # â€”â€” å¯¹å¤–å…¥å£ â€”â€” #
    def crawl(self):
        base = "https://www.hrloo.com/"
        r = self.session.get(base, timeout=20)
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥", r.status_code); return
        soup = BeautifulSoup(r.text, "html.parser")

        # åªæ”¶é›†â€œé“¾æ¥æ–‡æœ¬é‡Œå°±åŒ…å«â€˜ä¸‰èŒ…æ—¥æŠ¥â€™â€çš„ a æ ‡ç­¾ï¼Œå°½é‡å‡å°‘æ— æ•ˆè¯·æ±‚
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href", "")
            text = norm(a.get_text())
            if not re.search(r"/news/\d+\.html$", href):
                continue
            if self.daily_title_pat.search(text or ""):
                links.append(urljoin(base, href))

        # åŒä¿é™©ï¼šå¦‚æœé¦–é¡µ a æ–‡æœ¬æœªåŒ…å«â€œä¸‰èŒ…æ—¥æŠ¥â€ï¼Œä¹Ÿå…è®¸å°‘é‡å›é€€æŠ“å–ååœ¨è¯¦æƒ…é¡µå†è¿‡æ»¤
        if not links:
            links = [urljoin(base, a.get("href"))
                     for a in soup.select("a[href*='/news/']")
                     if re.search(r"/news/\d+\.html$", a.get("href",""))]

        seen = set()
        for url in links:
            if url in seen: 
                continue
            seen.add(url)

            pub_dt, subtitles, main_title = self._fetch_detail_clean(url)
            # â€”â€” è¯¦æƒ…é¡µå†åšä¸€æ¬¡â€œå¿…é¡»æ˜¯ä¸‰èŒ…æ—¥æŠ¥â€çš„ç¡¬è¿‡æ»¤ â€”â€” #
            if not main_title or not self.daily_title_pat.search(main_title):
                continue
            if not pub_dt or not within_24h(pub_dt):
                continue
            if not subtitles:
                continue

            self.results.append({
                "title": main_title,
                "url": url,
                "date": pub_dt.strftime("%Y-%m-%d %H:%M"),
                "titles": subtitles
            })
            print(f"[OK] {url} {pub_dt} è¦ç‚¹{len(subtitles)}ä¸ª")
            if len(self.results) >= self.max_items: 
                break
            time.sleep(self.detail_sleep)

    # â€”â€” æ˜ç»†é¡µæŠ½å– + æ¸…æ´— â€”â€” #
    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=self.detail_timeout)
            if r.status_code != 200: return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            # ä¸»æ ‡é¢˜
            h = soup.find(["h1","h2"])
            page_title = norm(h.get_text()) if h else ""

            # å‘å¸ƒæ—¶é—´
            pub_dt = self._extract_pub_time(soup)

            # å–â€œç¼–å·æ®µè½â€ï¼ˆ1. / 1ã€ / ä¸€ã€ ç­‰ï¼‰
            raw = []
            for t in soup.find_all(["strong","h2","h3","span","p","li"]):
                text = norm(t.get_text())
                if not text: 
                    continue
                if not re.match(r"^([ï¼ˆ(]?\d+[)ï¼‰]|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€|\d+\s*[ã€.ï¼])\s*.+", text):
                    continue
                raw.append(text)

            clean = self._clean_subtitles(raw)
            return pub_dt, clean, page_title
        except Exception as e:
            print("[DetailError]", url, e)
            return None, [], ""

    def _extract_pub_time(self, soup):
        tz = ZoneInfo("Asia/Shanghai")
        txt = soup.get_text(" ")
        m = re.search(r"(20\d{2})[-/.å¹´](\d{1,2})[-/.æœˆ](\d{1,2})(?:\s+(\d{1,2}):(\d{1,2}))?", txt)
        if not m: return None
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        hh = int(m[4]) if m[4] else 9
        mm = int(m[5]) if m[5] else 0
        try:
            return datetime(y, mo, d, hh, mm, tzinfo=tz)
        except:
            return None

    # â€”â€” åªä¿ç•™â€œæœ‰ç”¨ä¿¡æ¯â€çš„æ¸…æ´—å™¨ â€”â€” #
    def _clean_subtitles(self, items):
        out, seen = [], set()
        for t in items:
            # å»ç¼–å·
            t = re.sub(r"^([ï¼ˆ(]?\d+[)ï¼‰]|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€|\d+\s*[ã€.ï¼])\s*", "", t)
            t = norm(t)

            # é•¿åº¦é˜ˆå€¼
            if len(t) < 6 or len(t) > 50:
                continue

            # ä¸­æ–‡å æ¯”
            zh_ratio = len(re.findall(r"[\u4e00-\u9fa5]", t)) / max(len(t),1)
            if zh_ratio < 0.5:
                continue

            # å™ªå£°è¯è¿‡æ»¤
            if any(w in t for w in self.noise_words):
                continue

            # â€œè¦ç‚¹ç™½åå•â€è¿‡æ»¤
            if not any(k in t for k in self.keep_words):
                continue

            # å½’ä¸€åŒ–åŒä¹‰è¯
            t = re.sub(r"(é‡‘é¢|è¡¥è´´|æ ‡å‡†|é¢åº¦|æ¯”ä¾‹)", "é‡‘é¢/æ ‡å‡†", t)
            t = re.sub(r"(æ¡ä»¶|èµ„æ ¼)", "ç”³é¢†æ¡ä»¶", t)
            t = re.sub(r"(ææ–™|è¯æ˜|æ‰€éœ€)", "æ‰€éœ€ææ–™", t)
            t = re.sub(r"(æµç¨‹|æ­¥éª¤|åŠç†|ç”³è¯·|ç”³é¢†|ç™»è®°|æ¸ é“|å…¥å£|å¹³å°)", "åŠç†æµç¨‹/å…¥å£", t)
            t = re.sub(r"(æ—¶é—´|æœŸé™|æˆªè‡³|èµ·æ­¢)", "åŠç†æ—¶é—´", t)
            t = re.sub(r"(å¯¹è±¡|é€‚ç”¨|èŒƒå›´|åŸå¸‚|åœ°åŒº|åœ°åŸŸ|æˆ·ç±|èº«ä»½|å¹´é¾„)", "é€‚ç”¨å¯¹è±¡/èŒƒå›´", t)
            t = re.sub(r"(ä¾æ®|æ”¿ç­–|æ–‡ä»¶|é€šçŸ¥|æ¡æ¬¾)", "æ”¿ç­–ä¾æ®", t)

            # å»æœ«å°¾æ ‡ç‚¹
            t = re.sub(r"[ï¼Œã€‚ï¼›ã€,.]+$", "", t)

            # å»é‡
            key = t.lower().replace(" ", "")
            if key in seen: 
                continue
            seen.add(key)
            out.append(t)

            if len(out) >= 8:
                break
        return out

# ========= Markdown è¾“å‡º =========
def build_md(items):
    now = now_tz()
    out = []
    out.append(f"**æ—¥æœŸï¼š{now.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(now)}ï¼‰**  ")
    out.append("")
    out.append("**æ ‡é¢˜ï¼šæ¯æ—¥èµ„è®¯ï½œäººåŠ›èµ„æºæ¯æ—¥è¦ç‚¹**  ")
    out.append("")
    out.append("**ä¸»è¦å†…å®¹**  ")
    out.append("")

    if not items:
        out.append("> 24å°æ—¶å†…æœªå‘ç°æ–°çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚")
        return "\n".join(out)

    for i, it in enumerate(items, 1):
        out.append(f"{i}. [{it['title']}]({it['url']}) ï¼ˆ{it['date']}ï¼‰  ")
        for s in it['titles']:
            out.append(f"> ğŸŸ¦ {s}  ")
        out.append("")
    return "\n".join(out)

# ========= ä¸»å…¥å£ =========
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler_daily_only.pyï¼ˆä»…æŠ“â€œä¸‰èŒ…æ—¥æŠ¥â€ï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("æ¯æ—¥èµ„è®¯ï½œäººåŠ›èµ„æºæ¯æ—¥è¦ç‚¹", md)
