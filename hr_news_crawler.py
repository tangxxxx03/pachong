# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰ä¸“æŠ“ç‰ˆ Â· ä»…æå–å°æ ‡é¢˜ Â· ä»…æŠ“â€œæ˜¨å¤©â€çš„æ–‡ç« 
- è¿›å…¥æ¯ç¯‡ /news/xxxx.html è¯¦æƒ…é¡µï¼ŒåªæŠ½å–åˆ†èŠ‚æ ‡é¢˜ï¼ˆ1ã€2ã€3â€¦ï¼‰ï¼Œä¸æŠ“æ­£æ–‡/æ‘˜è¦
- è‡ªåŠ¨è¯†åˆ«å‘å¸ƒæ—¶é—´ï¼Œ**ä»…ä¿ç•™â€œæ˜¨å¤©â€çš„æ–‡ç« **ï¼ˆä»¥ Asia/Shanghai ä¸ºå‡†ï¼‰
- æ”¯æŒé’‰é’‰ Markdown æ¨é€ï¼ˆè¯»å– DINGTALK_BASEA / DINGTALK_SECRETA æˆ– DINGTALK_BASE / DINGTALK_SECRETï¼‰
"""

import os
import re
import time
import hmac
import ssl
import base64
import hashlib
import urllib.parse
import requests
from bs4 import BeautifulSoup, Comment
from urllib.parse import urljoin
from datetime import datetime, timedelta
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

try:
    from zoneinfo import ZoneInfo
except Exception:
    from backports.zoneinfo import ZoneInfo


# =============== åŸºç¡€å·¥å…· ===============
def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def zh_weekday(dt: datetime) -> str:
    return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]

def now_tz() -> datetime:
    return datetime.now(ZoneInfo(os.getenv("HR_TZ", "Asia/Shanghai")))

def is_yesterday(dt: datetime) -> bool:
    if not dt:
        return False
    y = (now_tz() - timedelta(days=1)).date()
    return dt.date() == y


# =============== é’‰é’‰ ===============
def _sign_webhook(base: str, secret: str) -> str:
    if not base or not secret:
        return ""
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    base = (os.getenv("DINGTALK_BASEA") or os.getenv("DINGTALK_BASE") or "").strip()
    secret = (os.getenv("DINGTALK_SECRETA") or os.getenv("DINGTALK_SECRET") or "").strip()
    webhook = _sign_webhook(base, secret)
    if not webhook:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    r = requests.post(webhook, json=payload, timeout=20)
    ok = (r.status_code == 200 and r.json().get("errcode") == 0)
    print("DingTalk resp:", r.status_code, r.text[:200])
    return ok


# =============== ç½‘ç»œè¯·æ±‚ ===============
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *args, **kwargs):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kwargs["ssl_context"] = ctx
        return super().init_poolmanager(*args, **kwargs)

def make_session() -> requests.Session:
    s = requests.Session()
    s.trust_env = False
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    })
    retries = Retry(total=3, backoff_factor=0.8, status_forcelist=(429, 500, 502, 503, 504))
    s.mount("http://", HTTPAdapter(max_retries=retries))
    s.mount("https://", LegacyTLSAdapter(max_retries=retries))
    return s


# =============== çˆ¬è™« ===============
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = int(os.getenv("HR_MAX_ITEMS", "15"))
        self.detail_timeout = (6.0, 20.0)
        self.detail_sleep = float(os.getenv("HR_DETAIL_SLEEP", "0.6"))

    def crawl(self):
        base = "https://www.hrloo.com/"
        self._crawl_list(base)

    def _crawl_list(self, base_url: str):
        print("[List] æŠ“å–é¦–é¡µï¼š", base_url)
        r = self.session.get(base_url, timeout=20)
        if r.status_code != 200:
            print("åˆ—è¡¨è¯·æ±‚å¤±è´¥ï¼š", r.status_code)
            return
        r.encoding = r.apparent_encoding or "utf-8"
        soup = BeautifulSoup(r.text, "html.parser")

        # æŠ“å–æ‰€æœ‰å¯èƒ½çš„æ–°é—»é“¾æ¥
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href") or ""
            if re.search(r"/news/\d+\.html$", href):
                links.append(urljoin(base_url, href))

        seen = set()
        for url in links:
            if url in seen:
                continue
            seen.add(url)
            # è¯¦æƒ…æŠ“å–ï¼šåªè¦â€œæ˜¨å¤©â€çš„æ–‡ç«  + æå–å°æ ‡é¢˜
            pub_dt, subtitles, title = self._fetch_detail_yesterday_and_titles(url)
            if not pub_dt:
                continue
            if not is_yesterday(pub_dt):
                continue
            if not subtitles:
                continue

            self.results.append({
                "title": title or url,
                "url": url,
                "source": "ä¸‰èŒ…äººåŠ›èµ„æºç½‘",
                "date": pub_dt.strftime("%Y-%m-%d %H:%M"),
                "subtitles": subtitles,
            })
            print(f"[Keep] {url} {pub_dt} å°æ ‡é¢˜{len(subtitles)}æ¡")
            if len(self.results) >= self.max_items:
                break
            time.sleep(self.detail_sleep)

    # â€”â€” è¯¦æƒ…é¡µï¼šè§£æå‘å¸ƒæ—¶é—´ï¼ˆdatetimeï¼‰+ åªæå–åˆ†èŠ‚å°æ ‡é¢˜ï¼ˆä¸æŠ“æ­£æ–‡ï¼‰
    def _fetch_detail_yesterday_and_titles(self, url: str):
        try:
            r = self.session.get(url, timeout=self.detail_timeout)
            if r.status_code != 200:
                return None, [], ""
            r.encoding = r.apparent_encoding or r.encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            # æ¸…ç†æ— ç”¨èŠ‚ç‚¹
            for tag in soup(["script","style","noscript","iframe","footer","header","nav","form"]):
                tag.decompose()
            for c in soup.find_all(string=lambda t: isinstance(t, Comment)):
                c.extract()

            # æ ‡é¢˜ï¼ˆé¡µé¢ä¸»æ ‡é¢˜ï¼‰
            title_tag = soup.find(["h1","h2"], limit=1)
            page_title = norm(title_tag.get_text()) if title_tag else ""

            # å‘å¸ƒæ—¶é—´ï¼šæ­£æ–‡ meta/ä¿¡æ¯åŒºã€æˆ–å…¨é¡µæ–‡æœ¬ä¸­æå–
            pub_dt = self._extract_pub_datetime(soup)

            # å°æ ‡é¢˜ï¼šstrong/h2/h3/span.bjh-pï¼ŒåŒ¹é…å½¢å¦‚â€œ1ã€xx / 1.xx / 1ï¼xxâ€
            subtitles = []
            candidates_parent = self._find_content_container(soup)
            for tag in candidates_parent.find_all(["strong","h2","h3","span","p"]):
                # é™å®š span ç±»åï¼ˆæ ¹æ®ä½ æˆªå›¾ï¼‰ä¹Ÿå¯èƒ½å« bjh-p
                if tag.name == "span":
                    cls = " ".join((tag.get("class") or []))
                    if cls and "bjh-p" not in cls:
                        # ä¸æ˜¯æ­£æ–‡å°æ ‡é¢˜ç±»ï¼Œè·³è¿‡ï¼ˆä»å…è®¸ strong/h2/h3/pï¼‰
                        pass
                text = norm(tag.get_text())
                if re.match(r"^\d+\s*[ã€.ï¼]\s*.+", text):
                    if text not in subtitles:
                        subtitles.append(text)

            return pub_dt, subtitles, page_title
        except Exception as e:
            print("[DetailError]", url, e)
            return None, [], ""

    def _find_content_container(self, soup: BeautifulSoup):
        # ä¼˜å…ˆæ­£æ–‡å®¹å™¨
        for css in [
            ".article-content", ".news-content", ".content", ".article_box",
            ".neirong", ".main-content", ".entry-content", ".post-content", "#article", "#content"
        ]:
            node = soup.select_one(css)
            if node and norm(node.get_text()):
                return node
        return soup

    def _extract_pub_datetime(self, soup: BeautifulSoup) -> datetime | None:
        """
        ä»è¯¦æƒ…é¡µæå–å‘å¸ƒæ—¶é—´ï¼›ä¼˜å…ˆåœ¨â€œæ—¶é—´/ä½œè€…/é˜…è¯»â€ç­‰ä¿¡æ¯åŒºæŸ¥æ‰¾ï¼›
        å…œåº•åœ¨å…¨é¡µæ–‡æœ¬é‡Œç”¨æ­£åˆ™åŒ¹é…ã€‚
        """
        tz = ZoneInfo(os.getenv("HR_TZ", "Asia/Shanghai"))
        text_candidates = []

        # ä¿¡æ¯åŒºå¸¸è§é€‰æ‹©å™¨
        for css in [".meta", ".info", ".news-info", ".article-info", ".time", ".date", ".post-meta"]:
            node = soup.select_one(css)
            if node:
                text_candidates.append(node.get_text(" "))

        # æ ‡é¢˜é™„è¿‘çš„å…„å¼ŸèŠ‚ç‚¹
        h = soup.find(["h1","h2"])
        if h and h.parent:
            text_candidates.append(h.parent.get_text(" "))

        # å…¨é¡µå…œåº•
        text_candidates.append(soup.get_text(" "))

        # æ­£åˆ™æ¨¡å¼ï¼ˆå¹´-æœˆ-æ—¥ å¯å¸¦æ—¶é—´ï¼›å¹´/æœˆ/æ—¥ï¼›ä¸­æ–‡æ—¥æœŸï¼‰
        patterns = [
            r"(20\d{2})[-/.å¹´](\d{1,2})[-/.æœˆ](\d{1,2})(?:\s+(\d{1,2}):(\d{1,2}))?",
        ]

        for raw in text_candidates:
            raw = norm(raw)
            for pat in patterns:
                m = re.search(pat, raw)
                if m:
                    y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
                    hh = int(m.group(4)) if m.group(4) else 9
                    mm = int(m.group(5)) if m.group(5) else 0
                    try:
                        return datetime(y, mo, d, hh, mm, tzinfo=tz)
                    except ValueError:
                        continue
        return None


# =============== Markdown è¾“å‡ºï¼ˆåªæ˜¾ç¤ºå°æ ‡é¢˜ï¼‰ ===============
def build_markdown(items: list[dict]) -> str:
    now = now_tz()
    lines = [
        f"**æ—¥æœŸï¼š{now.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(now)}ï¼‰**",
        "",
        "**æ ‡é¢˜ï¼šæ—©å®‰èµ„è®¯ï½œäººåŠ›èµ„æºæ¯æ—¥èµ„è®¯æ¨é€ï¼ˆä»…æ˜¨æ—¥ï¼‰**",
        "",
        "**ä¸»è¦å†…å®¹**",
    ]
    if not items:
        lines.append("> æ˜¨æ—¥æ— åŒ¹é…å†…å®¹ã€‚")
        return "\n".join(lines)

    for i, it in enumerate(items, 1):
        lines.append(f"{i}. [{it['title'] or it['url']}]({it['url']})ã€€â€”ã€€*{it['source']}*ï¼ˆ{it['date']}ï¼‰")
        for st in it.get("subtitles", []):
            lines.append(f"> ğŸŸ¦ {st}")
        lines.append("")
    return "\n".join(lines)


# =============== å…¥å£ ===============
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler.pyï¼ˆä»…æŠ“æ˜¨å¤©ï¼†åªæ‘˜å°æ ‡é¢˜ï¼‰")
    crawler = HRLooCrawler()
    crawler.crawl()
    md = build_markdown(crawler.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("æ—©å®‰èµ„è®¯ï½œä¸‰èŒ…æ˜¨æ—¥å°æ ‡é¢˜", md)
