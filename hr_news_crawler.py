# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰Â· ä¸‰èŒ…æ—¥æŠ¥ä¸“æŠ“ç‰ˆ
åŠŸèƒ½ï¼š
  1) ä»…æŠ“å–æ ‡é¢˜åŒ…å«ã€Œä¸‰èŒ…æ—¥æŠ¥/ä¸‰èŒ…æ—¥å ±ã€çš„æ–°é—»ï¼›
  2) ä»æ­£æ–‡ä¸­æŠ½å–â€œ1ã€â€¦ 2ã€â€¦â€ç­‰ç¼–å·æ¡ç›®çš„ã€æ ‡é¢˜ã€‘ï¼›
  3) ä»…ä¿ç•™å½“å¤©ï¼ˆHR_ONLY_TODAY=1ï¼‰æˆ–è¿‘24hï¼ˆé»˜è®¤ï¼‰ï¼›
  4) ç”Ÿæˆ Markdownï¼Œå¹¶æ¨é€åˆ°é’‰é’‰æœºå™¨äººï¼ˆå¯é€‰ï¼‰ã€‚

ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰ï¼š
  HR_TZ=Asia/Shanghai
  HR_ONLY_TODAY=1/0   # 1=åªè¦å½“å¤©ï¼›0=è¿‘24å°æ—¶ï¼ˆé»˜è®¤ï¼‰
  HR_MAX_ITEMS=15
  SRC_HRLOO_URLS=https://www.hrloo.com/   # å¯æ‰©å±•å¤šä¸ªï¼Œä»¥é€—å·åˆ†éš”
  DINGTALK_BASE / DINGTALK_SECRET         # æ ‡å‡†å˜é‡å
  DINGTALK_BASEA / DINGTALK_SECRETA       # å…¼å®¹ä½ çš„ Actions é…ç½®
"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta, date
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ====== æ—¶åŒºä¸æ—¶é—´ ======
try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo  # ä»…æ—§ç‰ˆ Python éœ€è¦

def tz():
    return ZoneInfo(os.getenv("HR_TZ", "Asia/Shanghai"))

def now_tz():
    return datetime.now(tz())

def norm(s): 
    return re.sub(r"\s+", " ", (s or "").strip())

def zh_weekday(dt):
    return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]

def within_24h(dt):
    if not dt: return False
    return (now_tz() - dt).total_seconds() <= 86400

def same_day(dt):
    if not dt: return False
    n = now_tz().date()
    return dt.astimezone(tz()).date() == n

# ====== é’‰é’‰æ¨é€ ======
def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    h = hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(h))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def _mask(v: str, head=6, tail=6):
    if not v: return ""
    if len(v) <= head + tail: return v
    return v[:head] + "..." + v[-tail:]

def send_dingtalk_markdown(title, md):
    # å…¼å®¹ä¸¤ç§å˜é‡å
    base = os.getenv("DINGTALK_BASE") or os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRET") or os.getenv("DINGTALK_SECRETA")
    if not base:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASE/BASEAï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    webhook = _sign_webhook(base, secret)
    try:
        r = requests.post(
            webhook,
            json={"msgtype": "markdown", "markdown": {"title": title, "text": md}},
            timeout=20,
        )
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} base={_mask(base)} http={r.status_code}")
        if not ok:
            print("DingTalk resp:", r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

# ====== ç½‘ç»œä¼šè¯ ======
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/123.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    })
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

# ====== çˆ¬è™«ä¸»ä½“ ======
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = int(os.getenv("HR_MAX_ITEMS", "15") or "15")
        self.detail_timeout = (6, 20)
        self.detail_sleep = 0.6

        # æ ‡é¢˜å¿…é¡»åŒ…å«â€œä¸‰èŒ…æ—¥æŠ¥/ä¸‰èŒ…æ—¥å ±â€
        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")

        # æŠ“å–å…¥å£ï¼ˆå¯é€—å·åˆ†éš”å¤šä¸ªï¼‰
        src = os.getenv("SRC_HRLOO_URLS", "https://www.hrloo.com/").strip()
        self.sources = [u.strip() for u in src.split(",") if u.strip()]

        # æ—¶é—´ç­–ç•¥ï¼šå½“å¤© or è¿‘24h
        self.only_today = (os.getenv("HR_ONLY_TODAY", "0") == "1")

    def crawl(self):
        for base in self.sources:
            try:
                self._crawl_source(base)
            except Exception as e:
                print(f"[SourceError] {base} -> {e}")

    def _crawl_source(self, base):
        r = self.session.get(base, timeout=20)
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥", base, r.status_code); 
            return
        soup = BeautifulSoup(r.text, "html.parser")

        # ä¼˜å…ˆï¼ša æ–‡æœ¬é‡Œå°±åŒ…å«â€œä¸‰èŒ…æ—¥æŠ¥â€
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href", "")
            text = norm(a.get_text())
            if not re.search(r"/news/\d+\.html$", href):
                continue
            if self.daily_title_pat.search(text or ""):
                links.append(urljoin(base, href))

        # å…œåº•ï¼šå¦‚æœé¦–é¡µ a æ–‡æœ¬æ²¡æœ‰æ˜ç¡®åŒ…å«ï¼Œé€€å›åˆ°å…¨éƒ¨ news é“¾æ¥ï¼Œå»è¯¦æƒ…é¡µäºŒæ¬¡åˆ¤å®š
        if not links:
            links = [urljoin(base, a.get("href"))
                     for a in soup.select("a[href*='/news/']")
                     if re.search(r"/news/\d+\.html$", a.get("href",""))]

        seen = set()
        for url in links:
            if url in seen:
                continue
            seen.add(url)

            pub_dt, item_titles, main_title = self._fetch_detail_clean(url)

            # å¿…é¡»æ˜¯â€œä¸‰èŒ…æ—¥æŠ¥â€
            if not main_title or not self.daily_title_pat.search(main_title):
                continue

            # æ—¶é—´è¿‡æ»¤
            if self.only_today:
                if not same_day(pub_dt):
                    continue
            else:
                if not within_24h(pub_dt):
                    continue

            if not item_titles:
                continue

            self.results.append({
                "title": norm(main_title),
                "url": url,
                "date": pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else "",
                "titles": item_titles
            })
            print(f"[OK] {url} {pub_dt} æ¡ç›®{len(item_titles)}ä¸ª")
            if len(self.results) >= self.max_items:
                break
            time.sleep(self.detail_sleep)

    # â€”â€” æ˜ç»†é¡µæŠ½å–ï¼ˆæ—¥æŠ¥æ ‡é¢˜ + å‘å¸ƒæ—¶é—´ + æ¡ç›®æ ‡é¢˜åˆ—è¡¨ï¼‰ â€”â€” #
    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=self.detail_timeout)
            if r.status_code != 200:
                return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            # ä¸»æ ‡é¢˜
            h = soup.find(["h1","h2"])
            page_title = norm(h.get_text()) if h else ""

            # å‘å¸ƒæ—¶é—´
            pub_dt = self._extract_pub_time(soup)

            # æŠ½â€œç¼–å·æ¡ç›®â€çš„æ ‡é¢˜
            item_titles = self._extract_daily_item_titles(soup)

            return pub_dt, item_titles, page_title
        except Exception as e:
            print("[DetailError]", url, e)
            return None, [], ""

    def _extract_pub_time(self, soup):
        txt = soup.get_text(" ")
        # åŒ¹é…ï¼š2025-10-29 08:53 æˆ– 2025å¹´10æœˆ29æ—¥ 08:53
        m = re.search(r"(20\d{2})[-/.å¹´](\d{1,2})[-/.æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?", txt)
        if not m: 
            return None
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        hh = int(m[4]) if m[4] else 9
        mm = int(m[5]) if m[5] else 0
        try:
            return datetime(y, mo, d, hh, mm, tzinfo=tz())
        except:
            return None

    # â€”â€” ä»æ—¥æŠ¥æ­£æ–‡æå–â€œ1ã€â€¦ 2ã€â€¦â€çš„æ¡ç›®æ ‡é¢˜ â€”â€” #
    def _extract_daily_item_titles(self, soup):
        items = []
        # å¸¸è§å®¹å™¨æ ‡ç­¾é‡Œæ‰¾æ–‡æœ¬
        for t in soup.find_all(["h2","h3","h4","strong","b","p","li","span","div"]):
            raw = (t.get_text() or "").strip()
            if not raw:
                continue
            # å…è®¸çš„ç¼–å·æ ·å¼ï¼š1ã€xxx / 1. xxx / 1ï¼xxx / ï¼ˆ1ï¼‰xxx / (1) xxx
            m = re.match(r"^\s*(?:ï¼ˆ?\(?\s*\d+\s*\)?ï¼‰?)\s*[ã€.ï¼]?\s*(.+)", raw)
            if not m:
                continue
            title = m.group(1).strip()

            # åˆ‡æ‰å¯èƒ½è·Ÿç€çš„è§£é‡Š/å†’å·åç¼€ç­‰ï¼Œåªç•™â€œæ ‡é¢˜çŸ­è¯­â€
            title = re.split(r"[ï¼š:ã€‚]|ï¼ˆ|ï¼ˆå›¾|ï¼ˆè¯¦è§|â€”|--|-{2,}", title)[0].strip()

            # è¿‡æ»¤å™ªå£°ä¸å¼‚å¸¸é•¿åº¦
            if not (4 <= len(title) <= 60):
                continue
            # ä¸­æ–‡å æ¯”ï¼ˆé¿å…çº¯æ•°å­—/é“¾æ¥å™ªå£°ï¼‰
            zh_ratio = len(re.findall(r"[\u4e00-\u9fa5]", title)) / max(len(title), 1)
            if zh_ratio < 0.3:
                continue

            items.append(title)

        # å»é‡ä¿åº
        seen = set()
        uniq = []
        for x in items:
            k = x.replace(" ", "").lower()
            if k in seen:
                continue
            seen.add(k)
            uniq.append(x)

        # é™åˆ¶æ•°é‡
        return uniq[:15]

# ====== Markdown è¾“å‡º ======
def build_md(items):
    n = now_tz()
    out = []
    out.append(f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰**  ")
    out.append("")
    out.append("**æ ‡é¢˜ï¼šæ¯æ—¥èµ„è®¯ï½œäººåŠ›èµ„æºç›¸å…³èµ„è®¯**  ")
    out.append("")
    if not items:
        out.append("> æŒ‡å®šæ—¶é—´èŒƒå›´å†…æœªå‘ç°æ–°çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚")
        return "\n".join(out)

    for i, it in enumerate(items, 1):
        out.append(f"{i}. [{it['title']}]({it['url']}) ï¼ˆ{it['date']}ï¼‰  ")
        for j, t in enumerate(it['titles'], 1):
            out.append(f"> {j}. {t}  ")
        out.append("")
    return "\n".join(out)

# ====== ä¸»å…¥å£ ======
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler_daily_only.pyï¼ˆåªæŠ“â€œä¸‰èŒ…æ—¥æŠ¥â€æ¡ç›®æ ‡é¢˜ï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("æ¯æ—¥èµ„è®¯ï½œäººåŠ›èµ„æºç›¸å…³èµ„è®¯", md)
