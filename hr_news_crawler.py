# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰Â· ä¸‰èŒ…æ—¥æŠ¥å‡€åŒ–ç‰ˆï¼ˆä¸‰é‡æ—¥æœŸæ ¡éªŒÂ·å½“å¤©ä¸€æ¡ï¼‰
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
ç‰¹æ€§ï¼š
1ï¼‰ä»…æŠ“â€œå½“å¤©â€çš„ã€Šä¸‰èŒ…æ—¥æŠ¥ã€‹ä¸€æ¡ï¼ˆå‘½ä¸­å³åœï¼‰ï¼›
2ï¼‰æ™ºèƒ½æå–æ­£æ–‡è¦ç‚¹â€”â€”åŒæ—¶å…¼å®¹â€œå¸¦ç¼–å·/ä¸å¸¦ç¼–å·â€çš„å°èŠ‚æ ‡é¢˜ï¼›
3ï¼‰å¼ºåŠ›å‡€åŒ–ï¼šå‰”é™¤â€œæ—¥æœŸ/é˜…è¯»é‡ï¼ˆcontent-descï¼‰â€â€œä¸Šä¸€ç¯‡/ä¸‹ä¸€ç¯‡â€
   â€œç›¸å…³èµ„è®¯/åº•éƒ¨æ¨èåŒºï¼ˆother-wrap/activeï¼‰â€â€œç»Ÿè®¡æŒ‰é’®ï¼ˆfn-dataStatistics-btnï¼‰â€
   ç­‰ä¸€åˆ‡æ— å…³å†…å®¹ï¼›
4ï¼‰è¾“å‡º Markdownï¼Œå¯å¯¹æ¥é’‰é’‰ï¼ˆæœªé…ç½®åˆ™è·³è¿‡æ¨é€ï¼‰ã€‚

ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰ï¼š
- HR_TARGET_DATE=YYYY-MM-DD   æŒ‡å®šç›®æ ‡æ—¥æœŸï¼ˆä¸è®¾åˆ™ç”¨ä»Šæ—¥ï¼ŒAsia/Shanghaiï¼‰
- SRC_HRLOO_URLS              æºç«™åˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼Œé»˜è®¤ https://www.hrloo.com/
- DINGTALK_BASE / DINGTALK_SECRET  é’‰é’‰æœºå™¨äºº

"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, date
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ========== æ—¶åŒº ==========
try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo

def _tz(): return ZoneInfo("Asia/Shanghai")
def now_tz(): return datetime.now(_tz())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())

# ========== é’‰é’‰ ==========
def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASE") or os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRET") or os.getenv("DINGTALK_SECRETA")
    if not base:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASEï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    try:
        r = requests.post(
            _sign_webhook(base, secret),
            json={"msgtype":"markdown","markdown":{"title":title,"text":md}},
            timeout=20
        )
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} code={r.status_code}")
        if not ok: print("resp:", r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e); return False

# ========== ä¼šè¯ ==========
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({"User-Agent":"Mozilla/5.0", "Accept-Language":"zh-CN,zh;q=0.9"})
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

# ========== å·¥å…· ==========
CN_TITLE_DATE = re.compile(r"[ï¼ˆ(]\s*(20\d{2})\s*[å¹´\-/.]\s*(\d{1,2})\s*[æœˆ\-/.]\s*(\d{1,2})\s*[)ï¼‰]")

def date_from_bracket_title(text: str):
    m = CN_TITLE_DATE.search(text or "")
    if not m: return None
    try:
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        return date(y, mo, d)
    except:
        return None

# éœ€è¦å¼ºåˆ¶å‰”é™¤çš„çˆ¶çº§ classï¼ˆå‘½ä¸­ä»»ä¸€å³è·³è¿‡ï¼‰
BLOCK_CONTAINER_CLASSES = {
    # é¡¶éƒ¨ï¼šæ—¥æœŸ/é˜…è¯»é‡ç­‰
    "content-desc",
    # åº•éƒ¨æ¨è/ç›¸å…³èµ„è®¯åŒºå—
    "other-wrap", "active", "other", "other-wrap-con",
    # ä¸Š/ä¸‹ä¸€ç¯‡å¯¼èˆªç­‰
    "prevnext", "prevnext-wrap",
    # æ ‡ç­¾/çƒ­è¯åŒº
    "tags-layout", "tag-layout", "hot-layout",
    # ç»Ÿè®¡æŒ‰é’®/é˜…è¯»ç»Ÿè®¡
    "fn-dataStatistics-btn", "fn-dataStatistics",
    # å³ä¾§æ ä¸é¡µè„šå¹²æ‰°ï¼ˆå…œåº•ï¼‰
    "aside", "sidebar", "copyright", "footer"
}

# å‘½ä¸­æ–‡æœ¬å…³é”®è¯å°±å‰”é™¤ï¼ˆå¤šä¸€å±‚å…œåº•ï¼‰
DROP_TEXT_PAT = re.compile(
    r"(é˜…è¯»é‡|é˜…è¯»\b|ä¸Šä¸€ç¯‡|ä¸‹ä¸€ç¯‡|ç›¸å…³èµ„è®¯|çƒ­é—¨|çƒ­æ¦œ|è´£ä»»ç¼–è¾‘|æ¥æº[:ï¼š]|å…è´£å£°æ˜|ç‰ˆæƒ|æœ¬æ–‡å†…å®¹|æ ‡ç­¾)",
    re.I
)

def has_block_ancestor(node):
    """å‘ä¸Šæ£€æŸ¥çˆ¶çº§æ˜¯å¦å¸¦æœ‰éœ€å‰”é™¤çš„ classã€‚"""
    p = node.parent
    while p and getattr(p, "attrs", None) is not None:
        classes = set((p.get("class") or []))
        if classes & BLOCK_CONTAINER_CLASSES:
            return True
        p = p.parent
    return False

# ========== çˆ¬è™« ==========
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = 1  # å½“å¤©ä»…ç•™ä¸€æ¡

        # ç›®æ ‡æ—¥æœŸï¼šç¯å¢ƒå˜é‡æˆ–ä»Šæ—¥
        t = (os.getenv("HR_TARGET_DATE") or "").strip()
        if t:
            try:
                y, m, d = map(int, re.split(r"[-/\.]", t))
                self.target_date = date(y, m, d)
            except:
                print("âš ï¸ HR_TARGET_DATE è§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šæ—¥ã€‚")
                self.target_date = now_tz().date()
        else:
            self.target_date = now_tz().date()

        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        # é¡¶éƒ¨é¢‘é“é¡µ + HR ä¸“æ é¡µåŒæ—¶å…œä½
        default_src = "https://www.hrloo.com/"
        add_src = "https://www.hrloo.com/news/hr"
        env_src = os.getenv("SRC_HRLOO_URLS", f"{default_src},{add_src}")
        self.sources = [u.strip() for u in env_src.split(",") if u.strip()]
        print(f"[CFG] target_date={self.target_date} {zh_weekday(now_tz())}  sources={self.sources}")

    def crawl(self):
        for base in self.sources:
            if self._crawl_source(base):
                break  # å‘½ä¸­å³åœ

    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=20)
        except Exception as e:
            print("é¦–é¡µè¯·æ±‚å¼‚å¸¸ï¼š", base, e); return False
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥ï¼š", base, r.status_code); return False

        soup = BeautifulSoup(r.text, "html.parser")

        # 1) ä¸»åˆ—è¡¨å®¹å™¨ï¼šdwdata-time + æ ‡é¢˜æ‹¬å·æ—¥æœŸ åŒé‡åŒ¹é…
        items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
        if items:
            for div in items:
                dts = (div.get("dwdata-time") or "").strip()
                if dts:
                    try:
                        pub_d = datetime.strptime(dts.split()[0], "%Y-%m-%d").date()
                        if pub_d != self.target_date:
                            continue
                    except:
                        pass
                a = div.find("a", href=True)
                if not a: continue
                title_text = norm(a.get_text())
                if not self.daily_title_pat.search(title_text):
                    continue
                t2 = date_from_bracket_title(title_text)
                if t2 and t2 != self.target_date:
                    continue
                abs_url = urljoin(base, a["href"])
                if self._try_detail(abs_url):
                    return True
            print("[MISS] å®¹å™¨é€šé“æœªå‘½ä¸­ï¼š", base)

        # 2) å¤‡ç”¨ï¼šå…¨é¡µé“¾æ¥æ‰«æ
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href", "")
            if not re.search(r"/news/\d+\.html$", href):
                continue
            text = norm(a.get_text())
            if not self.daily_title_pat.search(text):
                continue
            t2 = date_from_bracket_title(text)
            if t2 and t2 != self.target_date:
                continue
            links.append(urljoin(base, href))

        seen = set()
        for url in links:
            if url in seen: continue
            seen.add(url)
            if self._try_detail(url):
                return True

        print("[MISS] æœ¬æºæœªå‘½ä¸­ç›®æ ‡æ—¥æœŸï¼š", base)
        return False

    # è¯¦æƒ…å¤æ ¸ + æ¸…æ´—
    def _try_detail(self, abs_url):
        pub_dt, titles, page_title = self._fetch_detail_clean(abs_url)
        if not page_title or not self.daily_title_pat.search(page_title):
            return False
        # æ ‡é¢˜æ‹¬å·æ—¥æœŸå†æ ¡éªŒ
        t3 = date_from_bracket_title(page_title)
        if t3 and t3 != self.target_date:
            return False
        # è‹¥æœªåœ¨æ ‡é¢˜ä¸­ç»™å‡ºæ—¥æœŸï¼Œåˆ™ç”¨è¯¦æƒ…é¡µå‘å¸ƒæ—¶é—´å…œåº•
        if pub_dt and pub_dt.date() != self.target_date and not t3:
            return False
        if not titles:
            return False

        self.results.append({
            "title": page_title,
            "url": abs_url,
            "date": (pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else f"{self.target_date} 09:00"),
            "titles": titles
        })
        print(f"[HIT] {abs_url} -> {len(titles)} æ¡")
        return True

    def _extract_pub_time(self, soup):
        cand = []
        for t in soup.select("time[datetime]"):
            cand.append(t.get("datetime", ""))
        for m in soup.select("meta[property='article:published_time'],meta[name='pubdate'],meta[name='publishdate']"):
            cand.append(m.get("content", ""))
        for sel in [".time", ".date", ".pubtime", ".post-time", ".publish-time", ".info"]:
            for x in soup.select(sel):
                cand.append(x.get_text(" ", strip=True))

        pat = re.compile(r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?")
        def parse_one(s):
            m = pat.search(s or "")
            if not m: return None
            try:
                y, mo, d = int(m[1]), int(m[2]), int(m[3])
                hh = int(m[4]) if m[4] else 9
                mm = int(m[5]) if m[5] else 0
                return datetime(y, mo, d, hh, mm, tzinfo=_tz())
            except:
                return None

        dts = [dt for dt in map(parse_one, cand) if dt]
        if dts:
            now = now_tz()
            past = [dt for dt in dts if dt <= now]
            return min(past or dts, key=lambda dt: abs((now - dt).total_seconds()))
        return None

    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=(6, 20))
            if r.status_code != 200:
                print("[DetailFail]", url, r.status_code); return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            title_tag = soup.find(["h1", "h2"])
            page_title = norm(title_tag.get_text()) if title_tag else ""

            pub_dt = self._extract_pub_time(soup)

            # ä¸»è¦æ­£æ–‡å®¹å™¨ï¼ˆå¤šé€‰ä¸€å…œåº•ï¼‰
            container = (
                soup.select_one("article, .article, .detail-content, .news-content, .content, .post-content")
                or soup.select_one(".hr-rich-text, .content-wrap, .content-con")
                or soup
            )

            titles = self._extract_daily_item_titles(container)
            return pub_dt, titles, page_title
        except Exception as e:
            print("[DetailError]", url, e); return None, [], ""

    # â€”â€” æŠ½å–â€œæ¯æ—¥è¦ç‚¹â€æ ‡é¢˜ï¼ˆå…¼å®¹å¸¦ç¼–å·/ä¸å¸¦ç¼–å·ï¼‰ï¼Œå¹¶åšå¼ºåŠ›å‡€åŒ– â€”â€” #
    def _extract_daily_item_titles(self, root):
        by_num = {}   # 1,2,3â€¦ -> æ–‡æœ¬
        plain = []    # éç¼–å·å°èŠ‚ï¼ˆæŒ‰å‡ºç°é¡ºåºï¼‰

        # 1ï¼‰ä¼˜å…ˆï¼šè¯†åˆ«å¸¦ç¼–å·çš„è¡Œï¼ˆï¼ˆ1ï¼‰1. 1ã€ â‘  ä¹‹ç±»ï¼‰
        num_pat = re.compile(r"^\s*[ï¼ˆ(]?\s*(\d{1,2})\s*[)ï¼‰]?\s*[ã€.ï¼)]?\s*(.+)$")

        # 2ï¼‰å€™é€‰å…ƒç´ ï¼šæ ‡é¢˜å¸¸è§æ ‡ç­¾ + æ®µè½
        for node in root.find_all(["h2", "h3", "h4", "strong", "b", "p", "li", "div", "span"]):
            # çˆ¶çº§é»‘åå•è¿‡æ»¤
            if has_block_ancestor(node):
                continue

            raw = (node.get_text(" ", strip=True) or "").strip()
            if not raw:
                continue

            # æ–‡æœ¬é»‘åå•å…œåº•
            if DROP_TEXT_PAT.search(raw):
                continue

            # è¿‡æ»¤è¿‡çŸ­/å…¨æ•°å­—/ç–‘ä¼¼æ—¶é—´ä¸é˜…è¯»
            if len(raw) < 4:
                continue
            if re.fullmatch(r"\d+\s*(é˜…è¯»|æ¬¡)?", raw):
                continue
            if re.fullmatch(r"\d{2}-\d{2}-\d{2}.*", raw):
                continue

            # è§£æç¼–å·
            m = num_pat.match(raw)
            if m:
                try:
                    num = int(m.group(1))
                    txt = m.group(2).strip()
                    # å»æ‰æ‹¬å·å†…å¤‡æ³¨ï¼ˆå°½é‡ä¿ç•™æ ‡é¢˜çš„ä¸»ä½“ï¼‰
                    txt = re.split(r"[ï¼ˆ(]", txt)[0].strip()
                    if 3 <= len(txt) <= 80:
                        by_num.setdefault(num, txt)
                    continue
                except:
                    pass

            # éç¼–å·æ ‡é¢˜ï¼šå°½é‡æ•æ‰â€œä¸»é¢˜å¼å¥å­â€
            # ç»éªŒï¼šä¸­æ–‡æ¯”ä¾‹è¿‡ä½/è¿‡é•¿/åŒ…å«å¥æœ«å¥å·çš„æ­£æ–‡æ®µï¼Œè·³è¿‡
            zh_ratio = len(re.findall(r"[\u4e00-\u9fa5]", raw)) / max(len(raw), 1)
            if zh_ratio < 0.35:
                continue
            # å…¸å‹æ ‡é¢˜å¾€å¾€ä¸ä»¥å¥å·ç»“å°¾
            if raw.endswith(("ã€‚", ".", "ï¼", "ï¼Ÿ")) and len(raw) > 18:
                continue

            # é•¿åº¦åˆç†çš„éç¼–å·çŸ­æ ‡é¢˜
            if 4 <= len(raw) <= 60:
                plain.append(raw)

        # 3ï¼‰ç»„è£…é¡ºåºï¼šä¼˜å…ˆç¼–å· 1..Nï¼Œç¼ºå°‘ç¼–å·æ—¶å†å¡«å……éç¼–å·
        seq = []
        n = 1
        while n in by_num and len(seq) < 15:
            seq.append(by_num[n])
            n += 1
        if len(seq) < 3 and plain:  # å¦‚æœè¿™ä¸ªæ—¥æ›´æ²¡æœ‰ç¼–å·ï¼Œç”¨éç¼–å·å¤‡ä»½
            # å»é‡ä¿æŒé¡ºåº
            seen = set()
            for t in plain:
                if t in seen: continue
                seen.add(t)
                seq.append(t)
                if len(seq) >= 15: break

        return seq

# ========== Markdown ==========
def build_md(items):
    n = now_tz()
    out = [
        f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰**  ",
        "",
        "**æ ‡é¢˜ï¼šäººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹**  ",
        ""
    ]
    if not items:
        out.append("> æœªå‘ç°å½“å¤©çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚")
        return "\n".join(out)

    it = items[0]
    for j, t in enumerate(it["titles"], 1):
        out.append(f"{j}. {t}  ")
    out.append(f"[æŸ¥çœ‹è¯¦ç»†]({it['url']}) ï¼ˆ{it['date'][:10]}ï¼‰  ")
    return "\n".join(out)

# ========== ä¸»å…¥å£ ==========
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler.pyï¼ˆå½“å¤©ä¸€æ¡ Â· ä¸‰é‡æ—¥æœŸæ ¡éªŒç‰ˆï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("äººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹", md)
