# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰Â· ä¸‰èŒ…æ—¥æŠ¥ä¸“æŠ“ + æ¡ç›®å‡€åŒ–ç‰ˆ
åŠŸèƒ½ï¼š
  âœ… ä»…æŠ“å–æ ‡é¢˜åŒ…å«â€œä¸‰èŒ…æ—¥æŠ¥â€çš„æ–°é—»ï¼›
  âœ… ä»æ­£æ–‡ä¸­æå–ç¼–å·æ ‡é¢˜ï¼ˆ1ã€2ã€3ã€â€¦ï¼‰ï¼›
  âœ… è‡ªåŠ¨æ’é™¤å³æ å…¬å‘Šä¸æ‚è®¯ï¼›
  âœ… ä¿®å¤â€œï¼šâ€æˆªæ–­é—®é¢˜ï¼Œå®Œæ•´æå–æ–°é—»æ ‡é¢˜ï¼›
  âœ… è¾“å‡º Markdownï¼Œå¯æ¨é€é’‰é’‰ã€‚

ç¯å¢ƒå˜é‡ï¼š
  HR_ONLY_TODAY=1   # ä»…å½“å¤©ï¼Œå¦åˆ™ä¸ºè¿‘24å°æ—¶
  HR_MAX_ITEMS=15
  SRC_HRLOO_URLS=https://www.hrloo.com/
  DINGTALK_BASE / DINGTALK_SECRET æˆ– DINGTALK_BASEA / DINGTALK_SECRETA
"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ========= åŸºæœ¬å·¥å…· =========
try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo

def tz(): return ZoneInfo("Asia/Shanghai")
def now_tz(): return datetime.now(tz())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())

def within_24h(dt): return (now_tz() - dt).total_seconds() <= 86400 if dt else False
def same_day(dt): return dt and dt.date() == now_tz().date()

# ========= é’‰é’‰æ¨é€ =========
def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    h = hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(h))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASE") or os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRET") or os.getenv("DINGTALK_SECRETA")
    if not base:
        print("ğŸ”• æœªé…ç½®é’‰é’‰åœ°å€ï¼Œè·³è¿‡æ¨é€ã€‚"); return False
    webhook = _sign_webhook(base, secret)
    try:
        r = requests.post(webhook, json={"msgtype": "markdown","markdown":{"title":title,"text":md}}, timeout=20)
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} code={r.status_code}")
        if not ok: print("resp:", r.text[:200])
        return ok
    except Exception as e:
        print("DingTalk error:", e); return False

# ========= ç½‘ç»œä¼šè¯ =========
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({"User-Agent":"Mozilla/5.0","Accept-Language":"zh-CN,zh;q=0.9"})
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

# ========= ä¸»çˆ¬è™« =========
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = int(os.getenv("HR_MAX_ITEMS","15"))
        self.detail_timeout = (6,20)
        self.detail_sleep = 0.6
        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        self.only_today = os.getenv("HR_ONLY_TODAY","0") == "1"
        self.sources = [u.strip() for u in os.getenv("SRC_HRLOO_URLS","https://www.hrloo.com/").split(",") if u.strip()]

    def crawl(self):
        for base in self.sources:
            self._crawl_source(base)

    def _crawl_source(self, base):
        r = self.session.get(base, timeout=20)
        if r.status_code != 200:
            print("é¦–é¡µè®¿é—®å¤±è´¥", r.status_code); return
        soup = BeautifulSoup(r.text,"html.parser")
        links = []
        for a in soup.select("a[href*='/news/']"):
            href, text = a.get("href",""), norm(a.get_text())
            if re.search(r"/news/\d+\.html$", href) and self.daily_title_pat.search(text):
                links.append(urljoin(base, href))
        if not links:
            links = [urljoin(base,a.get("href")) for a in soup.select("a[href*='/news/']") if re.search(r"/news/\d+\.html$",a.get("href",""))]

        seen=set()
        for url in links:
            if url in seen: continue
            seen.add(url)
            pub_dt, titles, main = self._fetch_detail_clean(url)
            if not main or not self.daily_title_pat.search(main): continue
            if self.only_today and not same_day(pub_dt): continue
            if not self.only_today and not within_24h(pub_dt): continue
            if not titles: continue
            self.results.append({"title":main,"url":url,"date":pub_dt.strftime("%Y-%m-%d %H:%M"),"titles":titles})
            print(f"[OK] {url} -> {len(titles)} æ¡")
            if len(self.results)>=self.max_items: break
            time.sleep(self.detail_sleep)

    def _extract_pub_time(self,soup):
        txt = soup.get_text(" ")
        m = re.search(r"(20\d{2})[-/.å¹´](\d{1,2})[-/.æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?",txt)
        if not m: return None
        y,mo,d = int(m[1]),int(m[2]),int(m[3])
        hh = int(m[4]) if m[4] else 9; mm = int(m[5]) if m[5] else 0
        try: return datetime(y,mo,d,hh,mm,tzinfo=tz())
        except: return None

    def _fetch_detail_clean(self,url):
        try:
            r=self.session.get(url,timeout=self.detail_timeout)
            if r.status_code!=200: return None,[], ""
            r.encoding=r.apparent_encoding or "utf-8"
            soup=BeautifulSoup(r.text,"html.parser")
            title_tag=soup.find(["h1","h2"])
            page_title=norm(title_tag.get_text()) if title_tag else ""
            pub_dt=self._extract_pub_time(soup)
            titles=self._extract_daily_item_titles(soup)
            return pub_dt,titles,page_title
        except Exception as e:
            print("[DetailError]",url,e)
            return None,[], ""

    # â€”â€” æ ¸å¿ƒï¼šæå–æ—¥æŠ¥æ¡ç›®æ ‡é¢˜ï¼ˆå·²ä¿®å¤ç¬¬5æ¡æˆªæ–­é—®é¢˜ + è‡ªåŠ¨å»æ‰æ— å…³ä¿¡æ¯ï¼‰â€”â€” #
    def _extract_daily_item_titles(self,soup):
        blacklist=["ç²‰ä¸","å…¥ç¾¤","ç”³è¯‰","çŸ­ä¿¡","éªŒè¯ç ","ä¸¾æŠ¥","å®¡æ ¸","å‘å¸ƒåè®®",
                   "è´¦å·","ç™»å½•","APP","åˆ›å»ºç”³è¯·","å¹¿å‘Š","ä¸‹è½½","æ¨å¹¿","ç¤¼åŒ…"]
        items=[]
        for t in soup.find_all(["h2","h3","h4","strong","b","p","li","span","div"]):
            raw=(t.get_text() or "").strip()
            if not raw: continue
            m=re.match(r"^\s*[ï¼ˆ(]?\s*(\d{1,2})\s*[)ï¼‰]?\s*[ã€.ï¼]?\s*(.+)$",raw)
            if not m: continue
            num=int(m.group(1)); title=m.group(2).strip()
            # ä¸æˆªæ–­â€œï¼šâ€å†…å®¹ï¼Œä»…å»æ‹¬æ³¨/å¤šä½™è¯´æ˜
            title=re.split(r"[ï¼ˆ\(]{1}",title)[0].strip()
            if not (4<=len(title)<=60): continue
            if len(re.findall(r"[\u4e00-\u9fa5]",title))/max(len(title),1)<0.3: continue
            if any(b in title for b in blacklist): continue
            items.append((num,title))
        if not items: return []
        start=next((i for i,(n,_) in enumerate(items) if n==1),0)
        seq=[]; seen=set(); expect=items[start][0]
        for n,t in items[start:]:
            if n!=expect: break
            if t not in seen:
                seq.append(t); seen.add(t)
            expect+=1
            if expect>20: break
        return seq[:15]

# ========= Markdown è¾“å‡º =========
def build_md(items):
    n=now_tz()
    out=[f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰**  ","","**æ ‡é¢˜ï¼šä¸‰èŒ…æ—¥æŠ¥ï½œæ¡ç›®é€Ÿè§ˆ**  ",""]
    if not items:
        out.append("> æœªå‘ç°æ–°çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚"); return "\n".join(out)
    for i,it in enumerate(items,1):
        out.append(f"{i}. [{it['title']}]({it['url']}) ï¼ˆ{it['date']}ï¼‰  ")
        for j,t in enumerate(it['titles'],1):
            out.append(f"> {j}. {t}  ")
        out.append("")
    return "\n".join(out)

# ========= ä¸»å…¥å£ =========
if __name__=="__main__":
    print("æ‰§è¡Œ hr_news_crawler_daily_clean.pyï¼ˆåªæŠ“ä¸‰èŒ…æ—¥æŠ¥æ¡ç›®æ ‡é¢˜ï¼‰")
    c=HRLooCrawler()
    c.crawl()
    md=build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("ä¸‰èŒ…æ—¥æŠ¥ï½œæ¡ç›®é€Ÿè§ˆ",md)
