# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰ä¸“æŠ“ç‰ˆ Â· ä»…æå–å°æ ‡é¢˜
- ä»…æŠ“å– HRLooï¼Œæ”¯æŒå½“å¤©è¿‡æ»¤ã€å…³é”®è¯è¿‡æ»¤ï¼›
- è¿›å…¥è¯¦æƒ…é¡µåªæå–åˆ†èŠ‚æ ‡é¢˜ï¼ˆå¦‚â€œ1ã€xxx / 2ã€xxxâ€ï¼‰ï¼Œä¸æŠ“æ­£æ–‡ã€ä¸æŠ“æ‘˜è¦ï¼›
- å…¼å®¹é’‰é’‰æ¨é€ã€‚
"""

import os
import re
import time
import hmac
import base64
import hashlib
import urllib.parse
import requests
from bs4 import BeautifulSoup, Comment
from urllib.parse import urljoin
from datetime import datetime
import ssl
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo


# ========= åŸºç¡€å·¥å…· =========
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]
def now_tz(): return datetime.now(ZoneInfo(os.getenv("HR_TZ", "Asia/Shanghai")))


# ========= é’‰é’‰ =========
def _sign_webhook(base, secret):
    if not base or "REPLACE_ME" in base:
        return ""
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    h = hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(h))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRETA")
    webhook = _sign_webhook(base, secret)
    if not webhook:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md}}
    r = requests.post(webhook, json=payload, timeout=20)
    ok = (r.status_code == 200 and r.json().get("errcode") == 0)
    print("DingTalk:", ok)
    return ok


# ========= ç½‘ç»œè¯·æ±‚ =========
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *args, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*args, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/123.0.0.0 Safari/537.36",
    })
    retry = Retry(total=3, backoff_factor=0.8, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=retry))
    return s


# ========= ä¸»çˆ¬è™« =========
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = int(os.getenv("HR_MAX_ITEMS", "10"))
        self.only_today = os.getenv("HR_ONLY_TODAY", "1") in ("1","true")
        self.detail_timeout = (6, 20)
        self.detail_sleep = 0.6

    def crawl_hrloo(self):
        url = "https://www.hrloo.com/"
        self._crawl_list("ä¸‰èŒ…äººåŠ›èµ„æºç½‘", url)

    def _crawl_list(self, source, base_url):
        print("å¼€å§‹æŠ“å–ä¸‰èŒ…åˆ—è¡¨...")
        r = self.session.get(base_url, timeout=20)
        if r.status_code != 200: return
        soup = BeautifulSoup(r.text, "html.parser")
        links = soup.select("a[href*='/news/']")
        added = set()

        for a in links:
            href = a.get("href")
            if not href or href in added: continue
            full = urljoin(base_url, href)
            if not re.search(r"/news/\d+\.html", full): continue
            added.add(full)
            title = norm(a.get_text())
            subs = self._fetch_titles(full)
            if not subs: continue
            self.results.append({"title": title, "url": full, "source": source, "subtitles": subs})
            if len(self.results) >= self.max_items: break
            time.sleep(self.detail_sleep)

    def _fetch_titles(self, url):
        """ä»…æŠ“è¯¦æƒ…é¡µå°æ ‡é¢˜ strong / h2 / h3 / span.bjh-p"""
        try:
            r = self.session.get(url, timeout=self.detail_timeout)
            if r.status_code != 200: return []
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            # æ¸…é™¤æ— ç”¨èŠ‚ç‚¹
            for t in soup(["script","style","footer","header","nav","iframe"]): t.decompose()
            for c in soup.find_all(string=lambda t:isinstance(t, Comment)): c.extract()

            # å°æ ‡é¢˜è¯†åˆ«
            subs = []
            for tag in soup.find_all(["strong","h2","h3","span"], class_=lambda c: c in (None, "bjh-p")):
                txt = norm(tag.get_text())
                if re.match(r"^\d+\s*[ã€.ï¼]\s*.+", txt) and txt not in subs:
                    subs.append(txt)
            return subs
        except Exception as e:
            print("detail error", e)
            return []


# ========= Markdown è¾“å‡º =========
def build_md(items):
    now = now_tz()
    out = [f"**æ—¥æœŸï¼š{now.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(now)}ï¼‰**", "", "**æ ‡é¢˜ï¼šæ—©å®‰èµ„è®¯ï½œäººåŠ›èµ„æºæ¯æ—¥èµ„è®¯æ¨é€**", "", "**ä¸»è¦å†…å®¹**"]
    if not items:
        out.append("> æš‚æ— æ›´æ–°ã€‚")
        return "\n".join(out)

    for i, it in enumerate(items, 1):
        out.append(f"{i}. [{it['title']}]({it['url']})ã€€â€”ã€€*{it['source']}*")
        for st in it["subtitles"]:
            out.append(f"> ğŸŸ¦ {st}")
        out.append("")
    return "\n".join(out)


# ========= ä¸»ç¨‹åº =========
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler.py")
    crawler = HRLooCrawler()
    crawler.crawl_hrloo()
    md = build_md(crawler.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("æ—©å®‰èµ„è®¯ï½œä¸‰èŒ…å°æ ‡é¢˜æŠ“å–", md)
