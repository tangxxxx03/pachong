# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰çˆ¬è™« Â· è¶…å¹²å‡€ç‰ˆï¼ˆ24å°æ—¶ + å…³é”®è¯è¿‡æ»¤ + å™ªå£°æ¸…é™¤ï¼‰
"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo


# ========= åŸºç¡€å‡½æ•° =========
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]
def now_tz(): return datetime.now(ZoneInfo("Asia/Shanghai"))
def within_24h(dt): return (now_tz() - dt).total_seconds() <= 86400 if dt else False


# ========= é’‰é’‰ =========
def _sign_webhook(base, secret):
    if not base or not secret: return ""
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    h = hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(h))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRETA")
    if not base or "REPLACE_ME" in base:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    webhook = _sign_webhook(base, secret)
    r = requests.post(webhook, json={"msgtype": "markdown", "markdown": {"title": title, "text": md}}, timeout=20)
    ok = (r.status_code == 200 and r.json().get("errcode") == 0)
    print("DingTalk:", ok)
    return ok


# ========= ç½‘ç»œ =========
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/123.0.0.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    })
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s


# ========= ä¸»çˆ¬è™« =========
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = 15
        self.detail_timeout = (6, 20)
        self.detail_sleep = 0.6

        # âœ… å…³é”®è¯ç™½åå•ï¼ˆåªæŠ“è¿™äº›ç›¸å…³çš„ï¼‰
        self.keywords = [k.strip() for k in os.getenv("HR_FILTER_KEYWORDS", "äººåŠ›èµ„æº, ç¤¾ä¿, å‘˜å·¥, ç”¨å·¥, åŠ³åŠ¨, æ‹›è˜, å·¥èµ„, ç¼´è´¹").split(",") if k.strip()]
        # ğŸš« å™ªå£°å…³é”®è¯ï¼ˆé‡åˆ°è¿™äº›å°±åˆ ï¼‰
        self.noise_words = [
            "æ‰‹æœº", "çŸ­ä¿¡", "éªŒè¯ç ", "è¯ˆéª—", "ä¸¾æŠ¥", "è¿è¥å•†", "é»‘åå•", "å®‰å…¨", "app", 
            "å®¢æœ", "å……å€¼", "å¯†ç ", "å°å·", "ä¿¡å·", "æ³¨é”€", "æ³¨å†Œ", "è´¦å·", "å¹¿å‘Š", "ä¸‹è½½"
        ]

    def crawl(self):
        base = "https://www.hrloo.com/"
        r = self.session.get(base, timeout=20)
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥", r.status_code)
            return
        soup = BeautifulSoup(r.text, "html.parser")
        links = [urljoin(base, a.get("href")) for a in soup.select("a[href*='/news/']") if re.search(r"/news/\d+\.html$", a.get("href",""))]
        seen = set()
        for url in links:
            if url in seen: continue
            seen.add(url)
            pub_dt, titles, main_title = self._fetch_detail_clean(url)
            if not pub_dt or not within_24h(pub_dt): continue
            if not titles: continue
            if not self._match_keywords(main_title, titles): continue
            self.results.append({"title": main_title or url, "url": url, "date": pub_dt.strftime("%Y-%m-%d %H:%M"), "titles": titles})
            print(f"[OK] {url} {pub_dt} å°æ ‡é¢˜{len(titles)}ä¸ª")
            if len(self.results) >= self.max_items: break
            time.sleep(self.detail_sleep)

    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=self.detail_timeout)
            if r.status_code != 200: return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            # ä¸»æ ‡é¢˜
            h = soup.find(["h1", "h2"])
            page_title = norm(h.get_text()) if h else ""

            # å‘å¸ƒæ—¶é—´
            pub_dt = self._extract_pub_time(soup)

            # æå–å°æ ‡é¢˜ï¼ˆè¿‡æ»¤åƒåœ¾ï¼‰
            titles = []
            for t in soup.find_all(["strong","h2","h3","span","p"]):
                text = norm(t.get_text())
                if not re.match(r"^\d+\s*[ã€.ï¼]\s*.+", text): 
                    continue
                if any(n in text for n in self.noise_words):  # ğŸš« å±è”½åƒåœ¾å†…å®¹
                    continue
                if len(text) < 6 or len(text) > 60:  # æ’é™¤å¤ªçŸ­æˆ–å¤ªé•¿çš„å¥å­
                    continue
                if text not in titles:
                    titles.append(text)
            return pub_dt, titles, page_title
        except Exception as e:
            print("[DetailError]", url, e)
            return None, [], ""

    def _extract_pub_time(self, soup):
        tz = ZoneInfo("Asia/Shanghai")
        txt = soup.get_text(" ")
        m = re.search(r"(20\d{2})[-/.å¹´](\d{1,2})[-/.æœˆ](\d{1,2})(?:\s+(\d{1,2}):(\d{1,2}))?", txt)
        if not m: return None
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        hh = int(m[4]) if m[4] else 9
        mm = int(m[5]) if m[5] else 0
        try:
            return datetime(y, mo, d, hh, mm, tzinfo=tz)
        except: return None

    def _match_keywords(self, title, subtitles):
        if not self.keywords: return True
        hay = (title or "") + " " + " ".join(subtitles)
        for kw in self.keywords:
            if kw in hay:
                return True
        return False


# ========= Markdown è¾“å‡º =========
def build_md(items):
    now = now_tz()
    out = [f"**æ—¥æœŸï¼š{now.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(now)}ï¼‰**", "",
           "**æ ‡é¢˜ï¼šæ—©å®‰èµ„è®¯ï½œäººåŠ›èµ„æºæ¯æ—¥èµ„è®¯æ¨é€**", "", "**ä¸»è¦å†…å®¹**"]
    if not items:
        out.append("> 24å°æ—¶å†…æ— ç¬¦åˆå…³é”®è¯çš„å†…å®¹ã€‚")
        return "\n".join(out)
    for i, it in enumerate(items, 1):
        out.append(f"{i}. [{it['title']}]({it['url']}) ï¼ˆ{it['date']}ï¼‰")
        for s in it['titles']:
            out.append(f"> ğŸŸ¦ {s}")
        out.append("")
    return "\n".join(out)


# ========= ä¸»å…¥å£ =========
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler.pyï¼ˆè¶…å¹²å‡€ç‰ˆï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("æ—©å®‰èµ„è®¯ï½œäººåŠ›èµ„æºæ¯æ—¥èµ„è®¯æ¨é€", md)
