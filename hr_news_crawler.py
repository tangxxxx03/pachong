# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰ä¸“æŠ“ç‰ˆ Â· æŠ“å–æ­£æ–‡+æ‘˜è¦
- ä»…æŠ“å– HRLooï¼›æ”¯æŒå½“å¤©è¿‡æ»¤ã€å…³é”®è¯è¿‡æ»¤ï¼›è¿›å…¥è¯¦æƒ…é¡µæŠ“æ­£æ–‡å¹¶æ¨é€é’‰é’‰
- å…¼å®¹å¤šå¥—é’‰é’‰å˜é‡åï¼šDINGTALK_BASEA / DINGTALK_WEBHOOKA / DINGTALK_BASE / DINGTALK_WEBHOOK
                       DINGTALK_SECRETA / DINGTALK_SECRET
- ä¸»è¦ç¯å¢ƒå˜é‡ï¼š
    HR_TZ=Asia/Shanghai
    HR_ONLY_TODAY=1              # ä»…æŠ“å½“å¤©ï¼ˆ1/0ï¼‰
    HR_FILTER_KEYWORDS="äººåŠ›èµ„æº,ç¤¾ä¿,ç”¨å·¥"
    HR_REQUIRE_ALL=0             # å…³é”®è¯éœ€å…¨éƒ¨å‘½ä¸­ï¼ˆ1ï¼‰æˆ–ä»»ä¸€å‘½ä¸­ï¼ˆ0ï¼‰
    HR_MAX_ITEMS=10              # å•ç«™æœ€å¤§æŠ“å–æ¡æ•°ï¼ˆåˆ—è¡¨é¡µï¼‰
    HR_SHOW_LIMIT=20             # Markdown å±•ç¤ºä¸Šé™
    SRC_HRLOO_URLS="https://www.hrloo.com/"

    HR_FETCH_DETAIL=1            # æ˜¯å¦æŠ“è¯¦æƒ…é¡µï¼ˆ1/0ï¼‰
    HR_DETAIL_MAXCHARS=1200      # è¯¦æƒ…æ­£æ–‡æŠ“å–çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆè¶…å‡ºä¼šæˆªæ–­ï¼‰
    HR_DETAIL_TIMEOUT="6.0,20"   # è¿æ¥/è¯»å–è¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œæ ¼å¼ "connect,read"
    HR_DETAIL_SLEEP=0.8          # æ¯ç¯‡è¯¦æƒ…æŠ“å–åçš„ä¼‘çœ ç§’æ•°ï¼ˆé™é€Ÿé˜²å°ï¼‰

- CLI å¯è¦†ç›–ï¼š--limit --keywords --require-all --only-today/--all --max-per-source
"""

import os
import re
import time
import argparse
import hmac
import hashlib
import base64
import urllib.parse
from urllib.parse import urljoin
from datetime import datetime
import ssl

# å…¼å®¹ Py<3.9 çš„ zoneinfo
try:
    from zoneinfo import ZoneInfo  # Py3.9+
except Exception:  # pragma: no cover
    from backports.zoneinfo import ZoneInfo  # pip install backports.zoneinfo

import requests
from bs4 import BeautifulSoup, Comment
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ====================== é€šç”¨å·¥å…· ======================
def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").replace("\u3000", " ").strip())

def zh_weekday(dt):
    return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]

def get_env_any(names: list[str], default: str = "") -> str:
    for n in names:
        v = os.getenv(n, "").strip()
        if v:
            return v
    return default.strip()

# ====================== DingTalk ç»Ÿä¸€é…ç½®ï¼ˆå¤šåå…¼å®¹ï¼‰ ======================
DEFAULT_DINGTALK_WEBHOOK = (
    "https://oapi.dingtalk.com/robot/send?access_token=REPLACE_ME"
)
DEFAULT_DINGTALK_SECRET  = "SEC_REPLACE_ME"

DINGTALK_BASE = get_env_any(
    ["DINGTALK_BASEA", "DINGTALK_WEBHOOKA", "DINGTALK_BASE", "DINGTALK_WEBHOOK"],
    DEFAULT_DINGTALK_WEBHOOK
)
DINGTALK_SECRET = get_env_any(["DINGTALK_SECRETA", "DINGTALK_SECRET"], DEFAULT_DINGTALK_SECRET)
DINGTALK_KEYWORD = os.getenv("DINGTALK_KEYWORD", "").strip()

def _sign_webhook(base_webhook: str, secret: str) -> str:
    if not base_webhook:
        return ""
    if not secret:
        return base_webhook
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    sep = "&" if "?" in base_webhook else "?"
    return f"{base_webhook}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    webhook = _sign_webhook(DINGTALK_BASE, DINGTALK_SECRET)
    if not webhook or "REPLACE_ME" in webhook:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼ˆæˆ–ä»ä¸ºå ä½å€¼ï¼‰ï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    if DINGTALK_KEYWORD and (DINGTALK_KEYWORD not in title and DINGTALK_KEYWORD not in md_text):
        title = f"{DINGTALK_KEYWORD} | {title}"
    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    try:
        r = requests.post(webhook, json=payload, timeout=20)
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print("DingTalk resp:", r.status_code, r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

# ====================== HTTP ä¼šè¯ï¼ˆé‡è¯•/æ—§TLSå…¼å®¹ï¼‰ ======================
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *args, **kwargs):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kwargs["ssl_context"] = ctx
        return super().init_poolmanager(*args, **kwargs)

def make_session():
    s = requests.Session()
    s.trust_env = False
    s.headers.update({
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    })
    retries = Retry(
        total=3,
        backoff_factor=0.8,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "POST"]),
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    legacy = LegacyTLSAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    s.mount("http://", adapter)
    s.mount("https://", legacy)
    return s

def now_tz():
    tz = ZoneInfo(os.getenv("HR_TZ", "Asia/Shanghai").strip())
    return datetime.now(tz)

# ====================== å‚æ•° & è§£æ ======================
def parse_args():
    parser = argparse.ArgumentParser(description="äººåŠ›èµ„æºæ¯æ—¥èµ„è®¯æ¨é€ï¼ˆHRLoo ä¸“æŠ“+æ­£æ–‡ï¼‰")
    parser.add_argument("--tz", default=os.getenv("HR_TZ", "Asia/Shanghai"))
    parser.add_argument("--limit", type=int, default=int(os.getenv("HR_SHOW_LIMIT", "20")))
    parser.add_argument("--no-push", action="store_true")
    parser.add_argument("--keywords", default=os.getenv("HR_FILTER_KEYWORDS", "äººåŠ›èµ„æº"))
    parser.add_argument("--require-all", action="store_true",
                        default=os.getenv("HR_REQUIRE_ALL", "0").strip().lower() in ("1","true","yes","y"))
    parser.add_argument("--only-today", action="store_true",
                        default=os.getenv("HR_ONLY_TODAY", "1").strip().lower() in ("1","true","yes","y"))
    parser.add_argument("--all", dest="only_today", action="store_false")
    parser.add_argument("--max-per-source", type=int, default=int(os.getenv("HR_MAX_ITEMS", "10")))
    return parser.parse_args()

def split_keywords(s: str) -> list[str]:
    return [k.strip() for k in re.split(r"[,\sï¼Œï¼›;|]+", s or "") if k.strip()]

DEFAULT_SELECTORS = [
    ".list li", ".news-list li", ".content-list li", ".box-list li",
    "ul.list li", "ul.news li", "ul li", "li"
]

def as_list(env_name: str, defaults: list[str]) -> list[str]:
    raw = os.getenv(env_name, "").strip()
    if not raw: return defaults
    return [u.strip() for u in raw.split(",") if u.strip()]

# ====================== HRLoo ä¸“æŠ“ ======================
class HRLooCrawler:
    def __init__(self, only_today: bool, require_all: bool, keywords: list[str], max_per_source: int):
        self.session = make_session()
        self.results, self._seen = [], set()
        self.only_today = only_today
        self.require_all = require_all
        self.keywords = [k.lower() for k in keywords]
        self.max_per_source = max_per_source

        # è¯¦æƒ…é…ç½®
        self.fetch_detail = os.getenv("HR_FETCH_DETAIL", "1").strip().lower() in ("1","true","yes","y")
        self.detail_maxchars = int(os.getenv("HR_DETAIL_MAXCHARS", "1200"))
        # è¶…æ—¶é…ç½® "connect,read"
        tconf = (os.getenv("HR_DETAIL_TIMEOUT", "6.0,20").split(",") + ["6.0","20"])[:2]
        try:
            self.detail_timeout = (float(tconf[0]), float(tconf[1]))
        except Exception:
            self.detail_timeout = (6.0, 20.0)
        self.detail_sleep = float(os.getenv("HR_DETAIL_SLEEP", "0.8"))

    def crawl_hrloo(self):
        urls = as_list("SRC_HRLOO_URLS", ["https://www.hrloo.com/"])
        self._crawl_generic("ä¸‰èŒ…äººåŠ›èµ„æºç½‘", "https://www.hrloo.com", urls)

    # é€šç”¨æŠ“å–ï¼šåªç”¨äº HRLoo
    def _crawl_generic(self, source_name: str, base: str | None, list_urls: list[str], selectors=None):
        if not list_urls: return
        selectors = selectors or DEFAULT_SELECTORS
        total = 0
        for url in list_urls:
            if total >= self.max_per_source: break
            try:
                resp = self.session.get(url, timeout=(6.1, 20))
                resp.encoding = resp.apparent_encoding or resp.encoding or "utf-8"
                if resp.status_code != 200:
                    continue
                soup = BeautifulSoup(resp.text, "html.parser")
                items = []
                for css in selectors:
                    items = soup.select(css)
                    if items: break
                if not items:
                    items = soup.select("a")
                for node in items:
                    if total >= self.max_per_source: break
                    a = node if getattr(node, "name", None) == "a" else node.find("a")
                    if not a:
                        continue
                    title = norm(a.get_text())
                    if not title:
                        continue
                    href = a.get("href") or ""
                    full_url = urljoin(base or url, href)

                    # æ—¥æœŸ
                    date_text = self._find_date(node) or self._find_date(a)
                    if self.only_today and (not date_text or not self._is_today(date_text)):
                        continue

                    # åˆæ­¥æ‘˜è¦ï¼ˆåˆ—è¡¨èŠ‚ç‚¹ï¼‰
                    list_snippet = self._snippet(node)

                    if not self._hit_keywords(title, list_snippet):
                        continue

                    detail_text = ""
                    detail_summary = ""
                    if self.fetch_detail:
                        detail_text, detail_summary = self._fetch_detail(full_url)
                        time.sleep(self.detail_sleep)

                    item = {
                        "title": title,
                        "url": full_url,
                        "source": source_name,
                        "date": date_text,
                        "content": detail_text or list_snippet,
                        "summary": detail_summary or self._first_sentences(list_snippet, 2),
                    }
                    if self._push_if_new(item):
                        total += 1
            except Exception:
                continue

    # è¯¦æƒ…æŠ“å–
    def _fetch_detail(self, url: str) -> tuple[str, str]:
        try:
            r = self.session.get(url, timeout=self.detail_timeout)
            r.encoding = r.apparent_encoding or r.encoding or "utf-8"
            if r.status_code != 200:
                return "", ""
            soup = BeautifulSoup(r.text, "html.parser")

            # å»æ‰è„šæœ¬/æ ·å¼/æ³¨é‡Š
            for tag in soup(["script", "style", "noscript"]):
                tag.decompose()
            for c in soup.find_all(string=lambda t: isinstance(t, Comment)):
                c.extract()

            # å¸¸è§æ­£æ–‡å®¹å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            candidates = [
                ".article", ".article-content", ".article-body", ".news-content",
                ".post-content", ".content", ".entry-content", "#content", "#article",
                ".article_box", ".neirong", ".detail", ".detail-content"
            ]
            node = None
            for css in candidates:
                node = soup.select_one(css)
                if node and norm(node.get_text()):
                    break
            if not node:
                # å…œåº•ï¼šé¡µé¢ä¸­æœ€é•¿æ–‡æœ¬å—
                blocks = soup.find_all(["div","section","article","main"])
                node = max(blocks, key=lambda n: len(norm(n.get_text() or "")), default=None)

            text = norm(node.get_text(" ")) if node else ""
            if not text:
                return "", ""
            text = text[: self.detail_maxchars].strip()

            # æ‘˜è¦ï¼šå–å‰ 2~3 å¥
            summary = self._first_sentences(text, 3, hard_limit=220)
            return text, summary
        except Exception:
            return "", ""

    # å·¥å…·å‡½æ•°
    def _push_if_new(self, item: dict) -> bool:
        key = item.get("url") or f"{item.get('title','')}|{item.get('date','')}"
        if key in self._seen:
            return False
        self._seen.add(key)
        self.results.append(item)
        return True

    @staticmethod
    def _snippet(node) -> str:
        try:
            text = node.get_text(" ", strip=True)
            text = re.sub(r"\s+", " ", text)
            return (text[:150] + "...") if len(text) > 150 else text
        except Exception:
            return ""

    def _find_date(self, node) -> str:
        if not node: return ""
        raw = node.get_text(" ", strip=True)
        if re.search(r"(åˆšåˆš|åˆ†é’Ÿ|å°æ—¶å‰|ä»Šæ—¥|ä»Šå¤©)", raw):
            return now_tz().strftime("%Y-%m-%d")
        if re.search(r"(\d+)\s*(åˆ†é’Ÿ|å°æ—¶)å‰", raw):
            return now_tz().strftime("%Y-%m-%d")
        if re.search(r"ä»Šå¤©\s*\d{1,2}:\d{1,2}", raw):
            return now_tz().strftime("%Y-%m-%d")
        normtxt = raw.replace("å¹´","-").replace("æœˆ","-").replace("æ—¥","-").replace("/", "-").replace(".", "-")
        m = re.search(r"(20\d{2}|19\d{2})-(\d{1,2})-(\d{1,2})", normtxt)
        if m:
            return f"{int(m.group(1)):04d}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"
        m2 = re.search(r"\b(\d{1,2})-(\d{1,2})\b", normtxt)
        if m2:
            return f"{now_tz().year:04d}-{int(m2.group(1)):02d}-{int(m2.group(2)):02d}"
        return ""

    def _parse_date(self, s: str):
        if not s: return None
        s = s.strip().replace("å¹´","-").replace("æœˆ","-").replace("æ—¥","-").replace("/", "-").replace(".", "-")
        for fmt in ("%Y-%m-%d", "%y-%m-%d", "%Y-%m", "%m-%d"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m-%d":
                    dt = dt.replace(year=now_tz().year)
                if fmt == "%y-%m-%d" and dt.year < 2000:
                    dt = dt.replace(year=2000 + dt.year % 100)
                return dt.replace(tzinfo=ZoneInfo(os.getenv("HR_TZ", "Asia/Shanghai").strip()))
            except ValueError:
                continue
        return None

    def _is_today(self, date_str: str) -> bool:
        dt = self._parse_date(date_str)
        return bool(dt and dt.date() == now_tz().date())

    def _hit_keywords(self, title: str, content: str) -> bool:
        if not self.keywords:
            return True
        hay_low = ((title or "") + " " + (content or "")).lower()
        if self.require_all:
            return all(k in hay_low for k in self.keywords)
        return any(k in hay_low for k in self.keywords)

    @staticmethod
    def _first_sentences(text: str, n: int = 2, hard_limit: int = 180) -> str:
        """ç²—ç•¥æŒ‰å¥å·/æ¢è¡Œåˆ‡å¥ï¼Œå–å‰ n å¥ï¼›å†åšç¡¬æˆªæ–­ä»¥é¿å…è¿‡é•¿ã€‚"""
        if not text: return ""
        # ä»¥ä¸­æ–‡/è‹±æ–‡å¥å·ã€é—®å·ã€å¹å·ã€åˆ†å·ã€æ¢è¡Œåˆ‡åˆ†
        parts = re.split(r"[ã€‚ï¼ï¼Ÿï¼›.!?;\n\r]+", text)
        parts = [p.strip() for p in parts if p.strip()]
        summary = "ã€‚".join(parts[:max(1, n)])
        summary = (summary[:hard_limit] + "â€¦") if len(summary) > hard_limit else summary
        return summary

# ====================== æ„å»ºæ¨é€æ­£æ–‡ï¼ˆé™„æ‘˜è¦+èŠ‚é€‰ï¼‰ ======================
def build_markdown(hr_items: list[dict], tz_str: str, total_limit: int = 20):
    tz = ZoneInfo(tz_str)
    now_dt = datetime.now(tz)
    today_str = now_dt.strftime("%Y-%m-%d")
    wd = zh_weekday(now_dt)

    items = hr_items[:total_limit] if (total_limit and total_limit > 0) else hr_items

    lines = [
        f"**æ—¥æœŸï¼š{today_str}ï¼ˆ{wd}ï¼‰**",
        "",
        f"**æ ‡é¢˜ï¼šæ—©å®‰èµ„è®¯ï½œäººåŠ›èµ„æºæ¯æ—¥èµ„è®¯æ¨é€**",
        "",
        "**ä¸»è¦å†…å®¹**",
    ]
    if not items:
        lines.append("> æš‚æ— æ›´æ–°ã€‚")
        return "\n".join(lines)

    for i, it in enumerate(items, 1):
        title_line = f"{i}. [{it['title']}]({it['url']})"
        if it.get("source"):
            title_line += f"ã€€â€”ã€€*{it['source']}*"
        lines.append(title_line)

        if it.get("summary"):
            lines.append(f"> æ‘˜è¦ï¼š{it['summary']}")
        if it.get("content"):
            # ç»™æ­£æ–‡èŠ‚é€‰å†æ¥ä¸€æ®µï¼Œé¿å…è¿‡é•¿
            excerpt = it["content"][:300].rstrip()
            lines.append(f"> æ­£æ–‡èŠ‚é€‰ï¼š{excerpt}{'â€¦' if len(it['content'])>300 else ''}")
        lines.append("")
    return "\n".join(lines)

# ====================== è¿è¡Œå…¥å£ ======================
def main():
    args = parse_args()
    print(f"[CFG] tz={args.tz} limit={args.limit} only_today={args.only_today} "
          f"max_per_source={args.max_per_source} require_all={args.require_all} "
          f"keywords={args.keywords!r} fetch_detail={os.getenv('HR_FETCH_DETAIL','1')}")

    crawler = HRLooCrawler(
        only_today=args.only_today,
        require_all=args.require_all,
        keywords=split_keywords(args.keywords),
        max_per_source=args.max_per_source,
    )
    crawler.crawl_hrloo()
    items = crawler.results or []
    print(f"[HRLoo] æŠ“åˆ° {len(items)} æ¡ã€‚")

    md = build_markdown(items, args.tz, args.limit)
    print("\n===== Markdown Preview =====\n")
    print(md)

    if not args.no_push:
        ok = send_dingtalk_markdown("æ—©å®‰èµ„è®¯ï½œäººåŠ›èµ„æºèµ„è®¯æ¨é€", md)
        print(f"[Push] DingTalk success={ok}")
    else:
        print("[Push] no-push æ¨¡å¼ï¼Œè·³è¿‡å‘é€ã€‚")

if __name__ == "__main__":
    main()
