# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰Â· ä¸‰èŒ…æ—¥æŠ¥å‡€åŒ–ç‰ˆï¼ˆå½“å¤©ä¸€æ¡ Â· ä¸‰é‡æ—¥æœŸæ ¡éªŒ Â· å…¼å®¹â€œæœ‰/æ— ç¼–å·â€ï¼‰
ç­–ç•¥ï¼š
1ï¼‰å…ˆæŠ½å–æ­£æ–‡ä¸­çš„ strong/h2/h3 ç­‰â€œåŠ ç²—å°æ ‡é¢˜â€ï¼ˆé€‚é…æ— ç¼–å·é¡µé¢ï¼‰
2ï¼‰å†æŠ½å– 1./ï¼ˆ1ï¼‰/1ã€ ç­‰â€œç¼–å·é¡¹â€ï¼ˆé€‚é…æœ‰ç¼–å·é¡µé¢ï¼‰
3ï¼‰æŒ‰ DOM é¡ºåºåˆå¹¶å»é‡ï¼›è¿‡æ»¤è¿è¥/ç¾¤å‘/å®¡æ ¸ç­‰å™ªå£°
"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, date
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ========== æ—¶åŒº ==========
try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo

def _tz(): return ZoneInfo("Asia/Shanghai")
def now_tz(): return datetime.now(_tz())
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]

# ========== é’‰é’‰ ==========
def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASE") or os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRET") or os.getenv("DINGTALK_SECRETA")
    if not base:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASEï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    try:
        r = requests.post(
            _sign_webhook(base, secret),
            json={"msgtype":"markdown","markdown":{"title":title,"text":md}},
            timeout=20
        )
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} code={r.status_code}")
        if not ok: print("resp:", r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e); return False

# ========== ä¼šè¯ ==========
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({"User-Agent":"Mozilla/5.0","Accept-Language":"zh-CN,zh;q=0.9"})
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

# ========== å·¥å…· ==========
CN_TITLE_DATE = re.compile(r"[ï¼ˆ(]\s*(20\d{2})\s*[å¹´\-/.]\s*(\d{1,2})\s*[æœˆ\-/.]\s*(\d{1,2})\s*[)ï¼‰]")
def date_from_bracket_title(text:str):
    m = CN_TITLE_DATE.search(text or "")
    if not m: return None
    try:
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        return date(y, mo, d)
    except: return None

# ========== çˆ¬è™« ==========
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = 1

        t = (os.getenv("HR_TARGET_DATE") or "").strip()
        if t:
            try:
                y,m,d = map(int, re.split(r"[-/\.]", t))
                self.target_date = date(y,m,d)
            except:
                print("âš ï¸ HR_TARGET_DATE è§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šæ—¥ã€‚")
                self.target_date = now_tz().date()
        else:
            self.target_date = now_tz().date()

        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        # é¦–é¡µ + é¢‘é“é¡µåŒå…¥å£
        self.sources = [u.strip() for u in os.getenv(
            "SRC_HRLOO_URLS",
            "https://www.hrloo.com/,https://www.hrloo.com/news/hr"
        ).split(",") if u.strip()]
        print(f"[CFG] target_date={self.target_date} {zh_weekday(now_tz())}  sources={self.sources}")

    def crawl(self):
        for base in self.sources:
            if self._crawl_source(base): break

    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=20)
        except Exception as e:
            print("é¦–é¡µè¯·æ±‚å¼‚å¸¸ï¼š", base, e); return False
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥ï¼š", base, r.status_code); return False

        soup = BeautifulSoup(r.text, "html.parser")

        # æ”¶é›†ç–‘ä¼¼â€œæ—¥æŠ¥â€é“¾æ¥ï¼ˆæ ‡é¢˜å«â€œä¸‰èŒ…æ—¥æŠ¥â€ï¼Œæ‹¬å·æ—¥æœŸç¬¦åˆï¼‰
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href","")
            if not re.search(r"/news/\d+\.html$", href): continue
            text = norm(a.get_text())
            if not self.daily_title_pat.search(text): continue
            t2 = date_from_bracket_title(text)
            if t2 and t2 != self.target_date: continue
            links.append(urljoin(base, href))

        seen = set()
        for url in links:
            if url in seen: continue
            seen.add(url)
            if self._try_detail(url): return True

        print("[MISS] æœ¬æºæœªå‘½ä¸­ç›®æ ‡æ—¥æœŸï¼š", base)
        return False

    def _try_detail(self, abs_url):
        pub_dt, titles, page_title = self._fetch_detail_clean(abs_url)

        # æ ‡é¢˜å¿…é¡»çœŸæ—¥æŠ¥
        if not page_title or not self.daily_title_pat.search(page_title): return False
        if not re.search(r"(äººåŠ›èµ„æºç›¸å…³|ç®€è®¯|æ¯æ—¥è¦ç‚¹|æ—©æŠ¥)", page_title): return False

        # æ—¥æœŸå¤æ ¸
        t3 = date_from_bracket_title(page_title)
        if t3 and t3 != self.target_date: return False
        if pub_dt and pub_dt.date() != self.target_date and not t3: return False

        # æ¡ç›®æ ¡éªŒ
        if not titles or len(titles) < 3:
            print("[SKIP] æ¡ç›®è¿‡å°‘/éæ­£æ–‡ï¼š", page_title)
            return False

        self.results.append({
            "title": page_title,
            "url": abs_url,
            "date": (pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else f"{self.target_date} 09:00"),
            "titles": titles[:10]
        })
        print(f"[HIT] {abs_url} -> {len(titles)} æ¡")
        return True

    def _extract_pub_time(self, soup):
        cand = []
        for t in soup.select("time[datetime]"): cand.append(t.get("datetime",""))
        for m in soup.select("meta[property='article:published_time'],meta[name='pubdate'],meta[name='publishdate']"):
            cand.append(m.get("content",""))
        for sel in [".time",".date",".pubtime",".publish-time",".info"]:
            for x in soup.select(sel):
                cand.append(x.get_text(" ", strip=True))
        pat = re.compile(r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?")
        def parse_one(s):
            m = pat.search(s or "")
            if not m: return None
            try:
                y,mo,d = int(m[1]),int(m[2]),int(m[3])
                hh = int(m[4]) if m[4] else 9
                mm = int(m[5]) if m[5] else 0
                return datetime(y,mo,d,hh,mm,tzinfo=_tz())
            except: return None
        dts = [dt for dt in map(parse_one, cand) if dt]
        if dts:
            now = now_tz()
            past = [dt for dt in dts if dt <= now]
            return min(past or dts, key=lambda dt: abs((now - dt).total_seconds()))
        return None

    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=(6,20))
            if r.status_code != 200:
                print("[DetailFail]", url, r.status_code); return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            title_tag = soup.find(["h1","h2"])
            page_title = norm(title_tag.get_text()) if title_tag else ""
            pub_dt = self._extract_pub_time(soup)

            container = soup.select_one("article, .article, .article-content, .content, .news-content, .detail-content") or soup

            # â€”â€” å…¼å®¹â€œæ— ç¼–å· + æœ‰ç¼–å·â€çš„æŠ½å– â€”â€” #
            titles = self._extract_items_robust(container)

            return pub_dt, titles, page_title
        except Exception as e:
            print("[DetailError]", url, e); return None, [], ""

    # ç»Ÿä¸€çš„é²æ£’æŠ½å–ï¼šå…ˆ headline å numberedï¼Œå†æŒ‰ DOM é¡ºåºåˆå¹¶å»é‡
    def _extract_items_robust(self, root):
        # å™ªå£°è¯
        bad = ["ç¾¤å‘","é»‘åå•","è¿è¥","å®¡æ ¸","å…¥ç¾¤","æ‰«ç ","å¹¿å‘Š","æ¨å¹¿","APP","ç²‰ä¸","çŸ­ä¿¡","éªŒè¯ç ","ç”³è¯‰","å°å·"]
        def is_bad(t): return any(w in t for w in bad)

        # A. æŠ½å–åŠ ç²—/æ ‡é¢˜ç±»ï¼ˆé€‚é…æ— ç¼–å·ï¼‰
        headline_nodes = root.select("h2, h3, p strong, div strong")
        headlines = []
        for n in headline_nodes:
            t = norm(n.get_text()).strip(" ï¼š:ã€.ï¼Œ")
            if 6 <= len(t) <= 60 and not is_bad(t):
                headlines.append((n, t))

        # B. æŠ½å–ç¼–å·ç±»ï¼ˆé€‚é…æœ‰ç¼–å·ï¼‰
        num_pat = re.compile(r"^\s*[ï¼ˆ(]?\s*(\d{1,2})\s*[)ï¼‰]?\s*[ã€.ï¼]?\s*(.+)$")
        numbered = []
        for n in root.find_all(["p","li","div","span","h2","h3","strong"]):
            raw = norm(n.get_text())
            m = num_pat.match(raw or "")
            if not m: continue
            num, txt = int(m.group(1)), m.group(2).strip()
            txt = re.split(r"[ï¼ˆ\(]", txt)[0].strip(" ï¼š:ã€.ï¼Œ")
            if 3 <= len(txt) <= 100 and not is_bad(txt):
                numbered.append((n, txt))

        # C. åˆå¹¶å»é‡ï¼šæŒ‰èŠ‚ç‚¹åœ¨ DOM ä¸­çš„â€œæ–‡æ¡£é¡ºåºâ€æ’åº
        all_nodes = headlines + numbered
        if not all_nodes: return []

        # ç”¨ `sourceline` ä¿åºï¼›æ²¡æœ‰å°±æŒ‰å‡ºç°é¡ºåº
        def position_key(x):
            node, _ = x
            # bs4 è§£æå™¨ä¸ä¸€å®šæä¾› .sourcelineï¼Œè¿™é‡Œåšä¸¤çº§å›é€€
            return getattr(node, "sourceline", None)

        have_pos = all(getattr(n, "sourceline", None) is not None for n, _ in all_nodes)
        items = sorted(all_nodes, key=position_key) if have_pos else all_nodes

        seen, result = set(), []
        for _, txt in items:
            if txt in seen: continue
            seen.add(txt)
            result.append(txt)

        return result[:10]

# ========== Markdown ==========
def build_md(items):
    n = now_tz()
    out = [
        f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰**  ",
        "",
        "**æ ‡é¢˜ï¼šäººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹**  ",
        ""
    ]
    if not items:
        out.append("> æœªå‘ç°å½“å¤©çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚")
        return "\n".join(out)
    it = items[0]
    for j, t in enumerate(it["titles"], 1):
        out.append(f"{j}. {t}  ")
    out.append(f"[æŸ¥çœ‹è¯¦ç»†]({it['url']}) ï¼ˆ{it['date'][:10]}ï¼‰  ")
    return "\n".join(out)

# ========== ä¸»å…¥å£ ==========
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler.pyï¼ˆå½“å¤©ä¸€æ¡ Â· ä¸‰é‡æ—¥æœŸæ ¡éªŒ Â· å…¼å®¹æœ‰/æ— ç¼–å·ï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("äººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹", md)
