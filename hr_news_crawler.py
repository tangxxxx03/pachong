# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰Â· ä¸‰èŒ…æ—¥æŠ¥å‡€åŒ–ç‰ˆï¼ˆå½“å¤©ä¸€æ¡Â·å¼ºæ ‡é¢˜æŠ“å–ï¼‰
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
âœ… ä¸‰é‡æ ¡éªŒé€»è¾‘ï¼ˆä¸»é¡µ/æ ‡é¢˜æ‹¬å·/è¯¦æƒ…æ—¶é—´ï¼‰
âœ… ä»…æŠ“â€œå½“å¤©â€çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ä¸€æ¡
âœ… ä»…ä¿ç•™æ­£æ–‡ä¸»ä½“é‡Œæ¯æ¡æ–°é—»çš„ <strong> æ ‡é¢˜è¡Œï¼ˆä½ æ ‡æ³¨è¦ä¿ç•™çš„é‚£è¡Œï¼‰
âœ… å…¼å®¹ï¼šæœ‰ç¼–å·/æ— ç¼–å· çš„ä¸¤ç§æ¸²æŸ“ï¼ˆä»¥ strong ä¸ºå‡†ï¼Œfallback æ•°å­—å‰ç¼€ï¼‰
âœ… è¿‡æ»¤ï¼šç›¸å…³é˜…è¯»ï¼ˆ.other-wrapï¼‰ã€å…è´£å£°æ˜ï¼ˆ.txtï¼‰ã€ä¸Šä¸€ç¯‡/ä¸‹ä¸€ç¯‡ï¼ˆ.prev/.next btnï¼‰ç­‰å™ªå£°
âœ… è¾“å‡º Markdownï¼›å¯é€‰é’‰é’‰ Markdown æ¨é€ï¼ˆæœªé… webhook åˆ™è·³è¿‡ï¼‰
"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin
from datetime import datetime, date
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ========== æ—¶åŒº ==========
try:
    from zoneinfo import ZoneInfo  # py3.9+
except:
    from backports.zoneinfo import ZoneInfo  # py<3.9

def _tz(): return ZoneInfo("Asia/Shanghai")
def now_tz(): return datetime.now(_tz())
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]

# ========== é’‰é’‰ ==========
def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASE") or os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRET") or os.getenv("DINGTALK_SECRETA")
    if not base:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASEï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    try:
        r = requests.post(
            _sign_webhook(base, secret),
            json={"msgtype":"markdown","markdown":{"title":title,"text":md}},
            timeout=20
        )
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} code={r.status_code}")
        if not ok: print("resp:", r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e); return False

# ========== ä¼šè¯ ==========
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                     "(KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept-Language":"zh-CN,zh;q=0.9"
    })
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

# ========== å·¥å…· ==========
CN_TITLE_DATE = re.compile(r"[ï¼ˆ(]\s*(20\d{2})\s*[å¹´\-/.]\s*(\d{1,2})\s*[æœˆ\-/.]\s*(\d{1,2})\s*[)ï¼‰]")
def date_from_bracket_title(text:str):
    m = CN_TITLE_DATE.search(text or "")
    if not m: return None
    try:
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        return date(y, mo, d)
    except: return None

def looks_like_numbered(text: str) -> bool:
    # å½¢å¦‚ "1. xxx"ã€"1ã€xxx"ã€"ï¼ˆ1ï¼‰xxx" ç­‰
    return bool(re.match(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*\S+", text or ""))

# ========== ä¸»ç±» ==========
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = 1  # å‘½ä¸­å³åœï¼ˆåªä¿ç•™å½“å¤©çš„ä¸€æ¡æ—¥æŠ¥ï¼‰

        # ç›®æ ‡æ—¥æœŸï¼šç¯å¢ƒå˜é‡æŒ‡å®šæˆ–ä»Šæ—¥
        t = (os.getenv("HR_TARGET_DATE") or "").strip()
        if t:
            try:
                y,m,d = map(int, re.split(r"[-/\.]", t))
                self.target_date = date(y,m,d)
            except:
                print("âš ï¸ HR_TARGET_DATE è§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šæ—¥ã€‚")
                self.target_date = now_tz().date()
        else:
            self.target_date = now_tz().date()

        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")  # åªè®¤ä¸‰èŒ…æ—¥æŠ¥
        # åŒæ—¶æ”¯æŒé¦–é¡µå’Œâ€œäººåŠ›èµ„æº/HR å¿«è®¯é¡µâ€
        self.sources = [u.strip() for u in os.getenv("SRC_HRLOO_URLS","https://www.hrloo.com/,https://www.hrloo.com/news/hr").split(",") if u.strip()]
        print(f"[CFG] target_date={self.target_date} {zh_weekday(now_tz())}  sources={self.sources}")

    # â€”â€” å¤–éƒ¨å…¥å£
    def crawl(self):
        for base in self.sources:
            if self._crawl_source(base): break  # å‘½ä¸­å³åœ

    # â€”â€” ä¸»é¡µæ‰«æ
    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=20)
        except Exception as e:
            print("é¦–é¡µè¯·æ±‚å¼‚å¸¸ï¼š", base, e); return False
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥ï¼š", base, r.status_code); return False

        soup = BeautifulSoup(r.text, "html.parser")

        # ä¼˜å…ˆå®¹å™¨ï¼ˆå¸¦ dwdata-time çš„å·¦æ åˆ—è¡¨ï¼‰
        items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
        if items:
            for div in items:
                # â‘  å®¹å™¨ä¸Šçš„ dwdata-time
                dts = (div.get("dwdata-time") or "").strip()
                if dts:
                    try:
                        pub_d = datetime.strptime(dts.split()[0], "%Y-%m-%d").date()
                        if pub_d != self.target_date:
                            continue
                    except: pass
                # æ ‡é¢˜ä¸ url
                a = div.find("a", href=True)
                if not a: continue
                title_text = norm(a.get_text())
                if not self.daily_title_pat.search(title_text):
                    continue
                # â‘¡ æ ‡é¢˜æ‹¬å·æ—¥æœŸ
                t2 = date_from_bracket_title(title_text)
                if t2 and t2 != self.target_date:
                    continue
                # â‘¢ è¯¦æƒ…é¡µå¤æ ¸
                abs_url = urljoin(base, a["href"])
                if self._try_detail(abs_url): return True
            print("[MISS] å®¹å™¨é€šé“æœªå‘½ä¸­ï¼š", base)

        # å¤‡ç”¨ï¼š/news/123456.html çš„é“¾æ¥æ‰«æ
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href","")
            if not re.search(r"/news/\d+\.html$", href): continue
            text = norm(a.get_text())
            if not self.daily_title_pat.search(text): continue
            t2 = date_from_bracket_title(text)
            if t2 and t2 != self.target_date: continue
            links.append(urljoin(base, href))

        seen = set()
        for url in links:
            if url in seen: continue
            seen.add(url)
            if self._try_detail(url): return True

        print("[MISS] æœ¬æºæœªå‘½ä¸­ç›®æ ‡æ—¥æœŸï¼š", base)
        return False

    # â€”â€” è¯¦æƒ…é¡µå¤æ ¸ + æå–
    def _try_detail(self, abs_url):
        pub_dt, titles, page_title = self._fetch_detail_clean(abs_url)
        # åªè®¤â€œä¸‰èŒ…æ—¥æŠ¥â€
        if not page_title or not self.daily_title_pat.search(page_title): return False
        # æ ‡é¢˜æ‹¬å·æ—¥æœŸ
        t3 = date_from_bracket_title(page_title)
        if t3 and t3 != self.target_date: return False
        # è¯¦æƒ…é¡µå‘å¸ƒæ—¶é—´ï¼ˆè‹¥æ²¡æ‹¬å·æ—¥æœŸï¼Œåˆ™ç”¨å®ƒåšç¬¬ä¸‰é“æ ¡éªŒï¼‰
        if pub_dt and pub_dt.date() != self.target_date and not t3: return False
        if not titles: return False

        self.results.append({
            "title": page_title,
            "url": abs_url,
            "date": (pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else f"{self.target_date} 09:00"),
            "titles": titles
        })
        print(f"[HIT] {abs_url} -> {len(titles)} æ¡")
        return True

    # â€”â€” æå–è¯¦æƒ…é¡µæ—¶é—´
    def _extract_pub_time(self, soup: BeautifulSoup):
        cand = []
        # meta/time æ··åˆå°è¯•
        for t in soup.select("time[datetime]"): cand.append(t.get("datetime",""))
        for m in soup.select("meta[property='article:published_time'],meta[name='pubdate'],meta[name='publishdate']"):
            cand.append(m.get("content",""))
        for sel in [".time",".date",".pubtime",".publish-time",".post-time",".info","meta[itemprop='datePublished']"]:
            for x in soup.select(sel):
                if isinstance(x, Tag):
                    cand.append(x.get_text(" ", strip=True))
        # ä¸­æ–‡/æ•°å­—æ—¥æœŸè§£æ
        pat = re.compile(r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?")
        def parse_one(s):
            m = pat.search(s or "")
            if not m: return None
            try:
                y,mo,d = int(m[1]),int(m[2]),int(m[3])
                hh = int(m[4]) if m[4] else 9
                mm = int(m[5]) if m[5] else 0
                return datetime(y,mo,d,hh,mm,tzinfo=_tz())
            except: return None
        dts = [dt for dt in map(parse_one, cand) if dt]
        if dts:
            now = now_tz()
            past = [dt for dt in dts if dt <= now]
            return min(past or dts, key=lambda dt: abs((now - dt).total_seconds()))
        return None

    # â€”â€” è¯¦æƒ…é¡µæŠ“å– + æ­£æ–‡å‡€åŒ–ï¼ˆåªä¿ç•™ strong æ ‡é¢˜ï¼‰
    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=(6, 20))
            if r.status_code != 200:
                print("[DetailFail]", url, r.status_code); return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            # é¡µæ ‡é¢˜
            title_tag = soup.find(["h1","h2"])
            page_title = norm(title_tag.get_text()) if title_tag else ""

            # å‘å¸ƒæ—¶é—´ï¼ˆç¬¬ä¸‰é“æ ¡éªŒç”¨ï¼‰
            pub_dt = self._extract_pub_time(soup)

            # ä¸»ä½“å®¹å™¨ï¼ˆä½ åœˆå‡ºçš„é‚£ä¸ªï¼‰
            container = soup.select_one(
                ".content-con.hr-rich-text.fn-wenda-detail-infomation.fn-hr-rich-text.custom-style-w"
            ) or soup

            # è¿‡æ»¤æ˜æ˜¾ä¸éœ€è¦çš„æ¿å—ï¼ˆç›¸å…³é˜…è¯»/å…è´£å£°æ˜/ä¸Šä¸€ç¯‡-ä¸‹ä¸€ç¯‡ç­‰ï¼‰
            for sel in [
                ".other-wrap",           # ç›¸å…³é˜…è¯»åŒº
                ".txt",                  # å…è´£å£°æ˜â€œæ³¨ï¼šæ–‡ä¸­å†…å®¹â€¦â€
                "a.prev.fn-dataStatistics-btn",  # ä¸Šä¸€ç¯‡
                "a.next.fn-dataStatistics-btn",  # ä¸‹ä¸€ç¯‡
                ".footer",
                ".bottom",
            ]:
                for bad in container.select(sel):
                    bad.decompose()

            titles = self._extract_strong_titles(container)
            # fallbackï¼šæç«¯æƒ…å†µä¸‹ï¼Œé¡µé¢æ²¡æœ‰ strongï¼Œå°è¯•æ•°å­—ç¼–å·è¡Œ
            if not titles:
                titles = self._extract_numbered_titles(container)

            return pub_dt, titles, page_title
        except Exception as e:
            print("[DetailError]", url, e)
            return None, [], ""

    def _extract_strong_titles(self, root: Tag):
        """
        åªä¿ç•™æ­£æ–‡é‡Œæ¯æ¡æ–°é—»çš„ <strong> æ ‡é¢˜æ–‡æœ¬ã€‚
        - å¸¸è§ç»“æ„ï¼š<p><strong>æ ‡é¢˜</strong></p>  æˆ–  <strong>æ ‡é¢˜</strong>
        - å¿½ç•¥ç©º/è¿‡çŸ­/å¹¿å‘Šæ ·æ–‡æœ¬
        """
        keep = []
        # æ–°é—»æ ‡é¢˜å¾€å¾€åœ¨ <h2> åŒºå—å†…éƒ¨çš„ <p>/<strong> é‡Œï¼Œè¿™é‡Œå®½æ¾åŒ¹é…
        for st in root.select("strong"):
            text = norm(st.get_text())
            if not text: continue
            # æ¶ˆå™ªï¼šå¿½ç•¥æ— æ„ä¹‰/è¿‡çŸ­/éæ–°é—»å¥
            if len(text) < 4:  # æ¯”å¦‚â€œç›®å½•â€ç­‰
                continue
            # å¿½ç•¥â€œé˜…è¯»é‡/æ¥æºâ€ç­‰å°¾å·´ï¼ˆä¿é™©å†åˆ‡ä¸€æ¬¡ï¼‰
            text = re.split(r"[ï¼ˆ(]?(é˜…è¯»|é˜…è¯»é‡|æµè§ˆ|æ¥æº)[:ï¼š]\s*\d+.*$", text)[0].strip()
            if not text: continue
            keep.append(text)

        # å»é‡å¹¶ä¿åº
        seen, out = set(), []
        for t in keep:
            if t in seen: continue
            seen.add(t)
            out.append(t)
        return out

    def _extract_numbered_titles(self, root: Tag):
        """
        å…œåº•ï¼šè‹¥é¡µé¢æ²¡æœ‰ <strong>ï¼Œå–å¸¦ç¼–å·çš„æ ‡é¢˜è¡Œï¼ˆ1.  xx / 1ã€xx / ï¼ˆ1ï¼‰xxï¼‰
        """
        out = []
        for p in root.find_all(["p","h2","h3","div","span","li"]):
            text = norm(p.get_text())
            if looks_like_numbered(text):
                # æŠŠç¼–å·å»æ‰åªç•™æ ‡é¢˜
                text = re.sub(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*", "", text)
                # åˆ°â€œï¼ˆâ€å‰æˆªæ–­ï¼Œå»æ‰å¯èƒ½çš„è¡¥å……æ‹¬å·
                text = re.split(r"[ï¼ˆ(]", text)[0].strip()
                if text and len(text) >= 4:
                    out.append(text)
        # å»é‡å¹¶ä¿åº
        seen, final = set(), []
        for t in out:
            if t in seen: continue
            seen.add(t); final.append(t)
        return final

# ========== Markdown ==========
def build_md(items):
    n = now_tz()
    out = [
        f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰**  ",
        "",
        "**æ ‡é¢˜ï¼šäººèµ„æ—¥æŠ¥ï½œæ¯æ—¥è¦ç‚¹**  ",
        ""
    ]
    if not items:
        out.append("> æœªå‘ç°å½“å¤©çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚")
        return "\n".join(out)

    it = items[0]  # å½“å¤©ä»…ä¸€æ¡
    for idx, t in enumerate(it["titles"], 1):
        out.append(f"{idx}. {t}  ")
    out.append(f"[æŸ¥çœ‹è¯¦ç»†]({it['url']})  ")
    return "\n".join(out)

# ========== ä¸»å…¥å£ ==========
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler.pyï¼ˆå½“å¤©ä¸€æ¡ Â· ä¸‰é‡æ—¥æœŸæ ¡éªŒ Â· strong æ ‡é¢˜æå–ï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("äººèµ„æ—¥æŠ¥ï½œæ¯æ—¥è¦ç‚¹", md)
