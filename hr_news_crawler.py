# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰Â· ä¸‰èŒ…æ—¥æŠ¥å‡€åŒ–ç‰ˆï¼ˆå½“å¤©ä¸€æ¡ Â· ä¸‰é‡æ—¥æœŸæ ¡éªŒ Â· å‘½ä¸­å³åœï¼‰
- ä¼˜å…ˆï¼šé¦–é¡µ dwdata-time ç²¾ç¡®åŒ¹é…ç›®æ ‡æ—¥æœŸ
- å…¶æ¬¡ï¼šæ ‡é¢˜æ‹¬å·æ—¥æœŸï¼ˆï¼ˆYYYYå¹´MæœˆDæ—¥ï¼‰æˆ– (YYYY-MM-DD)ï¼‰åŒ¹é…
- å…œåº•ï¼šè¯¦æƒ…é¡µæ—¶é—´æå–åŒ¹é…
- è‡ªåŠ¨å»â€œé˜…è¯»é‡â€ç­‰å°¾å·´ï¼Œä»…è¾“å‡ºå½“å¤©ä¸€æ¡ï¼›å¯é’‰é’‰æ¨é€
"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, date
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ===== æ—¶åŒº/æ—¶é—´ =====
try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo
def _tz(): return ZoneInfo("Asia/Shanghai")
def now_tz(): return datetime.now(_tz())
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]

# ===== é’‰é’‰ =====
def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASE") or os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRET") or os.getenv("DINGTALK_SECRETA")
    if not base:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASEï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    try:
        r = requests.post(
            _sign_webhook(base, secret),
            json={"msgtype":"markdown","markdown":{"title":title,"text":md}},
            timeout=20
        )
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} code={r.status_code}")
        if not ok: print("resp:", r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e); return False

# ===== ä¼šè¯ =====
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)
def make_session():
    s = requests.Session()
    s.headers.update({"User-Agent":"Mozilla/5.0","Accept-Language":"zh-CN,zh;q=0.9"})
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

# ===== å·¥å…·ï¼šä»â€œæ‹¬å·æ ‡é¢˜â€æå–æ—¥æœŸ =====
CN_TITLE_DATE = re.compile(r"[ï¼ˆ(]\s*(20\d{2})\s*[å¹´\-/.]\s*(\d{1,2})\s*[æœˆ\-/.]\s*(\d{1,2})\s*[)ï¼‰]")
def date_from_bracket_title(text:str):
    m = CN_TITLE_DATE.search(text or "")
    if not m: return None
    try:
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        return date(y, mo, d)
    except: return None

# ===== ä¸»ä½“ =====
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = 1  # å‘½ä¸­å³åœ

        # ç›®æ ‡æ—¥æœŸï¼šenv æŒ‡å®šï¼Œå¦åˆ™ä»Šå¤©
        t = (os.getenv("HR_TARGET_DATE") or "").strip()
        if t:
            try:
                y,m,d = map(int, re.split(r"[-/\.]", t))
                self.target_date = date(y,m,d)
            except:
                print("âš ï¸ HR_TARGET_DATE è§£æå¤±è´¥ï¼Œæ”¹ç”¨ä»Šå¤©ã€‚")
                self.target_date = now_tz().date()
        else:
            self.target_date = now_tz().date()

        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        self.sources = [u.strip() for u in os.getenv("SRC_HRLOO_URLS","https://www.hrloo.com/").split(",") if u.strip()]
        print(f"[CFG] target_date={self.target_date} {zh_weekday(now_tz())} sources={self.sources}")

    def crawl(self):
        for base in self.sources:
            if self._crawl_source(base): break  # å‘½ä¸­å³åœ

    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=20)
        except Exception as e:
            print("é¦–é¡µè¯·æ±‚å¼‚å¸¸ï¼š", base, e); return False
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥ï¼š", base, r.status_code); return False

        soup = BeautifulSoup(r.text, "html.parser")

        # A. ä¼˜å…ˆï¼šå®˜æ–¹åˆ—è¡¨å®¹å™¨ï¼ˆæœ‰äº›æƒ…å†µä¸‹ä¸ä¼šä¸‹å‘ç»™éæµè§ˆå™¨ï¼‰
        items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
        if items:
            for div in items:
                # â‘  é¦–é¡µ data-time å¼ºç­›
                dts = (div.get("dwdata-time") or "").strip()
                if dts:
                    try:
                        pub_d = datetime.strptime(dts.split()[0], "%Y-%m-%d").date()
                        if pub_d != self.target_date:
                            continue
                    except: pass
                # â‘¡ æ ‡é¢˜æ‹¬å·æ—¥æœŸæ¬¡ç­›
                a = div.find("a", href=True)
                if not a: continue
                title_text = norm(a.get_text())
                if not self.daily_title_pat.search(title_text):  # å¿…é¡»æ˜¯â€œä¸‰èŒ…æ—¥æŠ¥â€
                    continue
                t2 = date_from_bracket_title(title_text)
                if t2 and t2 != self.target_date:
                    continue

                # è¯¦æƒ…å¤æ ¸
                abs_url = urljoin(base, a["href"])
                if self._try_detail(abs_url):
                    return True
            print("[MISS] å®¹å™¨é€šé“æœªå‘½ä¸­ï¼š", base)

        # B. å¤‡ç”¨ï¼šéå†æ‰€æœ‰æ–°é—»é“¾æ¥ + æ ‡é¢˜æ‹¬å·æ—¥æœŸå¼ºç­›
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href",""); 
            if not re.search(r"/news/\d+\.html$", href): 
                continue
            text = norm(a.get_text())
            if not self.daily_title_pat.search(text):
                continue
            t2 = date_from_bracket_title(text)
            if t2 and t2 != self.target_date:
                continue
            links.append(urljoin(base, href))

        # å»é‡ & é€ä¸ªå¤æ ¸
        seen = set()
        for url in links:
            if url in seen: continue
            seen.add(url)
            if self._try_detail(url):
                return True

        print("[MISS] æœ¬æºæœªå‘½ä¸­ç›®æ ‡æ—¥æœŸï¼š", base)
        return False

    # è¿›å…¥è¯¦æƒ…å¹¶æœ€ç»ˆåˆ¤å®š
    def _try_detail(self, abs_url):
        pub_dt, titles, page_title = self._fetch_detail_clean(abs_url)
        if not page_title or not self.daily_title_pat.search(page_title):
            return False

        # å…ˆç”¨æ ‡é¢˜æ‹¬å·æ—¥æœŸåˆ¤å®šï¼ˆå¾ˆå¤šè¯¦æƒ…é¡µæ—¶é—´å†™åœ¨æ ‡é¢˜é‡Œï¼‰
        t3 = date_from_bracket_title(page_title)
        if t3 and t3 != self.target_date:
            return False

        # å†ç”¨æ­£æ–‡æ—¶é—´åšå…œåº•
        if pub_dt and pub_dt.date() != self.target_date:
            # å¦‚æœæ ‡é¢˜æ²¡æ—¥æœŸæˆ–ä¸ç¡®å®šï¼Œå†ä¸¥æ ¼ç”¨æ­£æ–‡æ—¶é—´
            if not t3:
                return False

        if not titles:
            return False

        self.results.append({
            "title": page_title,
            "url": abs_url,
            "date": (pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else f"{self.target_date} 09:00"),
            "titles": titles
        })
        print(f"[HIT] {abs_url} -> {len(titles)} æ¡")
        return True

    # è¯¦æƒ…é¡µæ—¶é—´ï¼ˆå…œåº•ï¼‰
    def _extract_pub_time(self, soup):
        # å¤šæºå°è¯•ï¼štime/meta/å¸¸è§ç±»å/çº¯æ–‡æœ¬
        cand = []
        for t in soup.select("time[datetime]"):
            cand.append(t.get("datetime",""))
        for m in soup.select("meta[property='article:published_time'], meta[name='pubdate'], meta[name='publishdate']"):
            cand.append(m.get("content",""))
        for sel in [".time",".date",".pubtime",".post-time",".publish-time",".info"]:
            for x in soup.select(sel):
                cand.append(x.get_text(" ", strip=True))

        pat = re.compile(r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?")
        def parse_one(s):
            m = pat.search(s or "")
            if not m: return None
            try:
                y,mo,d = int(m[1]),int(m[2]),int(m[3])
                hh = int(m[4]) if m[4] else 9
                mm = int(m[5]) if m[5] else 0
                return datetime(y,mo,d,hh,mm,tzinfo=_tz())
            except: return None

        dts = [parse_one(x) for x in cand if x]
        dts = [dt for dt in dts if dt]
        if dts: 
            # å–æœ€æ¥è¿‘ç°åœ¨ä¸”ä¸åœ¨æœªæ¥
            now = now_tz()
            past = [dt for dt in dts if dt <= now]
            return min(past or dts, key=lambda dt: abs((now - dt).total_seconds()))

        # å½»åº•æ²¡æœ‰å°±è¿”å› Noneï¼Œè®©ä¸Šæ¸¸ç”¨æ ‡é¢˜æ—¥æœŸåˆ¤å®š
        return None

    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=(6,20))
            if r.status_code != 200:
                print("[DetailFail]", url, r.status_code); return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            title_tag = soup.find(["h1","h2"])
            page_title = norm(title_tag.get_text()) if title_tag else ""
            pub_dt = self._extract_pub_time(soup)
            container = soup.select_one(
                "article, .article, .article-content, .detail-content, .news-content, .content, .post-content"
            ) or soup
            titles = self._extract_daily_item_titles(container)
            return pub_dt, titles, page_title
        except Exception as e:
            print("[DetailError]", url, e); return None, [], ""

    # åªä¿ç•™â€œ1/2/3/â€¦â€ç¼–å·æ ‡é¢˜ï¼Œå»æ‰å¹¿å‘Š/é˜…è¯»é‡å°¾å·´
    def _extract_daily_item_titles(self, root):
        ad_words = ["æ‰‹æœº","å¢ƒå¤–","çŸ­ä¿¡","éªŒè¯ç ","å®¡æ ¸","ç²‰ä¸","å…¥ç¾¤","è´¦å·","APP","ç™»å½•","æ¨å¹¿","å¹¿å‘Š","åˆ›å»ºç”³è¯·","åè®®","å…³æ³¨","ç”³è¯‰","ä¸‹è½½","ç½‘ç›˜","å¤±ä¿¡","å°å·"]
        def strip_views(title:str)->str:
            t = title
            t = re.sub(r"(?:^|[\sÂ·|ï½œ:-])\s*\d+(?:\.\d+)?\s*(?:k|K|ä¸‡)?\s*(?:æ¬¡)?é˜…è¯»\s*$", "", t)
            t = re.sub(r"(?:é˜…è¯»é‡)\s*[:ï¼š]?\s*\d+(?:\.\d+)?\s*(?:k|K|ä¸‡)?\s*$", "", t)
            t = re.sub(r"\s*é˜…è¯»\s*$", "", t)
            return t.strip(" ã€ï¼Œ,.;ã€‚ï¼›|-â€”~â€¦ ")

        by_num = {}
        for node in root.find_all(["h2","h3","h4","strong","b","p","li","span","div"]):
            raw = (node.get_text() or "").strip()
            if not raw: continue
            m = re.match(r"^\s*[ï¼ˆ(]?\s*(\d{1,2})\s*[)ï¼‰]?\s*[ã€.ï¼]?\s*(.+)$", raw)
            if not m: continue
            num, txt = int(m.group(1)), m.group(2).strip()
            if num >= 10 or txt.startswith("æ—¥ï¼Œ") or txt.startswith("æ—¥ "): continue
            if any(w in txt for w in ad_words): continue
            title = re.split(r"[ï¼ˆ\(]{1}", txt)[0].strip()
            title = strip_views(title)
            if not (4 <= len(title) <= 80): continue
            zh_ratio = len(re.findall(r"[\u4e00-\u9fa5]", title)) / max(len(title),1)
            if zh_ratio < 0.3: continue
            by_num.setdefault(num, title)

        seq, n = [], 1
        while n in by_num:
            seq.append(by_num[n]); n += 1
            if n > 20: break
        return seq[:10]

# ===== Markdown =====
def build_md(items):
    n = now_tz()
    out = [
        f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰**  ",
        "",
        "**æ ‡é¢˜ï¼šäººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹**  ",
        ""
    ]
    if not items:
        out.append("> æœªå‘ç°å½“å¤©çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚")
        return "\n".join(out)
    it = items[0]
    for j, t in enumerate(it["titles"], 1):
        out.append(f"{j}. {t}  ")
    out.append(f"[æŸ¥çœ‹è¯¦ç»†]({it['url']}) ï¼ˆ{it['date'][:10]}ï¼‰  ")
    out.append("")
    return "\n".join(out)

# ===== å…¥å£ =====
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler_daily_clean_adfree.pyï¼ˆä¸‰é‡æ—¥æœŸæ ¡éªŒÂ·å½“å¤©ä¸€æ¡ï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("äººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹", md)
