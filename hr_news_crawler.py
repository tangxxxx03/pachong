# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰Â· ä¸‰èŒ…æ—¥æŠ¥å‡€åŒ–ç‰ˆï¼ˆåªæŠ“å½“å¤©ä¸€æ¡ï½œå»é˜…è¯»é‡ï½œå‘½ä¸­å³åœï¼‰

åŠŸèƒ½è¦ç‚¹
- ä»…æŠ“å–â€œä¸‰èŒ…æ—¥æŠ¥â€å½“æ—¥é‚£ä¸€æ¡èµ„è®¯ï¼ˆæˆ– HR_TARGET_DATE æŒ‡å®šçš„é‚£ä¸€å¤©ï¼‰
- ä»æ­£æ–‡æå–ç¼–å·æ ‡é¢˜ï¼ˆ1ã€2ã€3ã€â€¦ï¼‰ï¼Œè‡ªåŠ¨å‰”é™¤å¹¿å‘Š/æç¤º/é˜…è¯»é‡å°¾å·´
- å‘½ä¸­åç«‹å³åœæ­¢ç»§ç»­æŠ“å–ï¼Œé¿å…æ··å…¥å…¶å®ƒæ—¥æœŸ
- è¾“å‡º Markdownï¼Œå¯æ¨é€é’‰é’‰

ç¯å¢ƒå˜é‡ï¼ˆGitHub Actions env:ï¼‰
- HR_ONLY_TODAY: "1"  â†’ åªè¦è„šæœ¬è¿è¡Œå½“å¤©ï¼ˆé»˜è®¤å¼€å¯ï¼‰
- HR_TARGET_DATE: "YYYY-MM-DD" â†’ æŒ‡å®šç›®æ ‡æ—¥æœŸï¼ˆè®¾ç½®åä¼˜å…ˆç”Ÿæ•ˆï¼‰
- HR_MAX_ITEMS: é»˜è®¤ "1"ï¼ˆè¿™é‡Œä»ä¿ç•™ï¼ŒåšåŒä¿é™©ï¼‰
- SRC_HRLOO_URLS: ç«™ç‚¹åˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼Œé»˜è®¤ "https://www.hrloo.com/"
- DINGTALK_BASE / DINGTALK_SECRETï¼ˆæˆ– *_BASEA / *_SECRETAï¼‰ç”¨äºæ¨é€

å»ºè®® Actions é…ç½®ï¼š
  HR_ONLY_TODAY: "1"
  HR_MAX_ITEMS:  "1"
  SRC_HRLOO_URLS: "https://www.hrloo.com/"
"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta, date
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ========= æ—¶åŒº/æ—¶é—´ =========
try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo

def _tz(): return ZoneInfo("Asia/Shanghai")
def now_tz(): return datetime.now(_tz())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())

# ========= é’‰é’‰æ¨é€ =========
def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASE") or os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRET") or os.getenv("DINGTALK_SECRETA")
    if not base:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASEï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    try:
        r = requests.post(
            _sign_webhook(base, secret),
            json={"msgtype": "markdown", "markdown": {"title": title, "text": md}},
            timeout=20
        )
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} code={r.status_code}")
        if not ok:
            print("resp:", r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

# ========= ç½‘ç»œä¼šè¯ =========
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({"User-Agent": "Mozilla/5.0", "Accept-Language": "zh-CN,zh;q=0.9"})
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500, 502, 503, 504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

# ========= çˆ¬è™«ä¸»ä½“ =========
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []

        # é»˜è®¤å°±åªæŠ“å½“å¤©ä¸€æ¡ï¼›å¦‚éœ€å®½æ¾å¯åœ¨ env è¦†ç›–
        self.max_items = int(os.getenv("HR_MAX_ITEMS", "1") or "1")
        self.detail_timeout = (6, 20)
        self.detail_sleep = 0.6

        # ä»…å½“å¤© or æŒ‡å®šç›®æ ‡æ—¥æœŸ
        # è‹¥è®¾ç½®äº† HR_TARGET_DATEï¼Œåˆ™ä¸¥æ ¼åªå–è¯¥æ—¥ï¼›å¦åˆ™ HR_ONLY_TODAY é»˜è®¤å¼€å¯
        self.only_today = (os.getenv("HR_ONLY_TODAY", "1") == "1")
        target = (os.getenv("HR_TARGET_DATE") or "").strip()
        self.target_date = None
        if target:
            try:
                y, m, d = map(int, re.split(r"[-/\.]", target))
                self.target_date = date(y, m, d)
            except:
                print("âš ï¸ HR_TARGET_DATE æ— æ³•è§£æï¼Œå¿½ç•¥ã€‚")

        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        self.sources = [u.strip() for u in os.getenv("SRC_HRLOO_URLS", "https://www.hrloo.com/").split(",") if u.strip()]

    def crawl(self):
        for base in self.sources:
            self._crawl_source(base)
            if self.results:  # å‘½ä¸­å³åœ
                break

    def _crawl_source(self, base):
        r = self.session.get(base, timeout=20)
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥ï¼š", base, r.status_code)
            return
        soup = BeautifulSoup(r.text, "html.parser")

        # å…ˆæŒ‘æ ‡é¢˜é‡Œå°±å¸¦â€œä¸‰èŒ…æ—¥æŠ¥â€çš„
        links = []
        for a in soup.select("a[href*='/news/']"):
            href, text = a.get("href", ""), norm(a.get_text())
            if re.search(r"/news/\d+\.html$", href) and self.daily_title_pat.search(text):
                links.append(urljoin(base, href))

        # å…œåº•ï¼šæ²¡åœ¨é¦–é¡µæ ‡é¢˜å‘½ä¸­ï¼Œä¹ŸæŠŠæ–°é—»è¯¦æƒ…é“¾æ¥æ”¶é›†ï¼Œäº¤ç»™è¯¦æƒ…é¡µåˆ¤å®š
        if not links:
            links = [urljoin(base, a.get("href"))
                     for a in soup.select("a[href*='/news/']")
                     if re.search(r"/news/\d+\.html$", a.get("href", ""))]

        seen = set()
        for url in links:
            if url in seen:
                continue
            seen.add(url)

            pub_dt, titles, main = self._fetch_detail_clean(url)
            if not main or not self.daily_title_pat.search(main):
                continue
            if not pub_dt:
                continue

            pub_d = pub_dt.date()

            # â€”â€” ç›®æ ‡æ—¥æœŸè¿‡æ»¤ â€”â€” #
            if self.target_date:
                if pub_d != self.target_date:
                    continue
            elif self.only_today:
                if pub_d != now_tz().date():
                    continue
            else:
                # å®½æ¾çª—å£ï¼š36hï¼ˆé€šå¸¸ä¸ä¼šèµ°åˆ°è¿™é‡Œï¼Œå› ä¸ºé»˜è®¤ only_today å·²å¼€å¯ï¼‰
                if (now_tz() - pub_dt).total_seconds() > 36 * 3600:
                    continue

            if not titles:
                continue

            self.results.append({
                "title": main,
                "url": url,
                "date": pub_dt.strftime("%Y-%m-%d %H:%M"),
                "titles": titles
            })
            print(f"[OK] {url} -> {len(titles)} æ¡")
            break  # å‘½ä¸­è¯¥ç«™ç‚¹çš„å½“å¤©ä¸‰èŒ…æ—¥æŠ¥åç«‹å³åœæ­¢
            # è‹¥å¸Œæœ›åŒç«™ç‚¹è¿˜æœ‰å…¶å®ƒâ€œåˆ†æ—¶å‘å¸ƒâ€çš„åŒæ—¥æ—¥æŠ¥ï¼Œå¯æŠŠä¸Šè¡Œ break å»æ‰ï¼Œå¹¶é  max_items æ§åˆ¶

    # â€”â€” ç¨³å¥å‘å¸ƒæ—¶é—´æå– â€”â€” #
    def _extract_pub_time(self, soup):
        cand_texts = []

        # <time datetime="...">
        for t in soup.select("time[datetime]"):
            cand_texts.append(t.get("datetime", ""))

        # å¸¸è§ç±»å
        for sel in [".time", ".date", ".pubtime", ".post-time", ".publish-time"]:
            for x in soup.select(sel):
                cand_texts.append(x.get_text(" ", strip=True))

        # meta
        for m in soup.select("meta[property='article:published_time'], meta[name='pubdate'], meta[name='publishdate']"):
            cand_texts.append(m.get("content", ""))

        pat = re.compile(r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?")

        def parse_one(s):
            m = pat.search(s or "")
            if not m:
                return None
            y, mo, d = int(m[1]), int(m[2]), int(m[3])
            hh = int(m[4]) if m[4] else 9
            mm = int(m[5]) if m[5] else 0
            try:
                return datetime(y, mo, d, hh, mm, tzinfo=_tz())
            except:
                return None

        cand_dt = [parse_one(txt) for txt in cand_texts]
        cand_dt = [dt for dt in cand_dt if dt is not None]

        if not cand_dt:
            for m in pat.finditer(soup.get_text(" ")):
                try:
                    y, mo, d = int(m[1]), int(m[2]), int(m[3])
                    hh = int(m[4]) if m[4] else 9
                    mm = int(m[5]) if m[5] else 0
                    cand_dt.append(datetime(y, mo, d, hh, mm, tzinfo=_tz()))
                except:
                    pass

        if cand_dt:
            now = now_tz()
            past = [dt for dt in cand_dt if dt <= now]
            return min(past or cand_dt, key=lambda dt: abs((now - dt).total_seconds()))

        n = now_tz()
        return datetime(n.year, n.month, n.day, 9, 0, tzinfo=_tz())

    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=self.detail_timeout)
            if r.status_code != 200:
                print("[DetailFail]", url, r.status_code)
                return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            title_tag = soup.find(["h1", "h2"])
            page_title = norm(title_tag.get_text()) if title_tag else ""
            pub_dt = self._extract_pub_time(soup)

            container = soup.select_one(
                "article, .article, .article-content, .detail-content, .news-content, .content, .post-content"
            ) or soup

            titles = self._extract_daily_item_titles(container)
            return pub_dt, titles, page_title
        except Exception as e:
            print("[DetailError]", url, e)
            return None, [], ""

    # â€”â€” åªä¿ç•™çœŸæ­£æ–°é—»æ ‡é¢˜ï¼Œå‰”é™¤å¹¿å‘Šæç¤ºä¸â€œé˜…è¯»é‡â€ â€”â€” #
    def _extract_daily_item_titles(self, root):
        ad_words = [
            "æ‰‹æœº","å¢ƒå¤–","çŸ­ä¿¡","éªŒè¯ç ","å®¡æ ¸","ç²‰ä¸","å…¥ç¾¤","è´¦å·","APP","ç™»å½•",
            "æ¨å¹¿","å¹¿å‘Š","åˆ›å»ºç”³è¯·","åè®®","å…³æ³¨","ç”³è¯‰","ä¸‹è½½","ç½‘ç›˜","å¤±ä¿¡","å°å·"
        ]

        def strip_views(title: str) -> str:
            t = title
            # ç»Ÿä¸€ä¸€ä¸‹åˆ†éš”ç‚¹
            t = re.sub(r"[Â·â€¢Î‡â€§âˆ™â‹…ãƒ»â—â—¦]\s*", " Â· ", t).strip()
            # å»æ‰â€œ Â· 5k é˜…è¯» / 1200 æ¬¡é˜…è¯» / 1.2ä¸‡é˜…è¯» / é˜…è¯»é‡ï¼šxxx / â€¦é˜…è¯»â€
            t = re.sub(r"(?:^|[\sÂ·|ï½œ:-])\s*\d+(?:\.\d+)?\s*(?:k|K|ä¸‡)?\s*(?:æ¬¡)?é˜…è¯»\s*$", "", t)
            t = re.sub(r"(?:é˜…è¯»é‡)\s*[:ï¼š]?\s*\d+(?:\.\d+)?\s*(?:k|K|ä¸‡)?\s*$", "", t)
            t = re.sub(r"\s*é˜…è¯»\s*$", "", t)  # å…œåº•ï¼šå°¾éƒ¨å­¤ç«‹â€œé˜…è¯»â€
            return t.strip(" ã€ï¼Œ,.;ã€‚ï¼›|-â€”~â€¦ ")

        by_num = {}
        for node in root.find_all(["h2","h3","h4","strong","b","p","li","span","div"]):
            raw = (node.get_text() or "").strip()
            if not raw:
                continue

            m = re.match(r"^\s*[ï¼ˆ(]?\s*(\d{1,2})\s*[)ï¼‰]?\s*[ã€.ï¼]?\s*(.+)$", raw)
            if not m:
                continue

            num, txt = int(m.group(1)), m.group(2).strip()
            if num >= 10 or txt.startswith("æ—¥ï¼Œ") or txt.startswith("æ—¥ "):
                continue
            if any(w in txt for w in ad_words):
                continue

            # å»æ‰æ‹¬å·åçš„è§£é‡Šï¼Œåªä¿ç•™ä¸»æ ‡é¢˜
            title = re.split(r"[ï¼ˆ\(]{1}", txt)[0].strip()
            title = strip_views(title)  # â† åˆ é™¤é˜…è¯»é‡å°¾å·´

            if not (4 <= len(title) <= 80):
                continue

            zh_ratio = len(re.findall(r"[\u4e00-\u9fa5]", title)) / max(len(title), 1)
            if zh_ratio < 0.3:
                continue

            by_num.setdefault(num, title)

        seq, n = [], 1
        while n in by_num:
            seq.append(by_num[n])
            n += 1
            if n > 20:
                break
        return seq[:10]

# ========= Markdown è¾“å‡º =========
def build_md(items):
    n = now_tz()
    out = [
        f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰**  ",
        "",
        "**æ ‡é¢˜ï¼šäººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹**  ",
        ""
    ]
    if not items:
        out.append("> æœªå‘ç°æ–°çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚")
        return "\n".join(out)

    for it in items:
        for j, t in enumerate(it["titles"], 1):
            out.append(f"{j}. {t}  ")
        real_date = (it.get("date") or "")[:10]
        out.append(f"[æŸ¥çœ‹è¯¦ç»†]({it['url']}) ï¼ˆ{real_date}ï¼‰  ")
        out.append("")
    return "\n".join(out)

# ========= ä¸»å…¥å£ =========
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler_daily_clean_adfree.pyï¼ˆåªæŠ“å½“å¤©ä¸€æ¡ï½œå»é˜…è¯»é‡ï½œå‘½ä¸­å³åœï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("äººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹", md)
