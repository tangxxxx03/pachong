# -*- coding: utf-8 -*-
"""
HRLooï¼ˆä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼‰Â· ä¸‰èŒ…æ—¥æŠ¥ä¸“æŠ“ Â· æ¡ç›®èšåˆå‡€åŒ–ç‰ˆ

åŠŸèƒ½ï¼š
  1) ä»…æŠ“å–æ ‡é¢˜åŒ…å«ã€Œä¸‰èŒ…æ—¥æŠ¥/ä¸‰èŒ…æ—¥å ±ã€çš„æ–°é—»ï¼›
  2) åœ¨æ­£æ–‡å®¹å™¨ä¸­æå–ç¼–å·æ¡ç›®æ ‡é¢˜ï¼ˆ1ã€2ã€3ã€â€¦ï¼‰ï¼ŒæŒ‰ç¼–å·èšåˆï¼Œé¿å…è¢«â€œ28æ—¥ï¼Œæ®æŠ¥é“â€¦â€è¯¯åˆ¤ï¼›
  3) ä¿®å¤â€œï¼šâ€åçš„æ ‡é¢˜è¢«æˆªæ–­é—®é¢˜ï¼Œåªå»æ‹¬æ³¨ï¼›
  4) æ’é™¤å³æ /ç«™åŠ¡å™ªå£°ï¼›è¾“å‡º Markdownï¼Œå¯æ¨é€é’‰é’‰ã€‚

ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰ï¼š
  HR_ONLY_TODAY=1           # ä»…å½“å¤©ï¼ˆå¦åˆ™è¿‘24å°æ—¶ï¼‰
  HR_MAX_ITEMS=15
  SRC_HRLOO_URLS=https://www.hrloo.com/   # å¯é€—å·åˆ†éš”å¤šä¸ªå…¥å£é¡µ
  DINGTALK_BASE / DINGTALK_SECRET
  DINGTALK_BASEA / DINGTALK_SECRETA
"""

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ========= åŸºæœ¬å·¥å…· =========
try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo  # æ—§ç‰ˆ Python å…¼å®¹

def _tz(): return ZoneInfo("Asia/Shanghai")
def now_tz(): return datetime.now(_tz())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())
def within_24h(dt): return (now_tz() - dt).total_seconds() <= 86400 if dt else False
def same_day(dt): return bool(dt) and dt.astimezone(_tz()).date() == now_tz().date()

# ========= é’‰é’‰æ¨é€ =========
def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()))
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = os.getenv("DINGTALK_BASE") or os.getenv("DINGTALK_BASEA")
    secret = os.getenv("DINGTALK_SECRET") or os.getenv("DINGTALK_SECRETA")
    if not base:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASE/BASEAï¼Œè·³è¿‡æ¨é€ã€‚"); return False
    try:
        r = requests.post(_sign_webhook(base, secret),
                          json={"msgtype":"markdown","markdown":{"title":title,"text":md}}, timeout=20)
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} code={r.status_code}")
        if not ok: print("resp:", r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e); return False

# ========= ç½‘ç»œä¼šè¯ =========
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/123.0 Safari/537.36",
                      "Accept-Language":"zh-CN,zh;q=0.9,en;q=0.8"})
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

# ========= ä¸»çˆ¬è™« =========
class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = int(os.getenv("HR_MAX_ITEMS","15") or "15")
        self.detail_timeout = (6, 20)
        self.detail_sleep = 0.6
        self.only_today = (os.getenv("HR_ONLY_TODAY","0") == "1")
        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")

        src = os.getenv("SRC_HRLOO_URLS","https://www.hrloo.com/").strip()
        self.sources = [u.strip() for u in src.split(",") if u.strip()]

    def crawl(self):
        for base in self.sources:
            try:
                self._crawl_source(base)
            except Exception as e:
                print(f"[SourceError] {base} -> {e}")

    def _crawl_source(self, base):
        r = self.session.get(base, timeout=20)
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥", base, r.status_code); return
        soup = BeautifulSoup(r.text, "html.parser")

        # ä¼˜å…ˆï¼ša æ–‡æœ¬åŒ…å«â€œä¸‰èŒ…æ—¥æŠ¥â€
        links = []
        for a in soup.select("a[href*='/news/']"):
            href, text = a.get("href",""), norm(a.get_text())
            if re.search(r"/news/\d+\.html$", href) and self.daily_title_pat.search(text):
                links.append(urljoin(base, href))

        # å…œåº•ï¼šæŠ“å…¨éƒ¨ news é“¾æ¥ï¼Œè¿›è¯¦æƒ…å†åˆ¤å®š
        if not links:
            links = [urljoin(base, a.get("href"))
                     for a in soup.select("a[href*='/news/']")
                     if re.search(r"/news/\d+\.html$", a.get("href",""))]

        seen = set()
        for url in links:
            if url in seen: continue
            seen.add(url)

            pub_dt, item_titles, main_title = self._fetch_detail_clean(url)
            if not main_title or not self.daily_title_pat.search(main_title): continue

            # æ—¶é—´è¿‡æ»¤
            if self.only_today:
                if not same_day(pub_dt): continue
            else:
                if not within_24h(pub_dt): continue

            if not item_titles: continue

            self.results.append({
                "title": norm(main_title),
                "url": url,
                "date": pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else "",
                "titles": item_titles
            })
            print(f"[OK] {url} -> {len(item_titles)} æ¡")
            if len(self.results) >= self.max_items: break
            time.sleep(self.detail_sleep)

    # â€”â€” æ˜ç»†é¡µæŠ½å–ï¼ˆæ­£æ–‡å®¹å™¨ + æ¡ç›®èšåˆï¼‰ â€”â€” #
    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=self.detail_timeout)
            if r.status_code != 200: return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            # æ ‡é¢˜ & å‘å¸ƒæ—¶é—´
            h = soup.find(["h1","h2"])
            page_title = norm(h.get_text()) if h else ""
            pub_dt = self._extract_pub_time(soup)

            # ä»…åœ¨æ­£æ–‡å®¹å™¨å†…æŠ½å–ï¼ˆå‘½ä¸­å…¶ä¸€å³å¯ï¼‰
            container = soup.select_one(
                "article, .article, .article-content, .detail-content, .news-content, .content, .post-content"
            ) or soup

            item_titles = self._extract_daily_item_titles(container)
            return pub_dt, item_titles, page_title
        except Exception as e:
            print("[DetailError]", url, e)
            return None, [], ""

    def _extract_pub_time(self, soup):
        txt = soup.get_text(" ")
        m = re.search(r"(20\d{2})[-/.å¹´](\d{1,2})[-/.æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?", txt)
        if not m: return None
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        hh = int(m[4]) if m[4] else 9
        mm = int(m[5]) if m[5] else 0
        try:
            return datetime(y, mo, d, hh, mm, tzinfo=_tz())
        except:
            return None

    # â€”â€” æ ¸å¿ƒï¼šæå–å¹¶èšåˆâ€œ1ã€2ã€3ã€â€¦â€çš„æ¡ç›®æ ‡é¢˜ â€”â€” #
    def _extract_daily_item_titles(self, root):
        # ç«™åŠ¡/å³æ å™ªå£°é»‘è¯
        blacklist = ["ç²‰ä¸","å…¥ç¾¤","ç”³è¯‰","çŸ­ä¿¡","éªŒè¯ç ","ä¸¾æŠ¥","å®¡æ ¸","å‘å¸ƒåè®®",
                     "è´¦å·","ç™»å½•","APP","åˆ›å»ºç”³è¯·","å¹¿å‘Š","ä¸‹è½½","æ¨å¹¿","ç¤¼åŒ…","ç½‘ç›˜"]

        # 1) æ”¶é›†å€™é€‰ï¼šåªæ¥å— 1..9 çš„ç¼–å·ï¼›è¿‡æ»¤â€œ28æ—¥ï¼Œæ®æŠ¥é“â€¦â€è¿™ç±»æ—¥æœŸå‹è¯¯åˆ¤
        by_num = {}  # num -> é¦–æ¬¡å‡ºç°çš„æ ‡é¢˜
        for t in root.find_all(["h2","h3","h4","strong","b","p","li","span","div"]):
            raw = (t.get_text() or "").strip()
            if not raw:
                continue
            m = re.match(r"^\s*[ï¼ˆ(]?\s*(\d{1,2})\s*[)ï¼‰]?\s*[ã€.ï¼]?\s*(.+)$", raw)
            if not m:
                continue

            num = int(m.group(1))
            txt = m.group(2).strip()

            # â€”â€” æ—¥æœŸå‹è¯¯åˆ¤ï¼šå¦‚â€œ28æ—¥ï¼Œâ€â€œ28æ—¥ â€ï¼Œç›´æ¥è·³è¿‡
            if num >= 10 or txt.startswith("æ—¥ï¼Œ") or txt.startswith("æ—¥ "):
                continue

            # æ ‡é¢˜ä¿ç•™å†’å·åçš„ä¸»ä½“ï¼Œåªå»æ‰åç»­æ‹¬æ³¨/å°¾éƒ¨è¯´æ˜
            title = re.split(r"[ï¼ˆ\(]{1}", txt)[0].strip()

            # åŸºç¡€è¿‡æ»¤
            if not (1 <= num <= 9):    # æ—¥æŠ¥é€šå¸¸ 1~5/1~9
                continue
            if not (4 <= len(title) <= 60):
                continue
            zh_ratio = len(re.findall(r"[\u4e00-\u9fa5]", title)) / max(len(title), 1)
            if zh_ratio < 0.3:
                continue
            if any(k in title for k in blacklist):
                continue

            by_num.setdefault(num, title)  # åŒä¸€ç¼–å·åªå–ç¬¬ä¸€æ¬¡

        # 2) ä» 1 å¼€å§‹æŒ‰ç¼–å·èšåˆï¼Œé‡ç¼ºä½å³åœæ­¢ï¼ˆä¿è¯è¿ç»­ 1â†’2â†’â€¦ï¼‰
        seq = []
        n = 1
        while n in by_num:
            seq.append(by_num[n])
            n += 1
            if n > 20:
                break
        return seq[:15]

# ========= Markdown è¾“å‡º =========
def build_md(items):
    n = now_tz()
    out = []
    out.append(f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰**  ")
    out.append("")
    out.append("**æ ‡é¢˜ï¼šã€Œäººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹ã€**  ")
    out.append("")
    if not items:
        out.append("> æŒ‡å®šæ—¶é—´èŒƒå›´å†…æœªå‘ç°æ–°çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚")
        return "\n".join(out)

    for i, it in enumerate(items, 1):
        out.append(f"{i}. [{it['title']}]({it['url']}) ï¼ˆ{it['date']}ï¼‰  ")
        for j, t in enumerate(it['titles'], 1):
            out.append(f"> {j}. {t}  ")
        out.append("")
    return "\n".join(out)

# ========= ä¸»å…¥å£ =========
if __name__ == "__main__":
    print("æ‰§è¡Œ hr_news_crawler_daily_clean.pyï¼ˆåªæŠ“ä¸‰èŒ…æ—¥æŠ¥ Â· æ¡ç›®èšåˆå‡€åŒ–ç‰ˆï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("ã€Œäººèµ„æ—©æŠ¥ï½œæ¯æ—¥è¦ç‚¹ã€", md)
