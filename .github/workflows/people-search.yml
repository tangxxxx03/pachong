name: HR Search (Auto Range: Yesterday / Mon-Last3Days)

on:
  workflow_dispatch:
  schedule:
    - cron: "15 1 * * *"   # æ¯å¤© 09:15 CST (UTC+8) è¿è¡Œ

concurrency:
  group: hr-search-auto-range
  cancel-in-progress: false

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 urllib3

      - name: Debug env (optional)
        run: |
          date -u
          env | grep -i proxy || true

      # å†™å…¥å®Œæ•´è„šæœ¬ï¼šæ˜¨å¤©ä¸“æŠ“ + å‘¨ä¸€è‡ªåŠ¨æŠ“å‰ä¸‰å¤©ï¼ˆå¯é…ç½®ï¼‰
      - name: Write script hr_search_auto_range.py
        run: |
          cat > hr_search_auto_range.py <<'PY'
          # -*- coding: utf-8 -*-
          """
          hr_search_auto_range.py â€”â€” æ˜¨å¤©/å‘¨ä¸€è‡ªåŠ¨ä¸‰å¤© ä¸“æŠ“ç‰ˆ
          è§„åˆ™ï¼š
            - é»˜è®¤â€œè‡ªåŠ¨èŒƒå›´â€ï¼šä»Šå¤©æ˜¯å‘¨ä¸€ â†’ æŠ“å‰ä¸‰å¤©ï¼›å¦åˆ™æŠ“æ˜¨å¤©
            - ä»æ”¯æŒï¼š
                --date yesterday / YYYY-MM-DD   # æŒ‡å®šæŸå¤©
                --window-hours N                # æ»šåŠ¨çª—å£ï¼ˆä»…å½“ --date ä¸ºç©ºä¸” --auto-range=falseï¼‰
          ä¾èµ–ï¼šrequests beautifulsoup4 urllib3
          """

          import re, os, time, hmac, base64, hashlib, argparse
          from dataclasses import dataclass
          from typing import List, Tuple, Optional
          from urllib.parse import urljoin, urlencode, urlparse, quote
          from datetime import datetime, timedelta

          try:
              from zoneinfo import ZoneInfo  # Py3.9+
          except Exception:  # pragma: no cover
              from backports.zoneinfo import ZoneInfo

          import requests
          from requests.adapters import HTTPAdapter
          from urllib3.util.retry import Retry
          from bs4 import BeautifulSoup

          # ===================== å…¨å±€é…ç½® =====================
          TZ = ZoneInfo("Asia/Shanghai")
          UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/123.0.0.0 Safari/537.36")

          # é»˜è®¤é’‰é’‰ï¼ˆå¯è¢«ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
          DEFAULT_WEBHOOK = (
              "https://oapi.dingtalk.com/robot/send?"
              "access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
          )
          DEFAULT_SECRET = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"

          def _first_env(*keys: str, default: str = "") -> str:
              for k in keys:
                  v = os.getenv(k, "").strip()
                  if v:
                      return v
              return default

          DINGTALK_WEBHOOK = _first_env("DINGTALK_WEBHOOK", "DINGTALK_BASE", "WEBHOOK", default=DEFAULT_WEBHOOK)
          DINGTALK_SECRET  = _first_env("DINGTALK_SECRET",  "SECRET",        default=DEFAULT_SECRET)

          def _mask_tail(s: str, keep: int = 6) -> str:
              if not s: return ""
              return "*" * max(0, len(s)-keep) + s[-keep:]

          # ===================== HTTP =====================
          def make_session() -> requests.Session:
              s = requests.Session()
              s.headers.update({"User-Agent": UA, "Accept-Language": "zh-CN,zh;q=0.9"})
              retries = Retry(total=3, backoff_factor=0.6,
                              status_forcelist=(429,500,502,503,504),
                              allowed_methods=frozenset(["GET","POST"]),
                              raise_on_status=False)
              adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
              s.mount("https://", adapter); s.mount("http://", adapter)
              s.trust_env = False
              return s

          # ===================== DingTalk =====================
          def _sign_webhook(base_webhook: str, secret: str) -> str:
              if not base_webhook: return ""
              if not secret: return base_webhook
              ts = str(round(time.time()*1000))
              string_to_sign = f"{ts}\n{secret}".encode("utf-8")
              hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
              sign = quote(base64.b64encode(hmac_code))
              sep = "&" if "?" in base_webhook else "?"
              return f"{base_webhook}{sep}timestamp={ts}&sign={sign}"

          def send_dingtalk_markdown(title: str, md_text: str) -> bool:
              webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
              if not webhook:
                  print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡æ¨é€ã€‚"); return False
              try:
                  host = urlparse(webhook).netloc
                  print(f"[DingTalk] host={host}  token~{_mask_tail(DINGTALK_WEBHOOK)}  secret~{_mask_tail(DINGTALK_SECRET)}")
              except Exception:
                  pass
              payload = {"msgtype":"markdown","markdown":{"title":title,"text":md_text}}
              try:
                  r = requests.post(webhook, json=payload, timeout=20)
                  ok = (r.status_code==200 and isinstance(r.json(),dict) and r.json().get("errcode")==0)
                  print("DingTalk resp:", r.status_code, r.text[:200]); return ok
              except Exception as e:
                  print("DingTalk error:", e); return False

          # ===================== è§£æ/æ—¶é—´ =====================
          DATE_PATS = [
              r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",
              r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})",
              r"(\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",
          ]
          def parse_dt(text: str) -> Optional[datetime]:
              if not text: return None
              t = re.sub(r"\s+"," ", text.strip())
              for pat in DATE_PATS:
                  m = re.search(pat, t)
                  if not m: continue
                  if len(m.groups())==5:
                      y,mo,d,hh,mm = map(int, m.groups())
                      return datetime(y,mo,d,hh,mm,tzinfo=TZ)
                  if len(m.groups())==3:
                      y,mo,d = map(int, m.groups())
                      return datetime(y,mo,d,12,0,tzinfo=TZ)   # åªæœ‰æ—¥æœŸ â†’ é»˜è®¤ä¸­åˆ
                  if len(m.groups())==4:
                      mo,d,hh,mm = map(int, m.groups())
                      y = datetime.now(TZ).year
                      return datetime(y,mo,d,hh,mm,tzinfo=TZ)
              if re.search(r"(åˆšåˆš|åˆ†é’Ÿ|å°æ—¶å‰|ä»Šå¤©|ä»Šæ—¥)", t):
                  return datetime.now(TZ)
              return None

          def within_last_hours(dt: Optional[datetime], hours: int) -> bool:
              if not dt: return False
              now = datetime.now(TZ)
              return (now - timedelta(hours=hours)) <= dt <= now

          def day_range(date_str: str) -> Tuple[datetime, datetime]:
              if date_str.lower()=="yesterday":
                  base = datetime.now(TZ).date() - timedelta(days=1)
              else:
                  base = datetime.strptime(date_str, "%Y-%m-%d").date()
              start = datetime(base.year,base.month,base.day,0,0,0,tzinfo=TZ)
              end   = datetime(base.year,base.month,base.day,23,59,59,tzinfo=TZ)
              return start, end

          def auto_range(days_for_monday: int = 3) -> Tuple[datetime, datetime, str]:
              """éå‘¨ä¸€ï¼šæ˜¨å¤©ï¼›å‘¨ä¸€ï¼šå‰ä¸‰å¤©ï¼ˆå«å‘¨æ—¥/å‘¨å…­/å‘¨äº”ï¼‰"""
              now = datetime.now(TZ)
              if now.weekday() == 0:  # å‘¨ä¸€
                  end = datetime(now.year, now.month, now.day, 23,59,59, tzinfo=TZ) - timedelta(days=1)
                  start = end - timedelta(days=days_for_monday-1)
                  title = f"è¿‘{days_for_monday}å¤©åˆè¾‘"
              else:
                  start, end = day_range("yesterday")
                  title = "æ˜¨æ—¥ä¸“è¾‘"
              return start, end, title

          # ===================== æ•°æ®ç»“æ„ =====================
          @dataclass
          class Item:
              title: str; url: str; dt: Optional[datetime]; content: str; source: str

          # ===================== ç«™ç‚¹æŠ“å– =====================
          class MohrssHSearch:
              BASE="https://www.mohrss.gov.cn"; PATH="/hsearch/"
              def __init__(self, session, q, delay=1.0): self.session=session; self.q=q; self.delay=delay
              def _fetch_page(self, page:int)->str:
                  params={"searchword": self.q}
                  if page>1: params["page"]=page
                  url=self.BASE+self.PATH+"?"+urlencode(params)
                  r=self.session.get(url,timeout=20); r.encoding=r.apparent_encoding or "utf-8"
                  time.sleep(self.delay); return r.text
              def parse_list(self, html:str)->Tuple[List[Item], Optional[str]]:
                  soup=BeautifulSoup(html,"html.parser")
                  nodes=[]
                  for sel in ["ul.search-list li","div.search-list li","div.list li","ul li","div.result","div.row"]:
                      tmp=soup.select(sel)
                      if tmp: nodes=tmp; break
                  if not nodes: nodes=soup.select("a")
                  items=[]
                  for node in nodes:
                      a=node if node.name=="a" else node.find("a")
                      if not a or not a.get("href"): continue
                      title=a.get_text(" ",strip=True); href=a.get("href").strip()
                      url=urljoin(self.BASE, href)
                      abs_el=None
                      for sel in [".summary",".abs",".intro","p"]:
                          abs_el=node.select_one(sel)
                          if abs_el: break
                      content=abs_el.get_text(" ",strip=True) if abs_el else ""
                      ttxt=""
                      for sel in [".date",".time",".pubtime",".f-date",".info time",".post-time"]:
                          sub=node.select_one(sel)
                          if sub: ttxt=sub.get_text(" ",strip=True); break
                      if not ttxt: ttxt=node.get_text(" ",strip=True)
                      dt=parse_dt(ttxt)
                      items.append(Item(title=title,url=url,dt=dt,content=content,source="äººç¤¾éƒ¨ç«™å†…æœç´¢"))
                  next_link=None
                  for a in soup.select("a"):
                      txt=a.get_text(strip=True)
                      if txt in ("ä¸‹ä¸€é¡µ","ä¸‹é¡µ","â€º",">") or a.get("rel")==["next"]:
                          href=a.get("href") or ""
                          if href and href!="javascript:;" and not href.startswith("#"):
                              next_link=urljoin(self.BASE, href); break
                  return items, next_link
              def run(self,max_pages:int)->List[Item]:
                  all_items=[]; next_url=None
                  for p in range(1,max_pages+1):
                      if p==1 or not next_url: html=self._fetch_page(p)
                      else:
                          r=self.session.get(next_url,timeout=20); r.encoding=r.apparent_encoding or "utf-8"
                          time.sleep(self.delay); html=r.text
                      items,next_url=self.parse_list(html)
                      if not items and p==1: break
                      all_items.extend(items)
                      if not next_url: break
                  return all_items

          class JobMohrssSearch:
              BASE="http://job.mohrss.gov.cn"; PATH="/zxss/index.jhtml"
              def __init__(self, session, q, delay=1.0): self.session=session; self.q=q; self.delay=delay
              def _fetch_page(self, page:int, last_next:Optional[str])->str:
                  if last_next: url=last_next
                  else:
                      params={"textfield": self.q}
                      if page>1: params["pageNo"]=page
                      url=self.BASE+self.PATH+"?"+urlencode(params)
                  r=self.session.get(url,timeout=20); r.encoding=r.apparent_encoding or "utf-8"
                  time.sleep(self.delay); return r.text
              def parse_list(self, html:str)->Tuple[List[Item], Optional[str]]:
                  soup=BeautifulSoup(html,"html.parser")
                  nodes=[]
                  for sel in [".list li",".news-list li",".content-list li",".box-list li","ul.list li","ul.news li","ul li","li"]:
                      tmp=soup.select(sel)
                      if tmp: nodes=tmp; break
                  if not nodes: nodes=soup.select("a")
                  items=[]
                  for node in nodes:
                      a=node if node.name=="a" else node.find("a")
                      if not a or not a.get("href"): continue
                      title=a.get_text(" ",strip=True); href=a.get("href").strip()
                      url=urljoin(self.BASE, href)
                      host=urlparse(url).netloc.lower()
                      if not host.endswith("mohrss.gov.cn"): continue
                      abs_el=None
                      for sel in [".summary",".abs",".intro","p"]:
                          abs_el=node.select_one(sel)
                          if abs_el: break
                      content=abs_el.get_text(" ",strip=True) if abs_el else ""
                      ttxt=""
                      for sel in [".date",".time",".pubtime",".f-date",".info time",".post-time","em","span"]:
                          sub=node.select_one(sel)
                          if sub:
                              maybe=sub.get_text(" ",strip=True)
                              if re.search(r"\d{2,4}[^\d]\d{1,2}[^\d]\d{1,2}", maybe) or re.search(r"(åˆšåˆš|åˆ†é’Ÿ|å°æ—¶å‰|ä»Šå¤©|ä»Šæ—¥)", maybe):
                                  ttxt=maybe; break
                      if not ttxt: ttxt=node.get_text(" ",strip=True)
                      dt=parse_dt(ttxt)
                      items.append(Item(title=title,url=url,dt=dt,content=content,source="å…¬å…±æ‹›è˜ç½‘æœç´¢"))
                  next_link=None
                  for a in soup.select("a"):
                      txt=a.get_text(strip=True)
                      if txt in ("ä¸‹ä¸€é¡µ","ä¸‹é¡µ","â€º",">") or a.get("rel")==["next"]:
                          href=a.get("href") or ""
                          if href and href!="javascript:;" and not href.startswith("#"):
                              next_link=urljoin(self.BASE, href); break
                  return items, next_link
              def run(self,max_pages:int)->List[Item]:
                  all_items=[]; next_url=None
                  for p in range(1,max_pages+1):
                      html=self._fetch_page(p,last_next=next_url)
                      items,next_url=self.parse_list(html)
                      if not items and p==1: break
                      all_items.extend(items)
                      if not next_url: break
                  return all_items

          # ===================== æ±‡æ€»/è¾“å‡º =====================
          def dedup_by_url(items: List[Item]) -> List[Item]:
              seen=set(); out=[]
              for it in items:
                  if it.url and it.url not in seen:
                      seen.add(it.url); out.append(it)
              return out

          def filter_by_range(items: List[Item], start: datetime, end: datetime) -> List[Item]:
              return [it for it in items if it.dt and start <= it.dt <= end]

          def split_keywords(q: str) -> List[str]:
              if not q: return []
              parts = re.split(r"[ï¼Œ,]+", q)
              seen, out = set(), []
              for w in [p.strip() for p in parts if p.strip()]:
                  if w not in seen:
                      seen.add(w); out.append(w)
              return out

          def build_markdown(items: List[Item], keyword: str, title_prefix: str) -> str:
              now_dt=datetime.now(TZ)
              wd=["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][now_dt.weekday()]
              lines=[f"**æ—¥æœŸï¼š{now_dt.strftime('%Y-%m-%d')}ï¼ˆ{wd}ï¼‰**","",
                     f"**æ ‡é¢˜ï¼š{title_prefix}ï½œäººç¤¾éƒ¨ & å…¬å…±æ‹›è˜ç½‘æœç´¢ï½œ{keyword}**","",
                     "**ä¸»è¦å†…å®¹**"]
              if not items:
                  lines.append("> æš‚æ— æ›´æ–°ã€‚"); return "\n".join(lines)
              for i,it in enumerate(items,1):
                  dt_str=it.dt.strftime("%Y-%m-%d %H:%M") if it.dt else ""
                  title_line=f"{i}. [{it.title}]({it.url})"
                  if it.source: title_line+=f"ã€€â€”ã€€*{it.source}*"
                  if dt_str: title_line+=f"ã€€`{dt_str}`"
                  lines.append(title_line)
                  if it.content:
                      snippet=re.sub(r"\s+"," ", it.content).strip()[:120]
                      lines.append(f"> {snippet}")
                  lines.append("")
              return "\n".join(lines)

          # ===================== ä¸»æµç¨‹ =====================
          def main():
              ap = argparse.ArgumentParser(description="äººç¤¾éƒ¨ & å…¬å…±æ‹›è˜ç½‘ ç«™å†…æœç´¢ â†’ é’‰é’‰æ¨é€ï¼ˆè‡ªåŠ¨æ—¥æœŸï¼šæ˜¨å¤©/å‘¨ä¸€å‰ä¸‰å¤©ï¼‰")
              ap.add_argument("--q", default=os.getenv("QUERY", "äººåŠ›èµ„æº"), help="å…³é”®è¯ï¼Œæ”¯æŒé€—å·åˆ†éš”ï¼ˆä¾‹ï¼šå¤–åŒ…,å°±ä¸š,æ‹›è˜ï¼‰")
              ap.add_argument("--pages", type=int, default=int(os.getenv("PAGES", "2")))
              ap.add_argument("--window-hours", type=int, default=int(os.getenv("WINDOW_HOURS", "24")))
              ap.add_argument("--delay", type=float, default=float(os.getenv("DELAY", "1.0")))
              ap.add_argument("--limit", type=int, default=int(os.getenv("LIMIT", "20")))
              ap.add_argument("--date", default=os.getenv("DATE", ""), help="æŒ‡å®šæ—¥æœŸï¼ˆyesterday / YYYY-MM-DDï¼‰ã€‚ä¸ºç©ºå¯ç”¨è‡ªåŠ¨èŒƒå›´")
              ap.add_argument("--auto-range", default=os.getenv("AUTO_RANGE", "true").lower()=="true",
                              action="store_true", help="å¯ç”¨è‡ªåŠ¨èŒƒå›´ï¼ˆé»˜è®¤å¼€å¯ï¼‰")
              ap.add_argument("--days-for-monday", type=int, default=int(os.getenv("DAYS_FOR_MONDAY", "3")),
                              help="å‘¨ä¸€åˆå¹¶çš„å¤©æ•°ï¼ˆé»˜è®¤3ï¼‰")
              ap.add_argument("--no-push", action="store_true")
              args = ap.parse_args()

              session = make_session()

              # å¤šå…³é”®è¯åˆå¹¶æŠ“å–
              keywords = split_keywords(args.q) or ["äººåŠ›èµ„æº"]
              all_items_raw: List[Item] = []
              for kw in keywords:
                  all_items_raw += MohrssHSearch(session, kw, args.delay).run(args.pages)
                  all_items_raw += JobMohrssSearch(session, kw, args.delay).run(args.pages)

              all_items = dedup_by_url(all_items_raw)

              # é€‰æ‹©æ—¶é—´èŒƒå›´
              title_prefix = "æ—©å®‰èµ„è®¯"
              if args.date:
                  start, end = day_range(args.date)
                  title_prefix = f"{args.date} ä¸“é¢˜"
              elif args.auto_range:
                  start, end, tp = auto_range(args.days_for_monday)
                  title_prefix = tp
              else:
                  start = end = None

              if start and end:
                  all_items = filter_by_range(all_items, start, end)
              else:
                  all_items = [it for it in all_items if within_last_hours(it.dt, args.window_hours)]

              # æ’åº + æˆªæ–­
              all_items.sort(key=lambda x: x.dt or datetime(1970,1,1,tzinfo=TZ), reverse=True)
              show = all_items[:args.limit] if args.limit and args.limit>0 else all_items

              print(f"âœ… åŸå§‹æŠ“å– {len(all_items_raw)} æ¡ï¼›å»é‡å {len(all_items)} æ¡ï¼›å±•ç¤º {len(show)} æ¡ã€‚å…³é”®è¯ï¼š{keywords}")
              md = build_markdown(show, "ã€".join(keywords), title_prefix)
              print("\n--- Markdown Preview ---\n"); print(md)

              if not args.no_push:
                  ok = send_dingtalk_markdown(f"{title_prefix}ï½œéƒ¨ç½‘&æ‹›è˜ç½‘æœç´¢ï½œ{'ã€'.join(keywords)}", md)
                  print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥/æœªæ¨é€ âŒ")

          if __name__ == "__main__":
              main()
          PY
          chmod +x hr_search_auto_range.py

      - name: Run HR search (auto range)
        env:
          # â€”â€” ä¸šåŠ¡å‚æ•°ï¼ˆä½ å¯ä»¥åœ¨è¿™é‡Œæ”¹ï¼‰â€”â€”
          QUERY: "äººåŠ›èµ„æº, å°±ä¸š, æ‹›è˜, å¤–åŒ…"  # å¤šå…³é”®è¯ï¼Œé€—å·åˆ†éš”
          AUTO_RANGE: "true"                   # å¼€å¯è‡ªåŠ¨ï¼šå‘¨ä¸€æŠ“å‰ä¸‰å¤©ï¼Œå…¶å®ƒå¤©æŠ“æ˜¨å¤©
          DAYS_FOR_MONDAY: "3"                 # å‘¨ä¸€åˆå¹¶å¤©æ•°ï¼ˆå‘¨äº”+å‘¨å…­+å‘¨æ—¥ï¼‰
          DATE: ""                             # ç•™ç©ºä½¿ç”¨ AUTO_RANGEï¼›å¦‚éœ€æŒ‡å®šæŸå¤©å¡« YYYY-MM-DD æˆ– yesterday
          PAGES: "3"
          LIMIT: "50"
          DELAY: "0.5"

          # â€”â€” é’‰é’‰æœºå™¨äººï¼ˆåœ¨ä»“åº“ Secrets è®¾ç½®ï¼‰â€”â€”
          DINGTALK_WEBHOOK: ${{ secrets.DINGTALK_WEBHOOK }}
          DINGTALK_SECRET:  ${{ secrets.DINGTALK_SECRET }}

          # è°ƒè¯•ï¼šåªæ‰“å°ä¸æ¨é€
          NO_PUSH: "false"
        run: |
          unset http_proxy https_proxy all_proxy HTTP_PROXY HTTPS_PROXY ALL_PROXY NO_PROXY
          set -e
          CMD="python hr_search_auto_range.py \
            --q \"${QUERY}\" \
            --pages \"${PAGES}\" \
            --delay \"${DELAY}\" \
            --limit \"${LIMIT}\""
          # èŒƒå›´æ§åˆ¶
          if [ -n "${DATE}" ]; then
            CMD="${CMD} --date \"${DATE}\""
          else
            if [ "${AUTO_RANGE}" = "true" ]; then
              CMD="${CMD} --auto-range --days-for-monday \"${DAYS_FOR_MONDAY}\""
            else
              CMD="${CMD} --window-hours \"${WINDOW_HOURS:-48}\""
            fi
          fi
          # æ˜¯å¦æ¨é€
          if [ "${NO_PUSH}" = "true" ]; then
            CMD="${CMD} --no-push"
          fi
          echo "[Run] ${CMD}"
          eval ${CMD}

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hr-search-auto-range
          path: |
            people_search_*.csv
            people_search_*.json
