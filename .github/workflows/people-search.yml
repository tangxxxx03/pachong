name: HR Search (Auto Range: Yesterday / Mon-Last3Days)

on:
  workflow_dispatch:
  schedule:
    - cron: "15 1 * * *"  # ÊØèÂ§© 09:15 CST (UTC+8)

concurrency:
  group: hr-search-auto-range
  cancel-in-progress: false

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 urllib3

      - name: Debug env (optional)
        run: |
          date -u
          env | grep -i proxy || true

      - name: Write script hr_search_auto_range.py
        run: |
          cat > hr_search_auto_range.py <<'PY'
          # -*- coding: utf-8 -*-
          import re, os, time, hmac, base64, hashlib, argparse
          from dataclasses import dataclass
          from typing import List, Tuple, Optional
          from urllib.parse import urljoin, urlencode, urlparse, quote
          from datetime import datetime, timedelta
          try:
              from zoneinfo import ZoneInfo
          except Exception:
              from backports.zoneinfo import ZoneInfo
          import requests
          from requests.adapters import HTTPAdapter
          from urllib3.util.retry import Retry
          from bs4 import BeautifulSoup

          TZ = ZoneInfo("Asia/Shanghai")
          UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/123.0.0.0 Safari/537.36")

          DEFAULT_WEBHOOK = (
              "https://oapi.dingtalk.com/robot/send?"
              "access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
          )
          DEFAULT_SECRET = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"

          def _first_env(*keys: str, default: str = "") -> str:
              for k in keys:
                  v = os.getenv(k, "").strip()
                  if v:
                      return v
              return default

          DINGTALK_WEBHOOK = _first_env("DINGTALK_WEBHOOK", "DINGTALK_BASE", "WEBHOOK", default=DEFAULT_WEBHOOK)
          DINGTALK_SECRET  = _first_env("DINGTALK_SECRET",  "SECRET",        default=DEFAULT_SECRET)

          def make_session() -> requests.Session:
              s = requests.Session()
              s.headers.update({"User-Agent": UA, "Accept-Language": "zh-CN,zh;q=0.9"})
              retries = Retry(total=3, backoff_factor=0.6,
                              status_forcelist=(429,500,502,503,504),
                              allowed_methods=frozenset(["GET","POST"]),
                              raise_on_status=False)
              adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
              s.mount("https://", adapter); s.mount("http://", adapter)
              s.trust_env = False
              return s

          def _sign_webhook(base_webhook: str, secret: str) -> str:
              if not base_webhook: return ""
              if not secret: return base_webhook
              ts = str(round(time.time()*1000))
              string_to_sign = f"{ts}\n{secret}".encode("utf-8")
              hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
              sign = quote(base64.b64encode(hmac_code))
              sep = "&" if "?" in base_webhook else "?"
              return f"{base_webhook}{sep}timestamp={ts}&sign={sign}"

          def send_dingtalk_markdown(title: str, md_text: str) -> bool:
              webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
              if not webhook:
                  print("üîï Êú™ÈÖçÁΩÆÈíâÈíâ WebhookÔºåË∑≥ËøáÊé®ÈÄÅ„ÄÇ"); return False
              payload = {"msgtype":"markdown","markdown":{"title":title,"text":md_text}}
              try:
                  r = requests.post(webhook, json=payload, timeout=20)
                  ok = (r.status_code==200 and isinstance(r.json(),dict) and r.json().get("errcode")==0)
                  print("DingTalk resp:", r.status_code, r.text[:200]); return ok
              except Exception as e:
                  print("DingTalk error:", e); return False

          DATE_PATS = [
              r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",
              r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})",
              r"(\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",
          ]
          def parse_dt(text: str) -> Optional[datetime]:
              if not text: return None
              t = re.sub(r"\s+"," ", text.strip())
              for pat in DATE_PATS:
                  m = re.search(pat, t)
                  if not m: continue
                  if len(m.groups())==5:
                      y,mo,d,hh,mm = map(int, m.groups())
                      return datetime(y,mo,d,hh,mm,tzinfo=TZ)
                  if len(m.groups())==3:
                      y,mo,d = map(int, m.groups())
                      return datetime(y,mo,d,12,0,tzinfo=TZ)
                  if len(m.groups())==4:
                      mo,d,hh,mm = map(int, m.groups())
                      y = datetime.now(TZ).year
                      return datetime(y,mo,d,hh,mm,tzinfo=TZ)
              if re.search(r"(ÂàöÂàö|ÂàÜÈíü|Â∞èÊó∂Ââç|‰ªäÂ§©|‰ªäÊó•)", t):
                  return datetime.now(TZ)
              return None

          def within_last_hours(dt: Optional[datetime], hours: int) -> bool:
              if not dt: return False
              now = datetime.now(TZ)
              return (now - timedelta(hours=hours)) <= dt <= now

          def day_range(date_str: str) -> Tuple[datetime, datetime]:
              if date_str.lower()=="yesterday":
                  base = datetime.now(TZ).date() - timedelta(days=1)
              else:
                  base = datetime.strptime(date_str, "%Y-%m-%d").date()
              start = datetime(base.year,base.month,base.day,0,0,0,tzinfo=TZ)
              end   = datetime(base.year,base.month,base.day,23,59,59,tzinfo=TZ)
              return start, end

          def auto_range(days_for_monday: int = 3) -> Tuple[datetime, datetime, str]:
              now = datetime.now(TZ)
              if now.weekday() == 0:
                  end = datetime(now.year, now.month, now.day, 23,59,59, tzinfo=TZ) - timedelta(days=1)
                  start = end - timedelta(days=days_for_monday-1)
                  title = f"Ëøë{days_for_monday}Â§©ÂêàËæë"
              else:
                  start, end = day_range("yesterday")
                  title = "Êò®Êó•‰∏ìËæë"
              return start, end, title

          @dataclass
          class Item:
              title: str; url: str; dt: Optional[datetime]; content: str; source: str

          class MohrssHSearch:
              BASE="https://www.mohrss.gov.cn"; PATH="/hsearch/"
              def __init__(self, session, q, delay=1.0): self.session=session; self.q=q; self.delay=delay
              def _fetch_page(self, page:int)->str:
                  params={"searchword": self.q}
                  if page>1: params["page"]=page
                  url=self.BASE+self.PATH+"?"+urlencode(params)
                  r=self.session.get(url,timeout=20); r.encoding=r.apparent_encoding or "utf-8"
                  time.sleep(self.delay); return r.text
              def parse_list(self, html:str)->Tuple[List[Item], Optional[str]]:
                  soup=BeautifulSoup(html,"html.parser")
                  nodes=[]
                  for sel in ["ul.search-list li","div.search-list li","div.list li","ul li","div.result","div.row"]:
                      tmp=soup.select(sel)
                      if tmp: nodes=tmp; break
                  if not nodes: nodes=soup.select("a")
                  items=[]
                  for node in nodes:
                      a=node if node.name=="a" else node.find("a")
                      if not a or not a.get("href"): continue
                      title=a.get_text(" ",strip=True); href=a.get("href").strip()
                      url=urljoin(self.BASE, href)
                      abs_el=None
                      for sel in [".summary",".abs",".intro","p"]:
                          abs_el=node.select_one(sel)
                          if abs_el: break
                      content=abs_el.get_text(" ",strip=True) if abs_el else ""
                      ttxt=""
                      for sel in [".date",".time",".pubtime",".f-date",".info time",".post-time"]:
                          sub=node.select_one(sel)
                          if sub: ttxt=sub.get_text(" ",strip=True); break
                      if not ttxt: ttxt=node.get_text(" ",strip=True)
                      dt=parse_dt(ttxt)
                      items.append(Item(title=title,url=url,dt=dt,content=content,source="‰∫∫Á§æÈÉ®Á´ôÂÜÖÊêúÁ¥¢"))
                  next_link=None
                  for a in soup.select("a"):
                      txt=a.get_text(strip=True)
                      if txt in ("‰∏ã‰∏ÄÈ°µ","‰∏ãÈ°µ","‚Ä∫",">") or a.get("rel")==["next"]:
                          href=a.get("href") or ""
                          if href and href!="javascript:;" and not href.startswith("#"):
                              next_link=urljoin(self.BASE, href); break
                  return items, next_link
              def run(self,max_pages:int)->List[Item]:
                  all_items=[]; next_url=None
                  for p in range(1,max_pages+1):
                      if p==1 or not next_url: html=self._fetch_page(p)
                      else:
                          r=self.session.get(next_url,timeout=20); r.encoding=r.apparent_encoding or "utf-8"
                          time.sleep(self.delay); html=r.text
                      items,next_url=self.parse_list(html)
                      if not items and p==1: break
                      all_items.extend(items)
                      if not next_url: break
                  return all_items

          class JobMohrssSearch:
              BASE="http://job.mohrss.gov.cn"; PATH="/zxss/index.jhtml"
              def __init__(self, session, q, delay=1.0): self.session=session; self.q=q; self.delay=delay
              def _fetch_page(self, page:int, last_next:Optional[str])->str:
                  if last_next: url=last_next
                  else:
                      params={"textfield": self.q}
                      if page>1: params["pageNo"]=page
                      url=self.BASE+self.PATH+"?"+urlencode(params)
                  r=self.session.get(url,timeout=20); r.encoding=r.apparent_encoding or "utf-8"
                  time.sleep(self.delay); return r.text
              def parse_list(self, html:str)->Tuple[List[Item], Optional[str]]:
                  soup=BeautifulSoup(html,"html.parser")
                  nodes=[]
                  for sel in [".list li",".news-list li",".content-list li",".box-list li","ul.list li","ul.news li","ul li","li"]:
                      tmp=soup.select(sel)
                      if tmp: nodes=tmp; break
                  if not nodes: nodes=soup.select("a")
                  items=[]
                  for node in nodes:
                      a=node if node.name=="a" else node.find("a")
                      if not a or not a.get("href"): continue
                      title=a.get_text(" ",strip=True); href=a.get("href").strip()
                      url=urljoin(self.BASE, href)
                      host=urlparse(url).netloc.lower()
                      if not host.endswith("mohrss.gov.cn"): continue
                      abs_el=None
                      for sel in [".summary",".abs",".intro","p"]:
                          abs_el=node.select_one(sel)
                          if abs_el: break
                      content=abs_el.get_text(" ",strip=True) if abs_el else ""
                      ttxt=""
                      for sel in [".date",".time",".pubtime",".f-date",".info time",".post-time","em","span"]:
                          sub=node.select_one(sel)
                          if sub:
                              maybe=sub.get_text(" ",strip=True)
                              if re.search(r"\d{2,4}[^\d]\d{1,2}[^\d]\d{1,2}", maybe) or re.search(r"(ÂàöÂàö|ÂàÜÈíü|Â∞èÊó∂Ââç|‰ªäÂ§©|‰ªäÊó•)", maybe):
                                  ttxt=maybe; break
                      if not ttxt: ttxt=node.get_text(" ",strip=True)
                      dt=parse_dt(ttxt)
                      items.append(Item(title=title,url=url,dt=dt,content=content,source="ÂÖ¨ÂÖ±ÊãõËÅòÁΩëÊêúÁ¥¢"))
                  next_link=None
                  for a in soup.select("a"):
                      txt=a.get_text(strip=True)
                      if txt in ("‰∏ã‰∏ÄÈ°µ","‰∏ãÈ°µ","‚Ä∫",">") or a.get("rel")==["next"]:
                          href=a.get("href") or ""
                          if href and href!="javascript:;" and not href.startswith("#"):
                              next_link=urljoin(self.BASE, href); break
                  return items, next_link
              def run(self,max_pages:int)->List[Item]:
                  all_items=[]; next_url=None
                  for p in range(1,max_pages+1):
                      html=self._fetch_page(p,last_next=next_url)
                      items,next_url=self.parse_list(html)
                      if not items and p==1: break
                      all_items.extend(items)
                      if not next_url: break
                  return all_items

          def dedup_by_url(items: List[Item]) -> List[Item]:
              seen=set(); out=[]
              for it in items:
                  if it.url and it.url not in seen:
                      seen.add(it.url); out.append(it)
              return out

          def filter_by_range(items: List[Item], start: datetime, end: datetime) -> List[Item]:
              return [it for it in items if it.dt and start <= it.dt <= end]

          def split_keywords(q: str) -> List[str]:
              if not q: return []
              parts = re.split(r"[Ôºå,]+", q)
              seen, out = set(), []
              for w in [p.strip() for p in parts if p.strip()]:
                  if w not in seen:
                      seen.add(w); out.append(w)
              return out

          def build_markdown(items: List[Item], keyword: str, title_prefix: str) -> str:
              now_dt=datetime.now(TZ)
              wd=["Âë®‰∏Ä","Âë®‰∫å","Âë®‰∏â","Âë®Âõõ","Âë®‰∫î","Âë®ÂÖ≠","Âë®Êó•"][now_dt.weekday()]
              lines=[f"**Êó•ÊúüÔºö{now_dt.strftime('%Y-%m-%d')}Ôºà{wd}Ôºâ**","",
                     f"**Ê†áÈ¢òÔºö{title_prefix}ÔΩú‰∫∫Á§æÈÉ® & ÂÖ¨ÂÖ±ÊãõËÅòÁΩëÊêúÁ¥¢ÔΩú{keyword}**","",
                     "**‰∏ªË¶ÅÂÜÖÂÆπ**"]
              if not items:
                  lines.append("> ÊöÇÊó†Êõ¥Êñ∞„ÄÇ"); return "\n".join(lines)
              for i,it in enumerate(items,1):
                  dt_str=it.dt.strftime("%Y-%m-%d %H:%M") if it.dt else ""
                  title_line=f"{i}. [{it.title}]({it.url})"
                  if it.source: title_line+=f"„ÄÄ‚Äî„ÄÄ*{it.source}*"
                  if dt_str: title_line+=f"„ÄÄ`{dt_str}`"
                  lines.append(title_line)
                  if it.content:
                      snippet=re.sub(r"\s+"," ", it.content).strip()[:120]
                      lines.append(f"> {snippet}")
                  lines.append("")
              return "\n".join(lines)

          def auto_range(days_for_monday: int = 3) -> Tuple[datetime, datetime, str]:
              now = datetime.now(TZ)
              if now.weekday() == 0:
                  end = datetime(now.year, now.month, now.day, 23,59,59, tzinfo=TZ) - timedelta(days=1)
                  start = end - timedelta(days=days_for_monday-1)
                  title = f"Ëøë{days_for_monday}Â§©ÂêàËæë"
              else:
                  start, end = day_range("yesterday")
                  title = "Êò®Êó•‰∏ìËæë"
              return start, end, title

          def main():
              ap = argparse.ArgumentParser(description="‰∫∫Á§æÈÉ® & ÂÖ¨ÂÖ±ÊãõËÅòÁΩë Á´ôÂÜÖÊêúÁ¥¢ ‚Üí ÈíâÈíâÊé®ÈÄÅÔºàËá™Âä®Êó•ÊúüÔºöÊò®Â§©/Âë®‰∏ÄÂâç‰∏âÂ§©Ôºâ")
              ap.add_argument("--q", default=os.getenv("QUERY", "‰∫∫ÂäõËµÑÊ∫ê"))
              ap.add_argument("--pages", type=int, default=int(os.getenv("PAGES", "2")))
              ap.add_argument("--window-hours", type=int, default=int(os.getenv("WINDOW_HOURS", "24")))
              ap.add_argument("--delay", type=float, default=float(os.getenv("DELAY", "1.0")))
              ap.add_argument("--limit", type=int, default=int(os.getenv("LIMIT", "20")))
              ap.add_argument("--date", default=os.getenv("DATE", ""))
              ap.add_argument("--auto-range", default=os.getenv("AUTO_RANGE", "true").lower()=="true",
                              action="store_true")
              ap.add_argument("--days-for-monday", type=int, default=int(os.getenv("DAYS_FOR_MONDAY", "3")))
              ap.add_argument("--no-push", action="store_true")
              args = ap.parse_args()

              session = make_session()

              keywords = split_keywords(args.q) or ["‰∫∫ÂäõËµÑÊ∫ê"]
              all_items_raw: List[Item] = []
              for kw in keywords:
                  all_items_raw += MohrssHSearch(session, kw, args.delay).run(args.pages)
                  all_items_raw += JobMohrssSearch(session, kw, args.delay).run(args.pages)

              all_items = dedup_by_url(all_items_raw)

              title_prefix = "Êó©ÂÆâËµÑËÆØ"
              if args.date:
                  start, end = day_range(args.date)
                  title_prefix = f"{args.date} ‰∏ìÈ¢ò"
              elif args.auto_range:
                  start, end, tp = auto_range(args.days_for_monday)
                  title_prefix = tp
              else:
                  start = end = None

              if start and end:
                  all_items = filter_by_range(all_items, start, end)
              else:
                  all_items = [it for it in all_items if within_last_hours(it.dt, args.window_hours)]

              all_items.sort(key=lambda x: x.dt or datetime(1970,1,1,tzinfo=TZ), reverse=True)
              show = all_items[:args.limit] if args.limit and args.limit>0 else all_items

              md = build_markdown(show, "„ÄÅ".join(keywords), title_prefix)
              print(md)

              if not args.no_push:
                  send_dingtalk_markdown(f"{title_prefix}ÔΩúÈÉ®ÁΩë&ÊãõËÅòÁΩëÊêúÁ¥¢ÔΩú{'„ÄÅ'.join(keywords)}", md)

          if __name__ == "__main__":
              main()
          PY
          chmod +x hr_search_auto_range.py

      - name: Run HR search (auto range)
        env:
          QUERY: "‰∫∫ÂäõËµÑÊ∫ê, Â∞±‰∏ö, ÊãõËÅò, Â§ñÂåÖ"
          AUTO_RANGE: "true"
          DAYS_FOR_MONDAY: "3"
          DATE: ""
          PAGES: "3"
          LIMIT: "50"
          DELAY: "0.5"
          DINGTALK_WEBHOOK: ${{ secrets.DINGTALK_WEBHOOK }}
          DINGTALK_SECRET:  ${{ secrets.DINGTALK_SECRET }}
          NO_PUSH: "false"
        run: |
          unset http_proxy https_proxy all_proxy HTTP_PROXY HTTPS_PROXY ALL_PROXY NO_PROXY
          set -e
          CMD="python hr_search_auto_range.py \
            --q \"${QUERY}\" \
            --pages \"${PAGES}\" \
            --delay \"${DELAY}\" \
            --limit \"${LIMIT}\""
          if [ -n "${DATE}" ]; then
            CMD="${CMD} --date \"${DATE}\""
          else
            if [ "${AUTO_RANGE}" = "true" ]; then
              CMD="${CMD} --auto-range --days-for-monday \"${DAYS_FOR_MONDAY}\""
            else
              CMD="${CMD} --window-hours \"${WINDOW_HOURS:-48}\""
            fi
          fi
          if [ "${NO_PUSH}" = "true" ]; then
            CMD="${CMD} --no-push"
          fi
          echo "[Run] ${CMD}"
          eval ${CMD}

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: hr-search-auto-range
          path: |
            people_search_*.csv
            people_search_*.json
