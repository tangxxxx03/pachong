# -*- coding: utf-8 -*-
"""
ç¬¬ä¸€è´¢ç»ã€Œä¸€è´¢æ—©æŠ¥ã€(feed/669) â€” åªæŠ“ RSS description ä¸­çš„ã€è§‚å›½å†…ã€‘å’Œã€å¤§å…¬å¸ã€‘ä¸¤æ®µ

æœ¬ç‰ˆæœ¬ä¿®å¤ï¼š
- RSSHub å…¬å…±å®ä¾‹åœ¨ GitHub Actions å¸¸è§ 403/429ï¼šå¢åŠ å¤šå®ä¾‹ fallback + é‡è¯•é€€é¿
- æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›– RSSHub å®ä¾‹åˆ—è¡¨ï¼ˆæ¨èä½ åç»­ç”¨è‡ªå»ºï¼‰

ç¯å¢ƒå˜é‡ï¼ˆå¿…é€‰ï¼‰ï¼š
- DINGTALK_WEBHOOK
- DINGTALK_SECRETï¼ˆå¯é€‰ï¼‰

ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰ï¼š
- TOP_N: æ¯å¤©æ¨é€æ¡æ•°ï¼Œé»˜è®¤ 8
- RSSHUB_BASES: å¤šä¸ª RSSHub baseï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼š
    https://rsshub.app,https://rsshub.rssforever.com
  ä¸å¡«åˆ™ä½¿ç”¨å†…ç½®åˆ—è¡¨
- RSSHUB_ROUTE: é»˜è®¤ /yicai/feed/669
"""

import os
import re
import json
import time
import hmac
import base64
import hashlib
import urllib.parse
from datetime import datetime
from typing import List, Dict, Any, Optional

import requests
import feedparser
from bs4 import BeautifulSoup, Tag

# =========================
# é…ç½®
# =========================
DATA_DIR = "data"
SENT_PATH = os.path.join(DATA_DIR, "sent_links.json")

DEFAULT_RSSHUB_ROUTE = "/yicai/feed/669"

# å†…ç½®å¤‡ç”¨ RSSHub å®ä¾‹ï¼ˆå…¬å…±é•œåƒä¸ä¿è¯é•¿æœŸå¯ç”¨ï¼Œä½†å¯ä½œä¸ºä¸´æ—¶æ•‘ç«ï¼‰
DEFAULT_RSSHUB_BASES = [
    "https://rsshub.app",
    "https://rsshub.rssforever.com",
    "https://rsshub.feeded.xyz",
]

UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
FETCH_TIMEOUT = 25
TOP_N = int(os.getenv("TOP_N", "8"))

SECTION_ALLOW = ["è§‚å›½å†…", "å¤§å…¬å¸"]
TITLE_BLOCKLIST = ["æŠ¥å", "è¯¾ç¨‹", "è®­ç»ƒè¥", "ä¼˜æƒ ", "ä¿ƒé”€", "å¹¿å‘Š", "è½¯æ–‡", "å¸¦è´§"]


# =========================
# åŸºç¡€å·¥å…·
# =========================
def ensure_data_dir():
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR, exist_ok=True)

def load_sent_links() -> set:
    ensure_data_dir()
    if os.path.exists(SENT_PATH):
        try:
            with open(SENT_PATH, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()

def save_sent_links(sent: set):
    ensure_data_dir()
    with open(SENT_PATH, "w", encoding="utf-8") as f:
        json.dump(sorted(list(sent))[-5000:], f, ensure_ascii=False, indent=2)

def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def looks_blocked(title: str) -> bool:
    t = title or ""
    return any(x in t for x in TITLE_BLOCKLIST)

def safe_get(url: str) -> requests.Response:
    return requests.get(url, timeout=FETCH_TIMEOUT, headers={"User-Agent": UA})

def get_rsshub_bases() -> List[str]:
    bases_env = (os.getenv("RSSHUB_BASES") or "").strip()
    if bases_env:
        bases = [b.strip().rstrip("/") for b in bases_env.split(",") if b.strip()]
        return bases or [b.rstrip("/") for b in DEFAULT_RSSHUB_BASES]
    return [b.rstrip("/") for b in DEFAULT_RSSHUB_BASES]

def build_rsshub_urls() -> List[str]:
    route_env = (os.getenv("RSSHUB_ROUTE") or "").strip()
    route = route_env or DEFAULT_RSSHUB_ROUTE
    if not route.startswith("/"):
        route = "/" + route
    return [f"{base}{route}" for base in get_rsshub_bases()]


# =========================
# RSS æ‹‰å–ï¼ˆå¤šå®ä¾‹ fallbackï¼‰
# =========================
def fetch_rss_items() -> List[Dict[str, Any]]:
    urls = build_rsshub_urls()

    last_err = None
    for url in urls:
        # æ¯ä¸ªå®ä¾‹ç»™ 2 æ¬¡å°è¯•ï¼Œ403/429/5xx å°±æ¢ä¸‹ä¸€ä¸ª
        for attempt in range(2):
            try:
                r = safe_get(url)

                # å¯¹å¸¸è§æ‹’ç»åšæ˜¾å¼å¤„ç†
                if r.status_code in (403, 429):
                    raise requests.HTTPError(f"{r.status_code} Forbidden/RateLimit for url: {url}", response=r)
                if 500 <= r.status_code < 600:
                    raise requests.HTTPError(f"{r.status_code} ServerError for url: {url}", response=r)

                r.raise_for_status()

                feed = feedparser.parse(r.content)
                items: List[Dict[str, Any]] = []

                for e in feed.entries[:80]:
                    title = clean_text(getattr(e, "title", ""))
                    link = clean_text(getattr(e, "link", ""))
                    published = clean_text(getattr(e, "published", "") or getattr(e, "updated", ""))

                    desc = ""
                    if hasattr(e, "summary"):
                        desc = e.summary
                    elif hasattr(e, "description"):
                        desc = e.description

                    if not title or not link:
                        continue
                    if looks_blocked(title):
                        continue

                    items.append({
                        "title": title,
                        "url": link,
                        "published": published,
                        "description_html": desc,
                        "source": url
                    })

                if not items:
                    raise RuntimeError(f"RSS parsed but empty entries: {url}")

                print(f"[RSS] ok via: {url}, entries={len(items)}")
                return items

            except Exception as e:
                last_err = e
                # é€€é¿ä¸€ä¸‹å†è¯•
                time.sleep(1.5 * (attempt + 1))

        print(f"[RSS] switch to next base after failures: {url}")

    raise RuntimeError(f"æ‰€æœ‰ RSSHub å®ä¾‹éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯ï¼š{last_err}")


# =========================
# è§£æ descriptionï¼šåªæå–ã€è§‚å›½å†…ã€‘ã€å¤§å…¬å¸ã€‘
# =========================
def _normalize_section_name(text: str) -> Optional[str]:
    t = clean_text(text)
    if not t:
        return None
    t = t.replace("ã€", "").replace("ã€‘", "")
    t = re.sub(r"\s+", "", t)
    if t in SECTION_ALLOW:
        return t
    return None

def extract_sections_from_description(description_html: str) -> Dict[str, List[str]]:
    result = {name: [] for name in SECTION_ALLOW}
    if not description_html:
        return result

    soup = BeautifulSoup(description_html, "html.parser")
    ps = soup.find_all("p")
    current_section: Optional[str] = None

    def is_section_header_p(p: Tag) -> Optional[str]:
        strong = p.find("strong")
        if strong:
            sec = _normalize_section_name(strong.get_text(" "))
            if sec:
                return sec
        txt = clean_text(p.get_text(" "))
        m = re.search(r"ã€\s*([^ã€‘]+)\s*ã€‘", txt)
        if m:
            sec = _normalize_section_name(m.group(1))
            if sec:
                return sec
        return None

    def is_any_header_p(p: Tag) -> bool:
        strong = p.find("strong")
        if strong:
            return bool(clean_text(strong.get_text(" ")))
        txt = clean_text(p.get_text(" "))
        return bool(re.match(r"^ã€.+ã€‘$", txt))

    for p in ps:
        sec = is_section_header_p(p)
        if sec:
            current_section = sec
            continue

        if not current_section:
            continue

        # é‡åˆ°æ–°çš„æ ‡é¢˜ï¼ˆå“ªæ€•ä¸æ˜¯æˆ‘ä»¬å…³å¿ƒçš„ï¼‰ï¼Œåœæ­¢æ”¶é›†
        if is_any_header_p(p) and is_section_header_p(p) is None:
            current_section = None
            continue

        txt = clean_text(p.get_text(" "))
        if not txt:
            continue
        if "ç‚¹å‡»" in txt and "å¬æ–°é—»" in txt:
            continue

        result[current_section].append(txt)

    # æ¸…æ´—ï¼šå»é‡ + å»æ‰å¤ªçŸ­
    for k in list(result.keys()):
        cleaned = []
        seen = set()
        for x in result[k]:
            x = clean_text(x)
            if len(x) < 10:
                continue
            if x in seen:
                continue
            seen.add(x)
            cleaned.append(x)
        result[k] = cleaned

    return result


# =========================
# é’‰é’‰æ¨é€
# =========================
def dingtalk_sign(timestamp_ms: str, secret: str) -> str:
    string_to_sign = f"{timestamp_ms}\n{secret}"
    h = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), hashlib.sha256).digest()
    return urllib.parse.quote_plus(base64.b64encode(h))

def dingtalk_send_markdown(title: str, markdown: str):
    webhook = (os.getenv("DINGTALK_WEBHOOK") or "").strip()
    if not webhook:
        raise RuntimeError("ç¼ºå°‘ç¯å¢ƒå˜é‡ DINGTALK_WEBHOOK")

    secret = (os.getenv("DINGTALK_SECRET") or "").strip()
    url = webhook
    if secret:
        ts = str(int(time.time() * 1000))
        sign = dingtalk_sign(ts, secret)
        joiner = "&" if "?" in url else "?"
        url = f"{url}{joiner}timestamp={ts}&sign={sign}"

    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": markdown}}
    r = requests.post(url, json=payload, timeout=FETCH_TIMEOUT)
    r.raise_for_status()


# =========================
# ä¸»æµç¨‹
# =========================
def main():
    sent = load_sent_links()

    rss_items = fetch_rss_items()
    candidates = [it for it in rss_items if it["url"] not in sent]

    if not candidates:
        print("æ²¡æœ‰æ–°å†…å®¹ï¼ˆæˆ–éƒ½å·²æ¨é€ï¼‰ã€‚")
        return

    picked = []
    for it in candidates:
        sections = extract_sections_from_description(it.get("description_html", ""))

        if not any(sections.get(k) for k in SECTION_ALLOW):
            continue

        picked.append({
            "title": it["title"],
            "url": it["url"],
            "published": it.get("published", ""),
            "sections": sections
        })

        if len(picked) >= TOP_N:
            break

    if not picked:
        print("æ–°æ¡ç›®é‡Œæ²¡æœ‰è§£æåˆ°ã€è§‚å›½å†…ã€‘/ã€å¤§å…¬å¸ã€‘å†…å®¹ã€‚")
        return

    today = datetime.now().strftime("%Y-%m-%d")
    md_lines = [f"### ğŸ“° ä¸€è´¢æ—©æŠ¥ï¼ˆ{today}ï¼‰â€” åªçœ‹ã€è§‚å›½å†… / å¤§å…¬å¸ã€‘", ""]

    for idx, x in enumerate(picked, 1):
        md_lines.append(f"{idx}. **[{x['title']}]({x['url']})**")
        if x.get("published"):
            md_lines.append(f"   - æ—¶é—´ï¼š{x['published']}")

        for sec in SECTION_ALLOW:
            items = x["sections"].get(sec, [])
            if not items:
                continue
            md_lines.append(f"   - ****")
            for j, t in enumerate(items[:8], 1):
                md_lines.append(f"     {j}) {t}")

        md_lines.append("")

    markdown = "\n".join(md_lines).strip()
    dingtalk_send_markdown(f"ä¸€è´¢æ—©æŠ¥ç²¾é€‰ {today}", markdown)

    for x in picked:
        sent.add(x["url"])
    save_sent_links(sent)

    print(f"å·²æ¨é€ {len(picked)} æ¡ã€‚")


if __name__ == "__main__":
    main()
