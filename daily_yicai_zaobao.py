# -*- coding: utf-8 -*-
"""
ç¬¬ä¸€è´¢ç»ã€Œä¸€è´¢æ—©æŠ¥ã€(feed/669) â€” åªæŠ“ RSS description ä¸­çš„ã€è§‚å›½å†…ã€‘å’Œã€å¤§å…¬å¸ã€‘ä¸¤æ®µ

ä¿®å¤ç‚¹ï¼š
- GitHub Actions secrets è‹¥æœªé…ç½®ï¼Œä¼šæ³¨å…¥ç©ºå­—ç¬¦ä¸²ï¼Œå¯¼è‡´ RSSHub URL å˜æˆ "/"
- build_rsshub_url() ç°åœ¨ä¼šæŠŠç©ºå­—ç¬¦ä¸²è§†ä¸ºæœªé…ç½®ï¼Œè‡ªåŠ¨å›é€€åˆ°é»˜è®¤å€¼

ç¯å¢ƒå˜é‡ï¼ˆå¿…é€‰ï¼‰ï¼š
- DINGTALK_WEBHOOK: é’‰é’‰æœºå™¨äºº webhook
- DINGTALK_SECRET:  å¯é€‰ï¼Œæœºå™¨äººåŠ ç­¾ secret

ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰ï¼š
- RSSHUB_BASE: RSSHub å®ä¾‹åœ°å€ï¼Œé»˜è®¤ https://rsshub.app
- RSSHUB_ROUTE: é»˜è®¤ /yicai/feed/669
- TOP_N: æ¯å¤©æ¨é€æ¡æ•°ï¼Œé»˜è®¤ 8
"""

import os
import re
import json
import time
import hmac
import base64
import hashlib
import urllib.parse
from datetime import datetime
from typing import List, Dict, Any, Optional

import requests
import feedparser
from bs4 import BeautifulSoup, Tag

# =========================
# é…ç½®
# =========================
DATA_DIR = "data"
SENT_PATH = os.path.join(DATA_DIR, "sent_links.json")

DEFAULT_RSSHUB_BASE = "https://rsshub.app"
DEFAULT_RSSHUB_ROUTE = "/yicai/feed/669"

UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
FETCH_TIMEOUT = 20

TOP_N = int(os.getenv("TOP_N", "8"))

# åªæŠ½å–è¿™ä¸¤ä¸ªæ®µè½
SECTION_ALLOW = ["è§‚å›½å†…", "å¤§å…¬å¸"]

# æ ‡é¢˜é»‘åå•ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
TITLE_BLOCKLIST = ["æŠ¥å", "è¯¾ç¨‹", "è®­ç»ƒè¥", "ä¼˜æƒ ", "ä¿ƒé”€", "å¹¿å‘Š", "è½¯æ–‡", "å¸¦è´§"]


# =========================
# åŸºç¡€å·¥å…·
# =========================
def ensure_data_dir():
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR, exist_ok=True)

def load_sent_links() -> set:
    ensure_data_dir()
    if os.path.exists(SENT_PATH):
        try:
            with open(SENT_PATH, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()

def save_sent_links(sent: set):
    ensure_data_dir()
    with open(SENT_PATH, "w", encoding="utf-8") as f:
        json.dump(sorted(list(sent))[-5000:], f, ensure_ascii=False, indent=2)

def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def looks_blocked(title: str) -> bool:
    t = title or ""
    return any(x in t for x in TITLE_BLOCKLIST)

def safe_get(url: str) -> requests.Response:
    return requests.get(url, timeout=FETCH_TIMEOUT, headers={"User-Agent": UA})

def build_rsshub_url() -> str:
    """
    å…³é”®ä¿®å¤ï¼šæŠŠç©ºå­—ç¬¦ä¸²å½“ä½œæœªé…ç½®ï¼Œå›é€€é»˜è®¤å€¼
    """
    base_env = (os.getenv("RSSHUB_BASE") or "").strip()
    route_env = (os.getenv("RSSHUB_ROUTE") or "").strip()

    base = (base_env or DEFAULT_RSSHUB_BASE).rstrip("/")
    route = route_env or DEFAULT_RSSHUB_ROUTE

    if not route.startswith("/"):
        route = "/" + route

    full = f"{base}{route}"
    return full


# =========================
# RSS æ‹‰å–
# =========================
def fetch_rss_items() -> List[Dict[str, Any]]:
    url = build_rsshub_url()
    r = safe_get(url)
    r.raise_for_status()

    feed = feedparser.parse(r.content)
    items: List[Dict[str, Any]] = []

    for e in feed.entries[:80]:
        title = clean_text(getattr(e, "title", ""))
        link = clean_text(getattr(e, "link", ""))
        published = clean_text(getattr(e, "published", "") or getattr(e, "updated", ""))

        # RSS descriptionï¼ˆHTMLï¼‰
        desc = ""
        if hasattr(e, "summary"):
            desc = e.summary
        elif hasattr(e, "description"):
            desc = e.description

        if not title or not link:
            continue
        if looks_blocked(title):
            continue

        items.append({
            "title": title,
            "url": link,
            "published": published,
            "description_html": desc,
            "source": "RSSHub:yicai/feed/669"
        })

    return items


# =========================
# è§£æ descriptionï¼šåªæå–ã€è§‚å›½å†…ã€‘ã€å¤§å…¬å¸ã€‘
# =========================
def _normalize_section_name(text: str) -> Optional[str]:
    t = clean_text(text)
    if not t:
        return None
    t = t.replace("ã€", "").replace("ã€‘", "")
    t = re.sub(r"\s+", "", t)
    if t in SECTION_ALLOW:
        return t
    return None

def extract_sections_from_description(description_html: str) -> Dict[str, List[str]]:
    result = {name: [] for name in SECTION_ALLOW}
    if not description_html:
        return result

    soup = BeautifulSoup(description_html, "html.parser")
    ps = soup.find_all("p")
    current_section: Optional[str] = None

    def is_section_header_p(p: Tag) -> Optional[str]:
        strong = p.find("strong")
        if strong:
            sec = _normalize_section_name(strong.get_text(" "))
            if sec:
                return sec

        txt = clean_text(p.get_text(" "))
        m = re.search(r"ã€\s*([^ã€‘]+)\s*ã€‘", txt)
        if m:
            sec = _normalize_section_name(m.group(1))
            if sec:
                return sec
        return None

    def is_any_header_p(p: Tag) -> bool:
        strong = p.find("strong")
        if strong:
            maybe = clean_text(strong.get_text(" "))
            return bool(maybe)
        txt = clean_text(p.get_text(" "))
        return bool(re.match(r"^ã€.+ã€‘$", txt))

    for p in ps:
        sec = is_section_header_p(p)
        if sec:
            current_section = sec
            continue

        if not current_section:
            continue

        # é‡åˆ°æ–°çš„æ ‡é¢˜ï¼ˆå“ªæ€•ä¸æ˜¯æˆ‘ä»¬å…³å¿ƒçš„ï¼‰ï¼Œåœæ­¢æ”¶é›†
        if is_any_header_p(p) and is_section_header_p(p) is None:
            current_section = None
            continue

        txt = clean_text(p.get_text(" "))
        if not txt:
            continue
        if "ç‚¹å‡»" in txt and "å¬æ–°é—»" in txt:
            continue

        result[current_section].append(txt)

    # æ¸…æ´—ï¼šå»é‡ + å»æ‰å¤ªçŸ­
    for k in list(result.keys()):
        cleaned = []
        seen = set()
        for x in result[k]:
            x = clean_text(x)
            if len(x) < 10:
                continue
            if x in seen:
                continue
            seen.add(x)
            cleaned.append(x)
        result[k] = cleaned

    return result


# =========================
# é’‰é’‰æ¨é€
# =========================
def dingtalk_sign(timestamp_ms: str, secret: str) -> str:
    string_to_sign = f"{timestamp_ms}\n{secret}"
    h = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), hashlib.sha256).digest()
    return urllib.parse.quote_plus(base64.b64encode(h))

def dingtalk_send_markdown(title: str, markdown: str):
    webhook = (os.getenv("DINGTALK_WEBHOOK") or "").strip()
    if not webhook:
        raise RuntimeError("ç¼ºå°‘ç¯å¢ƒå˜é‡ DINGTALK_WEBHOOK")

    secret = (os.getenv("DINGTALK_SECRET") or "").strip()
    url = webhook
    if secret:
        ts = str(int(time.time() * 1000))
        sign = dingtalk_sign(ts, secret)
        joiner = "&" if "?" in url else "?"
        url = f"{url}{joiner}timestamp={ts}&sign={sign}"

    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": markdown}}
    r = requests.post(url, json=payload, timeout=FETCH_TIMEOUT)
    r.raise_for_status()


# =========================
# ä¸»æµç¨‹
# =========================
def main():
    sent = load_sent_links()

    rss_items = fetch_rss_items()
    candidates = [it for it in rss_items if it["url"] not in sent]

    if not candidates:
        print("æ²¡æœ‰æ–°å†…å®¹ï¼ˆæˆ–éƒ½å·²æ¨é€ï¼‰ã€‚")
        return

    picked = []
    for it in candidates:
        sections = extract_sections_from_description(it.get("description_html", ""))

        has_any = any(sections.get(k) for k in SECTION_ALLOW)
        if not has_any:
            continue

        picked.append({
            "title": it["title"],
            "url": it["url"],
            "published": it.get("published", ""),
            "sections": sections
        })

        if len(picked) >= TOP_N:
            break

    if not picked:
        print("æ–°æ¡ç›®é‡Œæ²¡æœ‰è§£æåˆ°ã€è§‚å›½å†…ã€‘/ã€å¤§å…¬å¸ã€‘å†…å®¹ã€‚")
        return

    today = datetime.now().strftime("%Y-%m-%d")
    md_lines = [f"### ğŸ“° ä¸€è´¢æ—©æŠ¥ï¼ˆ{today}ï¼‰â€” åªçœ‹ã€è§‚å›½å†… / å¤§å…¬å¸ã€‘", ""]

    for idx, x in enumerate(picked, 1):
        md_lines.append(f"{idx}. **[{x['title']}]({x['url']})**")
        if x.get("published"):
            md_lines.append(f"   - æ—¶é—´ï¼š{x['published']}")

        for sec in SECTION_ALLOW:
            items = x["sections"].get(sec, [])
            if not items:
                continue
            md_lines.append(f"   - ****")
            for j, t in enumerate(items[:8], 1):
                md_lines.append(f"     {j}) {t}")

        md_lines.append("")

    markdown = "\n".join(md_lines).strip()
    dingtalk_send_markdown(f"ä¸€è´¢æ—©æŠ¥ç²¾é€‰ {today}", markdown)

    for x in picked:
        sent.add(x["url"])
    save_sent_links(sent)

    print(f"å·²æ¨é€ {len(picked)} æ¡ã€‚")


if __name__ == "__main__":
    main()
