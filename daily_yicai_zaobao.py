# -*- coding: utf-8 -*-
import os
import re
import time
import hmac
import base64
import hashlib
import requests
import feedparser
from html import unescape
from datetime import datetime, timezone, timedelta

from bs4 import BeautifulSoup  # âœ… æ–°å¢ï¼šæ›´ç¨³çš„ HTML -> æ–‡æœ¬è§£æ


# ================= é…ç½®éƒ¨åˆ† =================
RSS_URLS = [
    "https://rsshub.app/yicai/feed/669",             # å®˜æ–¹æº
    "https://rss.fatpandac.com/yicai/feed/669",      # å¤‡ç”¨é•œåƒ 1
    "https://rsshub.liujiacai.net/yicai/feed/669",   # å¤‡ç”¨é•œåƒ 2
    "https://rsshub.feedlib.xyz/yicai/feed/669",     # å¤‡ç”¨é•œåƒ 3
    "https://rss.project44.net/yicai/feed/669",      # å¤‡ç”¨é•œåƒ 4
    "https://rsshub.rssforever.com/yicai/feed/669",  # å¤‡ç”¨é•œåƒ 5
]

UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
TIMEOUT = 30

TZ_CN = timezone(timedelta(hours=8))
# ===========================================


def fetch_feed():
    for url in RSS_URLS:
        try:
            print(f"Trying to fetch: {url}")
            r = requests.get(url, timeout=TIMEOUT, headers={"User-Agent": UA})
            r.raise_for_status()

            if "xml" not in r.headers.get("Content-Type", "").lower() and not r.text.strip().startswith("<?xml"):
                print(f"[RSS] Warning: Response via {url} might not be XML. Content-Type: {r.headers.get('Content-Type')}")

            feed = feedparser.parse(r.text)

            if feed.entries:
                print(f"[RSS] Success via {url}, entries count: {len(feed.entries)}")
                return feed
            else:
                print(f"[RSS] Parsed empty content via {url}, trying next...")

        except Exception as e:
            print(f"[RSS] Failed: {url} -> {e}")

    print("[RSS] All sources unavailable")
    return None


def get_entry_content(entry):
    """
    ä¼˜å…ˆè·å– RSS çš„å…¨æ–‡å†…å®¹ (content)ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·å–æ‘˜è¦ (description/summary)
    """
    if hasattr(entry, "content"):
        for c in entry.content:
            if c.get("value"):
                return c.get("value")

    if hasattr(entry, "summary_detail"):
        return entry.summary_detail.get("value", "")

    return entry.get("description", "")


def html_to_text_keep_lines(html_content: str) -> str:
    """
    ç”¨ BeautifulSoup æŠŠ HTML å˜æˆâ€œå°½é‡ä¿ç•™æ¢è¡Œç»“æ„â€çš„çº¯æ–‡æœ¬ã€‚
    """
    if not html_content:
        return ""

    html_content = unescape(html_content)

    # æœ‰äº› feedparser ä¼šæŠŠå†…å®¹å¡è¿› CDATAï¼Œé‡Œå¤´è¿˜æ˜¯ HTML
    soup = BeautifulSoup(html_content, "html.parser")

    # separator="\n" æ˜¯å…³é”®ï¼šæŠŠå—çº§å…ƒç´ /æ¢è¡Œç‚¹éƒ½å˜æˆçœŸå®æ¢è¡Œ
    text = soup.get_text(separator="\n")

    # ç»Ÿä¸€ç©ºç™½ï¼šæŠŠ NBSPã€å…¨è§’ç©ºæ ¼ç­‰å¤„ç†æ‰
    text = text.replace("\xa0", " ").replace("\u3000", " ")

    # å‹ç¼©å¤šä½™ç©ºè¡Œ
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def extract_numbered_titles(html_content):
    """
    ä» HTML/æ–‡æœ¬ä¸­æå–ç¼–å·è¦ç‚¹ã€‚
    æ”¯æŒï¼š1. / 1ã€ / 1ï¼ / 1) / 1ï¼‰ ä»¥åŠå‰é¢å¸¦ â€¢ â—‹ - ç­‰ç¬¦å·
    ä¸ä¾èµ–å¿…é¡»â€œè¡Œé¦–â€ï¼Œç”¨å…¨å±€ finditer æ›´ç¨³ã€‚
    """
    if not html_content:
        return []

    text = html_to_text_keep_lines(html_content)

    # å†åšä¸€æ¬¡è½»åº¦æ¸…æ´—ï¼Œé¿å…â€œç¼–å·ç²˜è¿â€
    # å¸¸è§æƒ…å†µï¼šå¤šä¸ªè¦ç‚¹åœ¨åŒä¸€è¡Œï¼Œç”¨ç©ºæ ¼éš”å¼€æˆ–ç¬¦å·éš”å¼€
    # æˆ‘ä»¬å…ˆæŠŠæ˜æ˜¾åˆ†éš”ç¬¦ä¹Ÿæ¢æˆæ¢è¡Œï¼Œæå‡è¯†åˆ«ç‡
    text2 = text
    text2 = re.sub(r"[â€¢â—‹â—â—†â—‡â– ]\s*", "\n", text2)  # ç¬¦å·å½“åˆ†éš”
    text2 = re.sub(r"\s{2,}", " ", text2)

    # âœ… å…¨å±€åŒ¹é…ç¼–å·
    # - å¼€å¤´å¯èƒ½æ˜¯æ¢è¡Œæˆ–è¡Œé¦–
    # - ç¼–å· 1~2 ä½ï¼ˆè¶³å¤Ÿè¦†ç›–æ—©æŠ¥æ¡æ•°ï¼‰
    # - åˆ†éš”ç¬¦ï¼š. ã€ ï¼ ) ï¼‰
    pattern = re.compile(r"(?:^|\n)\s*(?:[-â€“â€”]*)\s*(\d{1,2})\s*([\.ã€ï¼\)\ï¼‰])\s*(.+?)\s*(?=\n|$)")

    titles = []
    for m in pattern.finditer(text2):
        item = m.group(3).strip()
        # é¿å…æŠŠâ€œæ—¶é—´/æ¥æºâ€è¿™ç§ä¹ŸæŠ“è¿›æ¥ï¼šå¤ªçŸ­æˆ–åƒæ—¥æœŸå°±è¿‡æ»¤
        if len(item) < 4:
            continue
        titles.append(item)

    # å¦‚æœè¿˜æ²¡æŠ“åˆ°ï¼Œåšä¸€æ¬¡â€œè¶…çº§å…œåº•â€ï¼šä¸è¦æ±‚æ¢è¡Œè¾¹ç•Œ
    if not titles:
        pattern2 = re.compile(r"\b(\d{1,2})\s*([\.ã€ï¼\)\ï¼‰])\s*([^\n]{4,80})")
        for m in pattern2.finditer(text):
            item = m.group(3).strip()
            if len(item) < 4:
                continue
            titles.append(item)

    # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
    seen = set()
    uniq = []
    for t in titles:
        if t not in seen:
            seen.add(t)
            uniq.append(t)
    return uniq


def parse_zaobao_titles(entries):
    """
    ç²¾å‡†æŸ¥æ‰¾ã€ä»Šå¤©ã€‘å‘å¸ƒçš„ã€ä¸”æ ‡é¢˜åŒ…å«ã€æ—©æŠ¥ã€‘çš„æ–‡ç« 
    """
    today_cn = datetime.now(TZ_CN).date()
    print(f"DEBUG: Target Date (Beijing) = {today_cn}")

    target_entry = None

    # 1. ä¼˜å…ˆå¯»æ‰¾æ ‡é¢˜é‡Œå¸¦æœ‰ "æ—©æŠ¥" ä¸”æ˜¯ä»Šå¤©çš„æ–‡ç« 
    for e in entries:
        title = e.get("title", "No Title")
        if not hasattr(e, "published_parsed") or not e.published_parsed:
            continue

        dt_utc = datetime(*e.published_parsed[:6], tzinfo=timezone.utc)
        dt_cn = dt_utc.astimezone(TZ_CN)

        if dt_cn.date() == today_cn and "æ—©æŠ¥" in title:
            print(f"DEBUG: Found 'ZaoBao' article (Today): [{title}]")
            target_entry = e
            break

    # 2. å¦‚æœä»Šå¤©æ²¡æ‰¾åˆ°ï¼Œå°è¯•å›é€€ä¸€å¤©
    if not target_entry:
        print("DEBUG: No 'ZaoBao' found for today, checking yesterday...")
        yesterday_cn = today_cn - timedelta(days=1)
        for e in entries:
            title = e.get("title", "")
            if not hasattr(e, "published_parsed") or not e.published_parsed:
                continue

            dt_utc = datetime(*e.published_parsed[:6], tzinfo=timezone.utc)
            if dt_utc.astimezone(TZ_CN).date() == yesterday_cn and "æ—©æŠ¥" in title:
                print(f"DEBUG: Found 'ZaoBao' article (Yesterday): [{title}]")
                target_entry = e
                break

    # 3. å¼€å§‹è§£æå†…å®¹
    if target_entry:
        raw_content = get_entry_content(target_entry)
        results = extract_numbered_titles(raw_content)

        if results:
            return results
        else:
            print("DEBUG: Extraction failed.")
            print("DEBUG: raw_content repr preview (first 800 chars):")
            print(repr(raw_content[:800]))
            print("DEBUG: text preview after soup (first 800 chars):")
            print(html_to_text_keep_lines(raw_content)[:800])
            return []
    else:
        print("DEBUG: No 'ZaoBao' article found in recent feed.")
        return []


def sign(timestamp, secret):
    string_to_sign = f"{timestamp}\n{secret}"
    h = hmac.new(secret.encode(), string_to_sign.encode(), hashlib.sha256).digest()
    return base64.b64encode(h).decode()


def send_dingtalk(text):
    webhook = os.getenv("DINGTALK_WEBHOOK")
    secret = os.getenv("DINGTALK_SECRET")

    if not webhook or not secret:
        print("DingTalk not configured, skip send")
        return

    ts = str(round(time.time() * 1000))
    url = f"{webhook}&timestamp={ts}&sign={sign(ts, secret)}"

    payload = {
        "msgtype": "text",
        "text": {"content": text}
    }

    try:
        requests.post(url, json=payload, timeout=10).raise_for_status()
        print("DingTalk send success")
    except Exception as e:
        print(f"DingTalk send failed: {e}")


def main():
    feed = fetch_feed()
    if not feed:
        return

    titles = parse_zaobao_titles(feed.entries)

    if not titles:
        print("Error: Could not extract points. See DEBUG logs above.")
        return

    today_str = datetime.now(TZ_CN).strftime("%Y-%m-%d")
    lines = [f"ğŸ“° ä¸€è´¢æ—©æŠ¥ï¼ˆ{today_str}ï¼‰â€” è¦ç‚¹é€Ÿè§ˆ\n"]

    for i, t in enumerate(titles, 1):
        lines.append(f"{i}. {t}")

    final_text = "\n".join(lines)
    send_dingtalk(final_text)


if __name__ == "__main__":
    main()
