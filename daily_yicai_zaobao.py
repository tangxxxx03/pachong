# -*- coding: utf-8 -*-
"""
ç¬¬ä¸€è´¢ç»ã€Œä¸€è´¢æ—©æŠ¥ã€(feed/669) â€” åªæŠ“ RSS description ä¸­çš„ã€è§‚å›½å†…ã€‘å’Œã€å¤§å…¬å¸ã€‘ä¸¤æ®µ

åšæ³•ï¼š
- åˆ—è¡¨ï¼šé€šè¿‡ RSSHub è·¯ç”± /yicai/feed/669 è·å– RSS
- å†…å®¹ï¼šä¸å†æŠ“ /news/ æ­£æ–‡é¡µ
  è€Œæ˜¯è§£æ RSS <description>ï¼ˆHTMLï¼‰ï¼ŒåªæŠ½å–ï¼š
  1) ã€è§‚å›½å†…ã€‘æ ‡é¢˜åé¢çš„è‹¥å¹²æ®µå†…å®¹
  2) ã€å¤§å…¬å¸ã€‘æ ‡é¢˜åé¢çš„è‹¥å¹²æ®µå†…å®¹
- æ¨é€ï¼šé’‰é’‰æœºå™¨äºº Markdown
- å»é‡ï¼šdata/sent_links.json è®°å½•å·²æ¨é€ URL

ç¯å¢ƒå˜é‡ï¼ˆå¿…é€‰ï¼‰ï¼š
- DINGTALK_WEBHOOK: é’‰é’‰æœºå™¨äºº webhook
- DINGTALK_SECRET:  å¯é€‰ï¼Œæœºå™¨äººåŠ ç­¾ secret

ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰ï¼š
- RSSHUB_BASE: RSSHub å®ä¾‹åœ°å€ï¼Œé»˜è®¤ https://rsshub.app
- RSSHUB_ROUTE: é»˜è®¤ /yicai/feed/669

å¯é€‰ï¼š
- TOP_N: æ¯å¤©æ¨é€æ¡æ•°ï¼Œé»˜è®¤ 8
"""

import os
import re
import json
import time
import hmac
import base64
import hashlib
import urllib.parse
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple

import requests
import feedparser
from bs4 import BeautifulSoup, Tag

# =========================
# é…ç½®
# =========================
DATA_DIR = "data"
SENT_PATH = os.path.join(DATA_DIR, "sent_links.json")

DEFAULT_RSSHUB_BASE = "https://rsshub.app"
DEFAULT_RSSHUB_ROUTE = "/yicai/feed/669"

UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
FETCH_TIMEOUT = 20

TOP_N = int(os.getenv("TOP_N", "8"))

# åªæŠ½å–è¿™ä¸¤ä¸ªæ®µè½
SECTION_ALLOW = ["è§‚å›½å†…", "å¤§å…¬å¸"]

# æ ‡é¢˜é»‘åå•ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
TITLE_BLOCKLIST = ["æŠ¥å", "è¯¾ç¨‹", "è®­ç»ƒè¥", "ä¼˜æƒ ", "ä¿ƒé”€", "å¹¿å‘Š", "è½¯æ–‡", "å¸¦è´§"]


# =========================
# åŸºç¡€å·¥å…·
# =========================
def ensure_data_dir():
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR, exist_ok=True)

def load_sent_links() -> set:
    ensure_data_dir()
    if os.path.exists(SENT_PATH):
        try:
            with open(SENT_PATH, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()

def save_sent_links(sent: set):
    ensure_data_dir()
    with open(SENT_PATH, "w", encoding="utf-8") as f:
        json.dump(sorted(list(sent))[-5000:], f, ensure_ascii=False, indent=2)

def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def looks_blocked(title: str) -> bool:
    t = title or ""
    return any(x in t for x in TITLE_BLOCKLIST)

def safe_get(url: str) -> requests.Response:
    return requests.get(url, timeout=FETCH_TIMEOUT, headers={"User-Agent": UA})

def build_rsshub_url() -> str:
    base = os.getenv("RSSHUB_BASE", DEFAULT_RSSHUB_BASE).rstrip("/")
    route = os.getenv("RSSHUB_ROUTE", DEFAULT_RSSHUB_ROUTE)
    if not route.startswith("/"):
        route = "/" + route
    return f"{base}{route}"


# =========================
# RSS æ‹‰å–
# =========================
def fetch_rss_items() -> List[Dict[str, Any]]:
    url = build_rsshub_url()
    r = safe_get(url)
    r.raise_for_status()

    feed = feedparser.parse(r.content)
    items = []

    for e in feed.entries[:80]:
        title = clean_text(getattr(e, "title", ""))
        link = clean_text(getattr(e, "link", ""))
        published = clean_text(getattr(e, "published", "") or getattr(e, "updated", ""))

        # RSS descriptionï¼ˆHTMLï¼‰
        desc = ""
        if hasattr(e, "summary"):
            desc = e.summary
        elif hasattr(e, "description"):
            desc = e.description

        if not title or not link:
            continue
        if looks_blocked(title):
            continue

        items.append({
            "title": title,
            "url": link,
            "published": published,
            "description_html": desc,
            "source": "RSSHub:yicai/feed/669"
        })

    return items


# =========================
# è§£æ descriptionï¼šåªæå–ã€è§‚å›½å†…ã€‘ã€å¤§å…¬å¸ã€‘
# =========================
def _normalize_section_name(text: str) -> Optional[str]:
    """
    æŠŠç±»ä¼¼ï¼š
      ã€è§‚å›½å†…ã€‘ / è§‚å›½å†… / ã€ å¤§å…¬å¸ ã€‘ / å¤§å…¬å¸
    ç»Ÿä¸€æˆï¼šè§‚å›½å†… / å¤§å…¬å¸
    """
    t = clean_text(text)
    if not t:
        return None
    # å»æ‰æ‹¬å·è£…é¥°
    t = t.replace("ã€", "").replace("ã€‘", "")
    t = re.sub(r"\s+", "", t)
    if t in SECTION_ALLOW:
        return t
    return None

def extract_sections_from_description(description_html: str) -> Dict[str, List[str]]:
    """
    è¾“å…¥ï¼šRSS description çš„ HTMLï¼ˆé‡Œé¢æœ‰ <p><strong>ã€è§‚å›½å†…ã€‘</strong>...</p> ä¹‹ç±»ï¼‰
    è¾“å‡ºï¼š
    {
      "è§‚å›½å†…": ["æ¡ç›®1", "æ¡ç›®2", ...],
      "å¤§å…¬å¸": ["æ¡ç›®1", "æ¡ç›®2", ...]
    }

    è§„åˆ™ï¼ˆå°½é‡è´´åˆä½ æˆªå›¾é‚£ç§ç»“æ„ï¼‰ï¼š
    - ä»¥ <strong>ã€è§‚å›½å†…ã€‘</strong> æˆ–æ–‡æœ¬åŒ…å«â€œã€è§‚å›½å†…ã€‘â€ä½œä¸ºæ®µè½èµ·ç‚¹
    - æ”¶é›†å…¶åè¿ç»­çš„ <p> æ–‡æœ¬ï¼Œç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ª <strong>ã€xxxã€‘</strong> æ®µè½æ ‡é¢˜ä¸ºæ­¢
    - æ¯ä¸ª <p> é‡Œå¦‚æœæœ‰å¤šä¸ªé“¾æ¥/å¤šå¥ï¼Œä¼šæ•´æ®µæå–æˆä¸€æ¡æ–‡æœ¬ï¼ˆå¿…è¦æ—¶ä½ å¯ä»¥å†ç»†æ‹†ï¼‰
    """
    result = {name: [] for name in SECTION_ALLOW}
    if not description_html:
        return result

    soup = BeautifulSoup(description_html, "html.parser")

    # æŠŠ description å†…ä¸»è¦çš„ <p> æ‹¿å‡ºæ¥æŒ‰é¡ºåºæ‰«æ
    ps = soup.find_all("p")
    current_section: Optional[str] = None

    def is_section_header_p(p: Tag) -> Optional[str]:
        # 1) <p><strong>ã€è§‚å›½å†…ã€‘</strong></p>
        strong = p.find("strong")
        if strong:
            sec = _normalize_section_name(strong.get_text(" "))
            if sec:
                return sec

        # 2) ç›´æ¥æ–‡æœ¬åŒ…å«ã€è§‚å›½å†…ã€‘ï¼ˆé˜²æ­¢ç»“æ„ä¸æ ‡å‡†ï¼‰
        txt = clean_text(p.get_text(" "))
        m = re.search(r"ã€\s*([^ã€‘]+)\s*ã€‘", txt)
        if m:
            sec = _normalize_section_name(m.group(1))
            if sec:
                return sec

        return None

    def looks_like_new_any_header(p: Tag) -> bool:
        strong = p.find("strong")
        if strong:
            maybe = strong.get_text(" ")
            maybe = maybe.replace("ã€", "").replace("ã€‘", "")
            maybe = re.sub(r"\s+", "", maybe)
            return bool(maybe) and maybe != ""
        # æˆ–è€…æ–‡æœ¬åƒ ã€xxxã€‘
        txt = clean_text(p.get_text(" "))
        return bool(re.match(r"^ã€.+ã€‘$", txt))

    for p in ps:
        sec = is_section_header_p(p)
        if sec:
            current_section = sec
            continue

        if current_section:
            # ç¢°åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜æ®µï¼Œç»“æŸå½“å‰ section
            if looks_like_new_any_header(p) and is_section_header_p(p) is not None:
                # è¿™æ˜¯å¦ä¸€ä¸ªæˆ‘ä»¬å…³å¿ƒçš„ sectionï¼Œä¼šåœ¨ä¸Šé¢è¢«åˆ‡æ¢
                pass

            # å¦‚æœæ˜¯ä»»ä½•æ–°çš„ strong æ ‡é¢˜ï¼ˆä¸ç®¡æ˜¯ä¸æ˜¯æˆ‘ä»¬å…³å¿ƒçš„ï¼‰ï¼Œéƒ½ç»“æŸå½“å‰ section
            strong = p.find("strong")
            if strong:
                possible = _normalize_section_name(strong.get_text(" "))
                if possible is None:
                    # ä¸‹ä¸€ä¸ªæ ‡é¢˜ä¸æ˜¯æˆ‘ä»¬è¦çš„ï¼Œé‚£å½“å‰ section ä¹Ÿç»“æŸ
                    current_section = None
                    continue

            txt = clean_text(p.get_text(" "))
            if not txt:
                continue

            # åˆ é™¤æ˜æ˜¾çš„â€œç‚¹å‡»å¬æ–°é—»â€ç­‰å¹¿å‘Šå¥å¼ï¼ˆå¯æŒ‰éœ€å¢åŠ ï¼‰
            if "ç‚¹å‡»" in txt and "å¬æ–°é—»" in txt:
                continue

            # åŠ å…¥å½“å‰ section
            if current_section in result:
                result[current_section].append(txt)

    # æ¸…æ´—ï¼šå»é‡ + å»æ‰å¤ªçŸ­
    for k in list(result.keys()):
        cleaned = []
        seen = set()
        for x in result[k]:
            x = clean_text(x)
            if len(x) < 10:
                continue
            if x in seen:
                continue
            seen.add(x)
            cleaned.append(x)
        result[k] = cleaned

    return result


# =========================
# é’‰é’‰æ¨é€
# =========================
def dingtalk_sign(timestamp_ms: str, secret: str) -> str:
    string_to_sign = f"{timestamp_ms}\n{secret}"
    h = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), hashlib.sha256).digest()
    return urllib.parse.quote_plus(base64.b64encode(h))

def dingtalk_send_markdown(title: str, markdown: str):
    webhook = os.getenv("DINGTALK_WEBHOOK", "").strip()
    if not webhook:
        raise RuntimeError("ç¼ºå°‘ç¯å¢ƒå˜é‡ DINGTALK_WEBHOOK")

    secret = os.getenv("DINGTALK_SECRET", "").strip()
    url = webhook
    if secret:
        ts = str(int(time.time() * 1000))
        sign = dingtalk_sign(ts, secret)
        joiner = "&" if "?" in url else "?"
        url = f"{url}{joiner}timestamp={ts}&sign={sign}"

    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": markdown}}
    r = requests.post(url, json=payload, timeout=FETCH_TIMEOUT)
    r.raise_for_status()


# =========================
# ä¸»æµç¨‹
# =========================
def main():
    sent = load_sent_links()

    rss_items = fetch_rss_items()

    # åªæ¨â€œæ–°â€çš„
    candidates = [it for it in rss_items if it["url"] not in sent]

    if not candidates:
        print("æ²¡æœ‰æ–°å†…å®¹ï¼ˆæˆ–éƒ½å·²æ¨é€ï¼‰ã€‚")
        return

    picked = []
    for it in candidates:
        sections = extract_sections_from_description(it.get("description_html", ""))

        # ä½ è¦çš„ï¼šåªè¦è§‚å›½å†…ã€å¤§å…¬å¸ï¼›ä¸¤è€…éƒ½ç©ºå°±è·³è¿‡
        has_any = any(sections.get(k) for k in SECTION_ALLOW)
        if not has_any:
            continue

        picked.append({
            "title": it["title"],
            "url": it["url"],
            "published": it.get("published", ""),
            "sections": sections
        })

        if len(picked) >= TOP_N:
            break

    if not picked:
        print("æ–°æ¡ç›®é‡Œæ²¡æœ‰è§£æåˆ°ã€è§‚å›½å†…ã€‘/ã€å¤§å…¬å¸ã€‘å†…å®¹ã€‚")
        return

    today = datetime.now().strftime("%Y-%m-%d")
    md_lines = [f"### ğŸ“° ä¸€è´¢æ—©æŠ¥ï¼ˆ{today}ï¼‰â€” åªçœ‹ã€è§‚å›½å†… / å¤§å…¬å¸ã€‘", ""]

    for idx, x in enumerate(picked, 1):
        md_lines.append(f"{idx}. **[{x['title']}]({x['url']})**")
        if x.get("published"):
            md_lines.append(f"   - æ—¶é—´ï¼š{x['published']}")

        # è¾“å‡ºä¸¤ä¸ª section
        for sec in SECTION_ALLOW:
            items = x["sections"].get(sec, [])
            if not items:
                continue
            md_lines.append(f"   - ****")
            # æ§åˆ¶æ¯èŠ‚æœ€å¤šå±•ç¤º N æ¡ï¼Œé¿å…è¿‡é•¿
            for j, t in enumerate(items[:8], 1):
                md_lines.append(f"     {j}) {t}")

        md_lines.append("")

    markdown = "\n".join(md_lines).strip()
    dingtalk_send_markdown(f"ä¸€è´¢æ—©æŠ¥ç²¾é€‰ {today}", markdown)

    # è®°å½•å·²æ¨é€
    for x in picked:
        sent.add(x["url"])
    save_sent_links(sent)

    print(f"å·²æ¨é€ {len(picked)} æ¡ã€‚")


if __name__ == "__main__":
    main()
