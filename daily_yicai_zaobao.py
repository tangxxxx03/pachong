# -*- coding: utf-8 -*-
import os
import re
import time
import hmac
import base64
import hashlib
import requests
import feedparser
from html import unescape
from datetime import datetime, timezone, timedelta

# ================= é…ç½®éƒ¨åˆ† =================
# æ›´æ–°äº†é•œåƒæºåˆ—è¡¨ï¼Œå»æ‰äº†ä¸€äº›ä¸ç¨³å®šçš„ï¼ŒåŠ å…¥äº†ä¸€äº›æ–°çš„
RSS_URLS = [
    "https://rsshub.app/yicai/feed/669",             # å®˜æ–¹æº
    "https://rss.fatpandac.com/yicai/feed/669",      # å¤‡ç”¨é•œåƒ 1
    "https://rsshub.liujiacai.net/yicai/feed/669",   # å¤‡ç”¨é•œåƒ 2
    "https://rsshub.feedlib.xyz/yicai/feed/669",     # å¤‡ç”¨é•œåƒ 3
    "https://rss.project44.net/yicai/feed/669",      # å¤‡ç”¨é•œåƒ 4
    "https://rsshub.rssforever.com/yicai/feed/669",  # å¤‡ç”¨é•œåƒ 5
]

# å…³é”®ä¿®æ”¹ï¼šä¼ªè£…æˆ Windows ä¸‹çš„ Chrome æµè§ˆå™¨ï¼Œé˜²æ­¢è¢« 403 æ‹¦æˆª
UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

TIMEOUT = 30 

# å®šä¹‰åŒ—äº¬æ—¶åŒº (UTC+8)
TZ_CN = timezone(timedelta(hours=8))
# ===========================================

def fetch_feed():
    for url in RSS_URLS:
        try:
            print(f"Trying to fetch: {url}")
            # ä½¿ç”¨ä¼ªè£…çš„ UA å‘é€è¯·æ±‚
            r = requests.get(url, timeout=TIMEOUT, headers={"User-Agent": UA})
            r.raise_for_status()
            
            # å¢åŠ ä¸€æ­¥ï¼šæ£€æŸ¥è¿”å›çš„å†…å®¹æ˜¯å¦çœŸçš„æ˜¯ XML
            if "xml" not in r.headers.get("Content-Type", "").lower() and not r.text.strip().startswith("<?xml"):
                print(f"[RSS] Warning: Response via {url} might not be XML. Content-Type: {r.headers.get('Content-Type')}")
            
            feed = feedparser.parse(r.text)
            
            if feed.entries:
                print(f"[RSS] Success via {url}, entries count: {len(feed.entries)}")
                return feed
            else:
                print(f"[RSS] Parsed empty content via {url}, trying next...")
                
        except Exception as e:
            print(f"[RSS] Failed: {url} -> {e}")

    print("[RSS] All sources unavailable")
    return None


def get_entry_content(entry):
    """
    ä¼˜å…ˆè·å– RSS çš„å…¨æ–‡å†…å®¹ (content)ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·å–æ‘˜è¦ (description/summary)
    """
    if hasattr(entry, 'content'):
        for c in entry.content:
            if c.get('value'):
                return c.get('value')
    
    if hasattr(entry, 'summary_detail'):
        return entry.summary_detail.get('value', '')
        
    return entry.get('description', '')


def extract_numbered_titles(html_content):
    """
    ä» HTML æ–‡æœ¬ä¸­æå–å¸¦ç¼–å·çš„æ ‡é¢˜ã€‚
    """
    if not html_content:
        return []

    text = unescape(html_content)
    
    # é¢„å¤„ç† HTML æ ‡ç­¾ä»¥ä¿ç•™æ¢è¡Œç»“æ„
    text = re.sub(r"<(br|p|div)[^>]*>", "\n", text, flags=re.IGNORECASE)
    text = re.sub(r"</(p|div)>", "\n", text, flags=re.IGNORECASE)
    text = re.sub(r"<[^>]+>", "", text)

    titles = []
    for line in text.splitlines():
        line = line.strip()
        # åŒ¹é… "1. xxx" æˆ– "1ã€xxx"
        if re.match(r"^\s*\d+[\.ã€]\s*.+", line):
            clean_title = re.sub(r"^\s*\d+[\.ã€]\s*", "", line)
            titles.append(clean_title)

    return titles


def parse_zaobao_titles(entries):
    """
    ç²¾å‡†æŸ¥æ‰¾ã€ä»Šå¤©ã€‘å‘å¸ƒçš„ã€ä¸”æ ‡é¢˜åŒ…å«ã€æ—©æŠ¥ã€‘çš„æ–‡ç« 
    """
    today_cn = datetime.now(TZ_CN).date()
    print(f"DEBUG: Target Date (Beijing) = {today_cn}")

    target_entry = None

    # 1. ä¼˜å…ˆå¯»æ‰¾æ ‡é¢˜é‡Œå¸¦æœ‰ "æ—©æŠ¥" ä¸”æ˜¯ä»Šå¤©çš„æ–‡ç« 
    for e in entries:
        title = e.get("title", "No Title")
        if not hasattr(e, "published_parsed") or not e.published_parsed:
            continue
        
        dt_utc = datetime(*e.published_parsed[:6], tzinfo=timezone.utc)
        dt_cn = dt_utc.astimezone(TZ_CN)
        
        if dt_cn.date() == today_cn and "æ—©æŠ¥" in title:
            print(f"DEBUG: Found 'ZaoBao' article (Today): [{title}]")
            target_entry = e
            break

    # 2. å¦‚æœä»Šå¤©æ²¡æ‰¾åˆ°ï¼Œå°è¯•å›é€€ä¸€å¤©
    if not target_entry:
        print("DEBUG: No 'ZaoBao' found for today, checking yesterday...")
        yesterday_cn = today_cn - timedelta(days=1)
        for e in entries:
            title = e.get("title", "")
            if not hasattr(e, "published_parsed") or not e.published_parsed:
                continue
                
            dt_utc = datetime(*e.published_parsed[:6], tzinfo=timezone.utc)
            if dt_utc.astimezone(TZ_CN).date() == yesterday_cn and "æ—©æŠ¥" in title:
                print(f"DEBUG: Found 'ZaoBao' article (Yesterday): [{title}]")
                target_entry = e
                break

    # 3. å¼€å§‹è§£æå†…å®¹
    if target_entry:
        raw_content = get_entry_content(target_entry)
        results = extract_numbered_titles(raw_content)
        
        if results:
            return results
        else:
            print(f"DEBUG: Extraction failed. Preview of raw content (first 500 chars):")
            clean_preview = re.sub(r"<[^>]+>", "", raw_content)[:500]
            print(f"--- START RAW PREVIEW ---\n{clean_preview}\n--- END RAW PREVIEW ---")
            return []
    else:
        print("DEBUG: No 'ZaoBao' article found in recent feed.")
        return []


def sign(timestamp, secret):
    string_to_sign = f"{timestamp}\n{secret}"
    h = hmac.new(secret.encode(), string_to_sign.encode(), hashlib.sha256).digest()
    return base64.b64encode(h).decode()


def send_dingtalk(text):
    webhook = os.getenv("DINGTALK_WEBHOOK")
    secret = os.getenv("DINGTALK_SECRET")

    if not webhook or not secret:
        print("DingTalk not configured, skip send")
        return

    ts = str(round(time.time() * 1000))
    url = f"{webhook}&timestamp={ts}&sign={sign(ts, secret)}"

    payload = {
        "msgtype": "text",
        "text": {"content": text}
    }

    try:
        requests.post(url, json=payload, timeout=10).raise_for_status()
        print("DingTalk send success")
    except Exception as e:
        print(f"DingTalk send failed: {e}")


def main():
    feed = fetch_feed()
    if not feed:
        return

    titles = parse_zaobao_titles(feed.entries)
    
    if not titles:
        print("Error: Could not extract points. See DEBUG logs above.")
        return

    today_str = datetime.now(TZ_CN).strftime("%Y-%m-%d")
    lines = [f"ğŸ“° ä¸€è´¢æ—©æŠ¥ï¼ˆ{today_str}ï¼‰â€” è¦ç‚¹é€Ÿè§ˆ\n"]

    for i, t in enumerate(titles, 1):
        lines.append(f"{i}. {t}")

    final_text = "\n".join(lines)
    send_dingtalk(final_text)


if __name__ == "__main__":
    main()
