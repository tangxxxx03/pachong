# -*- coding: utf-8 -*-
"""
HR èµ„è®¯è‡ªåŠ¨æŠ“å–ï¼ˆä»…å½“å¤©ï¼‰ +ï¼ˆå¯é€‰ï¼‰é’‰é’‰æ¨é€ï¼ˆåŠ ç­¾ï¼‰
- è¦†ç›–ç«™ç‚¹ï¼ˆå‡åšçœŸå®æŠ“å–ï¼Œæ”¯æŒè‡ªå®šä¹‰ URL åˆ—è¡¨ï¼‰ï¼š
  1) äººç¤¾éƒ¨ï¼ˆmohrss.gov.cnï¼‰
  2) äººæ°‘ç½‘ï¼ˆpeople.com.cnï¼‰
  3) å…‰æ˜æ—¥æŠ¥ï¼ˆgmw.cnï¼‰
  4) åŒ—äº¬å¸‚äººç¤¾å±€ï¼ˆrsj.beijing.gov.cnï¼‰
  5) æ–°åç½‘ï¼ˆxinhuanet.comï¼‰
  6) ä¸­å›½äººåŠ›èµ„æºå¸‚åœºç½‘ï¼ˆchrm.mohrss.gov.cnï¼‰
  7) ä¸­å›½å…¬å…±æ‹›è˜ç½‘ï¼ˆjob.mohrss.gov.cnï¼‰
  8) ä¸­å›½å›½å®¶äººæ‰ç½‘ï¼ˆnewjobs.com.cnï¼‰
  9) ä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼ˆhrloo.comï¼‰*éƒ¨åˆ†æ ç›®å¯èƒ½éœ€ç™»å½•
  10) HRootï¼ˆhroot.comï¼‰
  11) å›½å®¶ç¨åŠ¡æ€»å±€Â·æ–°é—»åŠ¨æ€ï¼ˆchinatax.gov.cnï¼‰
  12) åŒ—äº¬å¸‚å¸æ³•å±€ï¼ˆsfj.beijing.gov.cnï¼‰
  13) å›½å®¶ç¤¾ä¼šä¿é™©å…¬å…±æœåŠ¡å¹³å°ï¼ˆsi.12333.gov.cnï¼‰
  14) ä¸­äººç½‘Â·äººåŠ›èµ„æºé¢‘é“ï¼ˆchinahrm.cnï¼‰
  15) ä¸­å›½å›½å®¶äººæ‰ç½‘Â·æ”¿ç­–æ³•è§„ï¼ˆnewjobs.com.cn çš„æ”¿ç­–æ ç›®ï¼‰
  16) åŒ—äº¬äººåŠ›èµ„æºæœåŠ¡åä¼š/è¡Œä¸šåä¼šï¼ˆéœ€åœ¨ç¯å¢ƒå˜é‡é‡Œé…ç½®åŸŸå/æ ç›® URLï¼‰
  17) å›½å®¶ç»Ÿè®¡å±€ï¼ˆstats.gov.cnï¼‰
- ä»…å½“å¤©ï¼šæœªèƒ½è§£æåˆ°æ—¥æœŸçš„æ¡ç›®åœ¨ ONLY_TODAY=1 æ—¶ä¼šè¢«ä¸¢å¼ƒ
- æ— äº¤äº’ inputï¼›é€‚é… GitHub Actions
- å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–å„ç«™ç‚¹æŠ“å–å…¥å£ï¼ˆé€—å·åˆ†éš” URLï¼‰

ä¾èµ–ï¼šrequests, beautifulsoup4
"""

import os
import re
import csv
import json
import time
import hmac
import base64
import hashlib
import urllib.parse
from datetime import datetime, timedelta
from urllib.parse import urljoin
from zoneinfo import ZoneInfo  # Python 3.9+

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

# ====================== ç¯å¢ƒå˜é‡é…ç½® ======================

SAVE_FORMAT = os.getenv("HR_SAVE_FORMAT", "both").strip().lower()  # csv/json/both
MAX_PER_SOURCE = int(os.getenv("HR_MAX_ITEMS", "10"))
ONLY_TODAY = os.getenv("HR_ONLY_TODAY", "1").strip().lower() in ("1", "true", "yes", "y")
TZ_STR = os.getenv("HR_TZ", "Asia/Shanghai").strip()
HTTP_PROXY = os.getenv("HTTP_PROXY", "").strip()
HTTPS_PROXY = os.getenv("HTTPS_PROXY", "").strip()

# é’‰é’‰
DINGTALK_WEBHOOK = os.getenv("DINGTALK_WEBHOOKHR", "").strip()
DINGTALK_SECRET = os.getenv("DINGTALK_SECRET_HR", "").strip()
DINGTALK_KEYWORD = os.getenv("DINGTALK_KEYWORD_HR", "").strip()

def now_tz():
    return datetime.now(ZoneInfo(TZ_STR))

# ====================== HTTP ä¼šè¯ï¼ˆé‡è¯•/è¶…æ—¶ï¼‰ ======================

def make_session():
    s = requests.Session()
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0.0.0 Safari/537.36"
        )
    }
    s.headers.update(headers)

    retries = Retry(
        total=3,
        backoff_factor=0.6,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=["GET", "POST"]
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=12)
    s.mount("http://", adapter)
    s.mount("https://", adapter)

    proxies = {}
    if HTTP_PROXY:
        proxies["http"] = HTTP_PROXY
    if HTTPS_PROXY:
        proxies["https"] = HTTPS_PROXY
    if proxies:
        s.proxies.update(proxies)

    return s

# ====================== é’‰é’‰å‘é€ï¼ˆåŠ ç­¾ï¼‰ ======================

def _sign_webhook(base_webhook: str, secret: str) -> str:
    if not secret:
        return base_webhook
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    return f"{base_webhook}&timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    if not DINGTALK_WEBHOOK or not DINGTALK_SECRET:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ WEBHOOK/SECRETï¼Œè·³è¿‡æ¨é€ã€‚")
        return False

    webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
    if DINGTALK_KEYWORD and (DINGTALK_KEYWORD not in title and DINGTALK_KEYWORD not in md_text):
        title = f"{DINGTALK_KEYWORD} | {title}"

    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    try:
        r = requests.post(webhook, json=payload, timeout=20)
        print("HR DingTalk resp:", r.status_code, r.text[:300])
        ok = (r.status_code == 200 and isinstance(r.json(), dict) and r.json().get("errcode") == 0)
        return ok
    except Exception as e:
        print("âŒ é’‰é’‰è¯·æ±‚å¼‚å¸¸ï¼š", e)
        return False

# ====================== æŠ“å–ç±» ======================

DEFAULT_SELECTORS = [
    ".list li", ".news-list li", ".content-list li", ".box-list li",
    "ul.list li", "ul.news li", "ul li", "li"
]

def as_list(env_name: str, defaults: list[str]) -> list[str]:
    """ä»ç¯å¢ƒå˜é‡å– URL åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå¦åˆ™ç”¨é»˜è®¤åˆ—è¡¨"""
    raw = os.getenv(env_name, "").strip()
    if not raw:
        return defaults
    return [u.strip() for u in raw.split(",") if u.strip()]

class HRNewsCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self._seen = set()

    # ------------ é€šç”¨æŠ“å–å™¨ ------------
    def crawl_generic(self, source_name: str, base: str | None, list_urls: list[str], selectors=None):
        if not list_urls:
            print(f"â„¹ï¸ {source_name}: æœªé…ç½®å…¥å£ URLï¼Œè·³è¿‡ã€‚")
            return
        selectors = selectors or DEFAULT_SELECTORS
        total = 0
        for url in list_urls:
            if total >= MAX_PER_SOURCE:
                break
            try:
                resp = self.session.get(url, timeout=15)
                resp.encoding = resp.apparent_encoding or "utf-8"
                if resp.status_code != 200:
                    print(f"âš ï¸ {source_name} è®¿é—®å¤±è´¥ {resp.status_code}: {url}")
                    continue
                soup = BeautifulSoup(resp.text, "html.parser")

                items = []
                for css in selectors:
                    items = soup.select(css)
                    if items:
                        break
                if not items:
                    # å…œåº•ï¼šæŠ“é¡µé¢é‡Œæ‰€æœ‰ a æ ‡ç­¾
                    items = soup.select("a")

                for node in items:
                    if total >= MAX_PER_SOURCE:
                        break
                    a = node if node.name == "a" else node.find("a")
                    if not a:
                        continue

                    title = self._norm(a.get_text())
                    if not title:
                        continue

                    href = a.get("href") or ""
                    full_url = urljoin(base or url, href)

                    # æ—¥æœŸï¼šä¼˜å…ˆæŸ¥å½“å‰èŠ‚ç‚¹/å…¶å†… date/time å…ƒç´ ï¼›å…¶æ¬¡çœ‹å…¶æ–‡æœ¬
                    date_text = self._find_date(node) or self._find_date(a)
                    if not date_text:
                        if ONLY_TODAY:
                            # åªè¦å½“å¤©ï¼šæ²¡æœ‰æ—¥æœŸçš„æ¡ç›®ä¸¢å¼ƒï¼Œé¿å…è¯¯æ”¶
                            continue
                        # éå½“å¤©ï¼šå¯åšå›é€€ç­–ç•¥ï¼Œè¿™é‡Œæˆ‘ä»¬ä¿æŒä¸¥æ ¼ï¼Œä¸å›å¡«æ—¥æœŸ
                        continue

                    # è¿‡æ»¤å½“å¤©
                    if ONLY_TODAY and (not self._is_today(date_text)):
                        continue

                    snippet = self._snippet(node)

                    item = {
                        "title": title,
                        "url": full_url,
                        "source": source_name,
                        "date": date_text,
                        "content": snippet
                    }
                    if self._push_if_new(item):
                        total += 1

            except Exception as e:
                print(f"âš ï¸ {source_name} æŠ“å–é”™è¯¯ {url}: {e}")

    # ------------ ç«™ç‚¹é€‚é…ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–å…¥å£ï¼‰ ------------
    def crawl_mohrss(self):
        # äººç¤¾éƒ¨
        urls = as_list("SRC_MOHRSS_URLS", [
            "https://www.mohrss.gov.cn/",
            "https://www.mohrss.gov.cn/SYrlzyhshbzb/zwgk/gzdt/index.html",
            "https://www.mohrss.gov.cn/SYrlzyhshbzb/zwgk/tzgg/index.html",
        ])
        self.crawl_generic("äººç¤¾éƒ¨", "https://www.mohrss.gov.cn", urls)

    def crawl_people(self):
        urls = as_list("SRC_PEOPLE_URLS", [
            "http://www.people.com.cn/",
        ])
        self.crawl_generic("äººæ°‘ç½‘", "http://www.people.com.cn", urls)

    def crawl_gmw(self):
        urls = as_list("SRC_GMW_URLS", [
            "https://www.gmw.cn/",
        ])
        self.crawl_generic("å…‰æ˜æ—¥æŠ¥", "https://www.gmw.cn", urls)

    def crawl_beijing_hrss(self):
        urls = as_list("SRC_RSJ_BJ_URLS", [
            "https://rsj.beijing.gov.cn/xxgk/tzgg/",
            "https://rsj.beijing.gov.cn/xxgk/gzdt/",
            "https://rsj.beijing.gov.cn/xxgk/zcfg/",
        ])
        self.crawl_generic("åŒ—äº¬äººç¤¾å±€", "https://rsj.beijing.gov.cn", urls)

    def crawl_xinhua(self):
        urls = as_list("SRC_XINHUA_URLS", [
            "https://www.xinhuanet.com/"
        ])
        self.crawl_generic("æ–°åç½‘", "https://www.xinhuanet.com", urls)

    def crawl_chrm(self):
        urls = as_list("SRC_CHRM_URLS", [
            "https://chrm.mohrss.gov.cn/"
        ])
        self.crawl_generic("ä¸­å›½äººåŠ›èµ„æºå¸‚åœºç½‘", "https://chrm.mohrss.gov.cn", urls)

    def crawl_job_mohrss(self):
        urls = as_list("SRC_JOB_MOHRSS_URLS", [
            "http://job.mohrss.gov.cn/"
        ])
        self.crawl_generic("ä¸­å›½å…¬å…±æ‹›è˜ç½‘", "http://job.mohrss.gov.cn", urls)

    def crawl_newjobs(self):
        urls = as_list("SRC_NEWJOBS_URLS", [
            "https://www.newjobs.com.cn/"
        ])
        self.crawl_generic("ä¸­å›½å›½å®¶äººæ‰ç½‘", "https://www.newjobs.com.cn", urls)

    def crawl_hrloo(self):
        urls = as_list("SRC_HRLOO_URLS", [
            "https://www.hrloo.com/"
        ])
        self.crawl_generic("ä¸‰èŒ…äººåŠ›èµ„æºç½‘", "https://www.hrloo.com", urls)

    def crawl_hroot(self):
        urls = as_list("SRC_HROOT_URLS", [
            "https://www.hroot.com/"
        ])
        self.crawl_generic("HRoot", "https://www.hroot.com", urls)

    def crawl_chinatax(self):
        urls = as_list("SRC_CHINATAX_URLS", [
            "https://www.chinatax.gov.cn/"
        ])
        self.crawl_generic("å›½å®¶ç¨åŠ¡æ€»å±€", "https://www.chinatax.gov.cn", urls)

    def crawl_bjsfj(self):
        urls = as_list("SRC_BJ_SFJ_URLS", [
            "https://sfj.beijing.gov.cn/"
        ])
        self.crawl_generic("åŒ—äº¬å¸‚å¸æ³•å±€", "https://sfj.beijing.gov.cn", urls)

    def crawl_si_12333(self):
        urls = as_list("SRC_SI_12333_URLS", [
            "https://si.12333.gov.cn/"
        ])
        self.crawl_generic("å›½å®¶ç¤¾ä¼šä¿é™©å¹³å°", "https://si.12333.gov.cn", urls)

    def crawl_chinahrm(self):
        urls = as_list("SRC_CHINAHRM_URLS", [
            "https://www.chinahrm.cn/"
        ])
        self.crawl_generic("ä¸­äººç½‘Â·äººåŠ›èµ„æºé¢‘é“", "https://www.chinahrm.cn", urls)

    def crawl_newjobs_policy(self):
        urls = as_list("SRC_NEWJOBS_POLICY_URLS", [
            "https://www.newjobs.com.cn/"
        ])
        self.crawl_generic("ä¸­å›½å›½å®¶äººæ‰ç½‘Â·æ”¿ç­–æ³•è§„", "https://www.newjobs.com.cn", urls)

    def crawl_bj_hr_associations(self):
        # ä¸¤ä¸ªåä¼šéœ€è¦ä½ æä¾›å®˜ç½‘/æ–°é—»æ ç›® URLï¼ˆé€—å·åˆ†éš”ï¼‰
        urls = as_list("SRC_BJ_HR_ASSOC_URLS", [
            # åœ¨ Actions é‡Œé…ç½®ï¼Œæ¯”å¦‚ï¼š
            # "https://www.bhrsa.org.cn/news/", "https://www.bhria.org.cn/notice/"
        ])
        if urls:
            self.crawl_generic("åŒ—äº¬äººåŠ›èµ„æºæœåŠ¡åä¼šï¼ˆå«è¡Œä¸šåä¼šï¼‰", None, urls)
        else:
            print("â„¹ï¸ åŒ—äº¬ HR åä¼šæœªé…ç½® URLï¼ˆSRC_BJ_HR_ASSOC_URLSï¼‰ï¼Œå·²è·³è¿‡ã€‚")

    def crawl_stats(self):
        urls = as_list("SRC_STATS_URLS", [
            "https://www.stats.gov.cn/"
        ])
        self.crawl_generic("å›½å®¶ç»Ÿè®¡å±€", "https://www.stats.gov.cn", urls)

    # ------------ ä¸»æµç¨‹ ------------
    def get_today_news(self):
        print("å¼€å§‹æŠ“å–äººåŠ›èµ„æºç›¸å…³èµ„è®¯ï¼ˆä»…å½“å¤©ï¼‰...")
        fns = [
            self.crawl_beijing_hrss,
            self.crawl_mohrss,
            self.crawl_people,
            self.crawl_gmw,
            self.crawl_xinhua,
            self.crawl_chrm,
            self.crawl_job_mohrss,
            self.crawl_newjobs,
            self.crawl_hrloo,
            self.crawl_hroot,
            self.crawl_chinatax,
            self.crawl_bjsfj,
            self.crawl_si_12333,
            self.crawl_chinahrm,
            self.crawl_newjobs_policy,
            self.crawl_bj_hr_associations,
            self.crawl_stats,
        ]
        for fn in fns:
            try:
                fn()
                time.sleep(0.8)
            except Exception as e:
                print(f"æŠ“å–æ¥æºæ—¶å‡ºé”™: {e}")
        return self.results

    # ------------ å·¥å…·æ–¹æ³• ------------
    def _push_if_new(self, item: dict) -> bool:
        key = item.get("url") or f"{item.get('title','')}|{item.get('date','')}"
        if key in self._seen:
            return False
        self._seen.add(key)
        self.results.append(item)
        return True

    @staticmethod
    def _norm(s: str) -> str:
        if not s:
            return ""
        return re.sub(r"\s+", " ", s.replace("\u3000", " ")).strip()

    @staticmethod
    def _snippet(node) -> str:
        try:
            text = node.get_text(" ", strip=True)
            text = re.sub(r"\s+", " ", text)
            return (text[:100] + "...") if len(text) > 100 else text
        except Exception:
            return "å†…å®¹è·å–ä¸­..."

    def _find_date(self, node) -> str:
        """å°½å¯èƒ½ä»èŠ‚ç‚¹æ‰¾åˆ° YYYY-MM-DDï¼ˆæ”¯æŒ å¹´/æœˆ/æ—¥ã€.ã€/ ç­‰æ›¿æ¢ï¼‰"""
        if not node:
            return ""
        raw = node.get_text(" ", strip=True)
        raw = raw.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "-")
        raw = raw.replace("/", "-").replace(".", "-")

        # ä¼˜å…ˆæŸ¥å­å…ƒç´ çš„ datetime/date class
        t = None
        for sel in ["time", ".time", ".date", "span.time", "span.date", "em.time", "em.date", "p.time", "p.date"]:
            sub = node.select_one(sel) if hasattr(node, "select_one") else None
            if sub:
                t = sub.get("datetime") or sub.get_text(strip=True)
                if t:
                    break
        if t:
            raw = t.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "-").replace("/", "-").replace(".", "-")

        # å®Œæ•´å¹´æœˆæ—¥
        m = re.search(r"(20\d{2}|19\d{2})-(\d{1,2})-(\d{1,2})", raw)
        if m:
            y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
            return f"{y:04d}-{mo:02d}-{d:02d}"
        # ä»…æœˆæ—¥ï¼ˆé»˜è®¤ä»Šå¹´ï¼‰
        m2 = re.search(r"\b(\d{1,2})-(\d{1,2})\b", raw)
        if m2:
            y = now_tz().year
            return f"{y:04d}-{int(m2.group(1)):02d}-{int(m2.group(2)):02d}"
        return ""

    def _parse_date(self, s: str):
        if not s:
            return None
        s = s.strip().replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "-").replace("/", "-").replace(".", "-")
        for fmt in ("%Y-%m-%d", "%y-%m-%d", "%Y-%m", "%m-%d"):
            try:
                dt = datetime.strptime(s, fmt)
                if fmt == "%m-%d":
                    dt = dt.replace(year=now_tz().year)
                if fmt == "%y-%m-%d" and dt.year < 2000:
                    dt = dt.replace(year=2000 + dt.year % 100)
                return dt.replace(tzinfo=ZoneInfo(TZ_STR))
            except ValueError:
                continue
        return None

    def _is_today(self, date_str: str) -> bool:
        dt = self._parse_date(date_str)
        if not dt:
            return False
        return dt.date() == now_tz().date()

    # ------------ è¾“å‡º ------------
    def save_results(self):
        if not self.results:
            print("æ²¡æœ‰æ‰¾åˆ°â€œå½“å¤©â€çš„ç›¸å…³èµ„è®¯")
            return None, None
        ts = now_tz().strftime("%Y%m%d_%H%M%S")
        csvf = jsonf = None

        if SAVE_FORMAT in ("csv", "both"):
            csvf = f"hr_news_{ts}.csv"
            with open(csvf, "w", newline="", encoding="utf-8-sig") as f:
                w = csv.DictWriter(f, fieldnames=["title", "url", "source", "date", "content"])
                w.writeheader()
                w.writerows(self.results)
            print(f"âœ… CSV å·²ä¿å­˜ï¼š{csvf}")

        if SAVE_FORMAT in ("json", "both"):
            jsonf = f"hr_news_{ts}.json"
            with open(jsonf, "w", encoding="utf-8") as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSON å·²ä¿å­˜ï¼š{jsonf}")

        return csvf, jsonf

    def to_markdown(self):
        if not self.results:
            return "ä»Šå¤©æœªæŠ“åˆ°ç¬¦åˆæ¡ä»¶çš„äººç¤¾ç±»èµ„è®¯ã€‚"
        lines = [
            "### ğŸ§© äººåŠ›èµ„æºèµ„è®¯æ¯æ—¥æ±‡æ€»ï¼ˆä»…å½“å¤©ï¼‰",
            f"**æ±‡æ€»æ—¶é—´ï¼š{now_tz().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}ï¼ˆ{TZ_STR}ï¼‰**",
            f"**ä»Šæ—¥èµ„è®¯ï¼š{len(self.results)} æ¡**",
            "",
            "ğŸ—ï¸ **èµ„è®¯è¯¦æƒ…**"
        ]
        for i, it in enumerate(self.results[:12], 1):
            lines.append(f"{i}. [{it['title']}]({it['url']})")
            lines.append(f"> ğŸ“… {it['date']}ã€€|ã€€ğŸ›ï¸ {it['source']}")
            if it.get("content"):
                lines.append(f"> {it['content'][:120]}")
            lines.append("")
        lines.append("ğŸ’¡ ä»Šæ—¥äººåŠ›èµ„æºèµ„è®¯å·²ä¸ºæ‚¨æ•´ç†å®Œæ¯•")
        return "\n".join(lines)

    def display_results(self):
        if not self.results:
            print("æ²¡æœ‰æ‰¾åˆ°â€œå½“å¤©â€çš„äººåŠ›èµ„æºç›¸å…³èµ„è®¯")
            return
        print(f"\næ‰¾åˆ° {len(self.results)} æ¡â€œå½“å¤©â€èµ„è®¯:\n" + "-" * 100)
        for i, it in enumerate(self.results, 1):
            print(f"{i}. {it['title']}")
            print(f"   æ¥æº: {it['source']} | æ—¥æœŸ: {it['date']}")
            print(f"   é“¾æ¥: {it['url']}")
            print(f"   å†…å®¹: {it['content']}")
            print("-" * 100)

# ====================== ç¨‹åºå…¥å£ ======================

def main():
    print("äººåŠ›èµ„æºèµ„è®¯è‡ªåŠ¨æŠ“å–å·¥å…·ï¼ˆä»…å½“å¤©ï¼‰")
    print("=" * 50)
    crawler = HRNewsCrawler()

    # æŠ“å–
    crawler.get_today_news()

    # æ‰“å°å±•ç¤º
    crawler.display_results()

    # ä¿å­˜
    crawler.save_results()

    #ï¼ˆå¯é€‰ï¼‰æ¨é€é’‰é’‰
    md = crawler.to_markdown()
    ok = send_dingtalk_markdown("äººåŠ›èµ„æºèµ„è®¯ï¼ˆå½“å¤©ï¼‰", md)
    print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "æœªæ¨é€/å¤±è´¥ âŒ")

if __name__ == "__main__":
    main()
