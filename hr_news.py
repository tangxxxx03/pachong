# -*- coding: utf-8 -*-
"""
HR èµ„è®¯è‡ªåŠ¨æŠ“å–ï¼ˆå·¥ä½œæ—¥ 8:30 æ¨é€ï¼‰
æ¥æºï¼šå›½åŠ¡é™¢ã€äººç¤¾éƒ¨ã€åŒ—äº¬å¸‚äººç¤¾å±€
è¿‡æ»¤ï¼šä»…è¿‘3å¤© + å…³é”®è¯ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡ KEYWORDS_HR è‡ªå®šä¹‰ï¼Œé€—å·åˆ†éš”ï¼‰
æ¨é€ï¼šé’‰é’‰è‡ªå®šä¹‰æœºå™¨äººï¼ˆå¼€å¯åŠ ç­¾ï¼‰
ä¾èµ–ï¼šrequests, beautifulsoup4, lxml

éœ€è¦çš„ Secretsï¼š
- DINGTALK_WEBHOOKHRï¼ˆå¿…å¡«ï¼šHR ç¾¤æœºå™¨äºº webhookï¼‰
- DINGTALK_SECRET_HRï¼ˆå¿…å¡«ï¼šHR æœºå™¨äººåŠ ç­¾å¯†é’¥ SEC...ï¼‰
- DINGTALK_KEYWORD_HRï¼ˆå¯é€‰ï¼šè‹¥å¯ç”¨â€œå…³é”®è¯â€å®‰å…¨ç­–ç•¥ï¼Œå°±å¡«ä½ çš„å…³é”®å­—ï¼‰
"""

import os, re, time, hmac, base64, hashlib, urllib.parse, json, csv
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup

# ========== é…ç½® ==========
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36"
}
TIMEOUT = 20
RECENT_DAYS = 3

DEFAULT_KEYWORDS = [
    "äººç¤¾","äººåŠ›èµ„æº","å°±ä¸š","ç¤¾ä¿","å…»è€","åŒ»ä¿","å·¥ä¼¤","å·¥èµ„","è–ªé…¬",
    "ç”¨å·¥","åŠ³åŠ¨","äººæ‰","åŸ¹è®­","æŠ€èƒ½","ç¨³å²—","å°±ä¸šæœåŠ¡","æ‹›è˜","æ‹›è˜ä¼š",
]
KEYWORDS = [k.strip() for k in (os.getenv("KEYWORDS_HR") or "").split(",") if k.strip()] or DEFAULT_KEYWORDS

# ========== é’‰é’‰ï¼ˆåŠ ç­¾ï¼‰ ==========
def _sign_webhook(base_webhook: str, secret: str) -> str:
    ts = str(round(time.time()*1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    return f"{base_webhook}&timestamp={ts}&sign={sign}"

def send_to_dingtalk_markdown_hr(title: str, md_text: str) -> bool:
    base = (os.getenv("DINGTALK_WEBHOOKHR") or "").strip()
    secret = (os.getenv("DINGTALK_SECRET_HR") or "").strip()
    if not base or not secret:
        print("âŒ ç¼ºå°‘ DINGTALK_WEBHOOKHR æˆ– DINGTALK_SECRET_HR"); return False
    kw = (os.getenv("DINGTALK_KEYWORD_HR") or "").strip()

    webhook = _sign_webhook(base, secret)
    if kw and (kw not in title and kw not in md_text):
        title = f"{kw} | {title}"

    payload = {"msgtype":"markdown","markdown":{"title":title,"text":md_text}}
    try:
        r = requests.post(webhook, json=payload, timeout=TIMEOUT)
        print("HR DingTalk resp:", r.status_code, r.text[:300])
        return (r.status_code == 200 and isinstance(r.json(), dict) and r.json().get("errcode") == 0)
    except Exception as e:
        print("âŒ é’‰é’‰å¼‚å¸¸ï¼š", e)
        return False

# ========== å·¥å…· ==========
DATE_PAT = re.compile(r"(\d{4})[-/\.](\d{1,2})[-/\.](\d{1,2})")

def parse_date(text: str) -> Optional[str]:
    m = DATE_PAT.search(text or "")
    if not m: return None
    y, mo, d = map(int, m.groups())
    try:
        return datetime(y, mo, d).strftime("%Y-%m-%d")
    except:
        return None

def is_recent(datestr: str, days:int=RECENT_DAYS) -> bool:
    try:
        d = datetime.strptime(datestr, "%Y-%m-%d")
        return (datetime.now() - d).days <= days
    except:
        return False

def match_keywords(title: str, content: str = "") -> bool:
    text = f"{title} {content}".lower()
    return any(k.lower() in text for k in KEYWORDS)

def http_get(url: str) -> Optional[requests.Response]:
    try:
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        if r.status_code == 200:
            r.encoding = r.apparent_encoding or "utf-8"
            return r
    except Exception as e:
        print(f"[HTTP] {url} -> {e}")
    return None

def extract_first_paragraph(url: str) -> str:
    r = http_get(url)
    if not r: return ""
    soup = BeautifulSoup(r.text, "lxml")
    for sel in ["article p","div.article p","div.TRS_Editor p","div#zoom p","div.article-con p","div.txt p","div.content p","p"]:
        ps = soup.select(sel)
        if ps:
            for p in ps:
                t = p.get_text(" ", strip=True)
                if len(t) >= 24:
                    return t[:120] + ("â€¦" if len(t)>120 else "")
    return ""

# ========== æŠ“å–ï¼šå›½åŠ¡é™¢ / äººç¤¾éƒ¨ / åŒ—äº¬å¸‚äººç¤¾å±€ ==========
def crawl_gov_cn() -> List[Dict]:
    results = []
    pages = [
        "https://www.gov.cn/yaowen/index.htm",
        "https://www.gov.cn/zhengce/zuixin.htm",
        "https://www.gov.cn/zhengce/index.htm",
    ]
    for url in pages:
        r = http_get(url)
        if not r: continue
        soup = BeautifulSoup(r.text, "lxml")
        items = []
        for sel in ["ul li a","div.list li a","div.biglist li a","div.news-list li a","a"]:
            items = soup.select(sel)
            if items: break
        for a in items[:25]:
            title = a.get_text(strip=True)
            href  = a.get("href","").strip()
            if not title or not href: continue
            link = href if href.startswith("http") else urllib.parse.urljoin(url, href)
            li_text = a.parent.get_text(" ", strip=True) if a.parent else title
            d = parse_date(li_text) or parse_date(r.text)
            if not d or not is_recent(d): continue
            summary = extract_first_paragraph(link)
            if not match_keywords(title, summary): continue
            results.append({"title": title, "url": link, "date": d, "source": "å›½åŠ¡é™¢", "summary": summary})
    return results

def crawl_mohrss() -> List[Dict]:
    results = []
    pages = [
        "https://www.mohrss.gov.cn/SYrlzyhshbzb/rsxw/",
        "https://www.mohrss.gov.cn/SYrlzyhshbzb/jiuye/",
        "https://www.mohrss.gov.cn/SYrlzyhshbzb/zcwj/",
    ]
    for url in pages:
        r = http_get(url)
        if not r: continue
        soup = BeautifulSoup(r.text, "lxml")
        items = []
        for sel in ["ul li a","div.list li a","div.news-list li a","a"]:
            items = soup.select(sel)
            if items: break
        for a in items[:25]:
            title = a.get_text(strip=True)
            href  = a.get("href","").strip()
            if not title or not href: continue
            link = href if href.startswith("http") else urllib.parse.urljoin(url, href)
            li_text = a.parent.get_text(" ", strip=True) if a.parent else title
            d = parse_date(li_text) or parse_date(r.text)
            if not d or not is_recent(d): continue
            summary = extract_first_paragraph(link)
            if not match_keywords(title, summary): continue
            results.append({"title": title, "url": link, "date": d, "source": "äººç¤¾éƒ¨", "summary": summary})
    return results

def crawl_bj_hrss() -> List[Dict]:
    results = []
    base = "https://rsj.beijing.gov.cn"
    pages = ["/xxgk/tzgg/","/xxgk/gzdt/"]
    for p in pages:
        url = base + p
        r = http_get(url)
        if not r: continue
        soup = BeautifulSoup(r.text, "lxml")
        items = []
        for sel in [".list li a",".news-list li a","ul li a","a"]:
            items = soup.select(sel)
            if items: break
        for a in items[:25]:
            title = a.get_text(strip=True)
            href  = a.get("href","").strip()
            if not title or not href: continue
            link = href if href.startswith("http") else urllib.parse.urljoin(url, href)
            li_text = a.parent.get_text(" ", strip=True) if a.parent else title
            d = parse_date(li_text) or parse_date(r.text)
            if not d or not is_recent(d): continue
            summary = extract_first_paragraph(link)
            if not match_keywords(title, summary): continue
            results.append({"title": title, "url": link, "date": d, "source": "åŒ—äº¬äººç¤¾å±€", "summary": summary})
    return results

# ========== æ±‡æ€» & æ¨é€ ==========
def dedup(items: List[Dict]) -> List[Dict]:
    seen, out = set(), []
    for it in items:
        key = (it["title"], it["url"])
        if key in seen: continue
        seen.add(key); out.append(it)
    return out

def save_results(items: List[Dict]):
    if not items: return
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    # CSV
    with open(f"hr_news_{ts}.csv","w",newline="",encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=["title","url","source","date","summary"])
        w.writeheader(); w.writerows(items)
    # JSON
    with open(f"hr_news_{ts}.json","w",encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

def format_markdown_daily(items: List[Dict]) -> str:
    now = datetime.now()
    head = [
        "### ğŸ§© äººåŠ›èµ„æºèµ„è®¯æ¯æ—¥æ±‡æ€»",
        f"**æ±‡æ€»æ—¶é—´ï¼š{now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}**",
        f"**ä»Šæ—¥èµ„è®¯ï¼š{len(items)} æ¡äººåŠ›èµ„æºç›¸å…³èµ„è®¯**",
        "",
        "ğŸ—ï¸ **èµ„è®¯è¯¦æƒ…**",
    ]
    body = []
    for idx, it in enumerate(items, 1):
        body.append(f"{idx}. [{it['title']}]({it['url']})")
        if it.get("summary"): body.append(f"> {it['summary']}")
        body.append(f"> ğŸ“… {it['date']}ã€€|ã€€ğŸ›ï¸ {it['source']}\n")
    tail = ["ğŸ’¡ æ—©å®‰ï¼ä»Šæ—¥äººåŠ›èµ„æºèµ„è®¯å·²ä¸ºæ‚¨æ•´ç†å®Œæ¯•"]
    return "\n".join(head + [""] + body + tail)

def main():
    print("HR æ¯æ—¥æŠ“å–å¼€å§‹ï¼š", datetime.now().isoformat(timespec="seconds"))
    all_items: List[Dict] = []
    try: all_items += crawl_gov_cn()
    except Exception as e: print("gov.cn æŠ“å–å¼‚å¸¸ï¼š", e)
    try: all_items += crawl_mohrss()
    except Exception as e: print("mohrss æŠ“å–å¼‚å¸¸ï¼š", e)
    try: all_items += crawl_bj_hrss()
    except Exception as e: print("åŒ—äº¬äººç¤¾å±€ æŠ“å–å¼‚å¸¸ï¼š", e)

    all_items = dedup(all_items)
    all_items.sort(key=lambda x: x["date"], reverse=True)

    if not all_items:
        send_to_dingtalk_markdown_hr("äººåŠ›èµ„æºèµ„è®¯æ¯æ—¥æ±‡æ€»", "ä»Šå¤©æœªæŠ“åˆ°ç¬¦åˆæ¡ä»¶çš„äººç¤¾/å°±ä¸šç±»èµ„è®¯ã€‚")
        print("æ— ç»“æœï¼Œå·²ç©ºæ’­æŠ¥ã€‚"); return

    TOP = min(8, len(all_items))  # æƒ³åªå‘ 2 æ¡å°±æ”¹æˆ 2
    chosen = all_items[:TOP]
    save_results(chosen)
    md = format_markdown_daily(chosen)
    ok = send_to_dingtalk_markdown_hr("äººåŠ›èµ„æºèµ„è®¯æ¯æ—¥æ±‡æ€»", md)
    print("æ¨é€ç»“æœï¼š", "æˆåŠŸ" if ok else "å¤±è´¥")

if __name__ == "__main__":
    main()
