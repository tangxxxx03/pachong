# -*- coding: utf-8 -*-
"""
HR èµ„è®¯è‡ªåŠ¨æŠ“å– + é’‰é’‰æ¨é€ï¼ˆå¼€å¯åŠ ç­¾ï¼‰
- å…¼å®¹æœ¬åœ°ä¸ GitHub Actionsï¼ˆæ— äº¤äº’ inputï¼‰
- é»˜è®¤ä½¿ç”¨ä½ æä¾›çš„ HR æœºå™¨äºº webhook/secretï¼›è‹¥å­˜åœ¨ç¯å¢ƒå˜é‡ï¼ˆGitHub Secretsï¼‰ä¼šè‡ªåŠ¨è¦†ç›–
- è¿è¡Œåï¼šæŠ“å– -> æ‰“å° -> ä¿å­˜ CSV/JSON -> æ¨é€é’‰é’‰ Markdown
ä¾èµ–ï¼šrequests, beautifulsoup4
"""

import os
import re
import csv
import json
import time
import hmac
import base64
import hashlib
import urllib.parse
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# ====== ä½ çš„ HR æœºå™¨äººï¼ˆé»˜è®¤å†™æ­»ï¼ŒSecrets ä¼šè‡ªåŠ¨è¦†ç›–å®ƒä»¬ï¼‰======
WEBHOOK_DEFAULT = "https://oapi.dingtalk.com/robot/send?access_token=9bb5d79464e0bf60f9c0f56ffd99744c4149fc43554982c0189ffe9c04162dce"
SECRET_DEFAULT  = "SEC4d9521a7cf6f96fcf6ea9832116df97b13300441f4e513f487a6502d833def75"

# âœ… ä¼˜å…ˆç”¨ç¯å¢ƒå˜é‡ï¼ˆGitHub Secretsï¼‰ï¼Œå¦åˆ™é€€å›é»˜è®¤å€¼
WEBHOOK = os.getenv("DINGTALK_WEBHOOKHR", WEBHOOK_DEFAULT).strip()
SECRET  = os.getenv("DINGTALK_SECRET_HR",  SECRET_DEFAULT).strip()
KEYWORD = os.getenv("DINGTALK_KEYWORD_HR", "").strip()  # è‹¥æœºå™¨äººå¯ç”¨â€œå…³é”®è¯â€ï¼Œå¯åœ¨ Secrets é‡Œè®¾ç½®

# ====================== é’‰é’‰å‘é€ï¼ˆåŠ ç­¾ï¼‰ ======================
def _sign_webhook(base_webhook: str, secret: str) -> str:
    if not secret:
        return base_webhook
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    return f"{base_webhook}&timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    if not WEBHOOK:
        print("âŒ ç¼ºå°‘ WEBHOOK"); return False
    if not SECRET:
        print("âŒ ç¼ºå°‘ SECRETï¼ˆä½ çš„æœºå™¨äººå¼€äº†â€œåŠ ç­¾â€å°±å¿…é¡»æä¾›ï¼‰"); return False

    webhook = _sign_webhook(WEBHOOK, SECRET)
    if KEYWORD and (KEYWORD not in title and KEYWORD not in md_text):
        title = f"{KEYWORD} | {title}"

    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    try:
        r = requests.post(webhook, json=payload, timeout=20)
        print("HR DingTalk resp:", r.status_code, r.text[:300])
        ok = (r.status_code == 200 and isinstance(r.json(), dict) and r.json().get("errcode") == 0)
        return ok
    except Exception as e:
        print("âŒ é’‰é’‰è¯·æ±‚å¼‚å¸¸ï¼š", e)
        return False

# ====================== æŠ“å–é€»è¾‘ï¼ˆä¿ç•™ä½ çš„ç»“æ„ï¼Œå»æ‰ inputï¼‰ ======================
class HRNewsCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.results = []

    def get_recent_hr_news(self):
        print("å¼€å§‹æŠ“å–äººåŠ›èµ„æºç›¸å…³èµ„è®¯...")
        sources = [self.crawl_beijing_hrss, self.crawl_mohrss, self.crawl_hr_portals]
        for source in sources:
            try:
                source()
                time.sleep(1.2)
            except Exception as e:
                print(f"æŠ“å–æ¥æºæ—¶å‡ºé”™: {e}")
                continue
        return self.results

    def crawl_beijing_hrss(self):
        print("æ­£åœ¨æŠ“å–åŒ—äº¬äººç¤¾å±€ä¿¡æ¯...")
        base_url = "https://rsj.beijing.gov.cn"
        urls_to_try = ['/xxgk/tzgg/', '/xxgk/gzdt/', '/xxgk/zcfg/']
        for url_path in urls_to_try:
            try:
                url = base_url + url_path
                response = self.session.get(url, headers=self.headers, timeout=15)
                response.encoding = 'utf-8'
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    selectors = ['.list li', '.news-list li', '.content-list li', 'ul li a']
                    for selector in selectors:
                        items = soup.select(selector)
                        if items:
                            for item in items[:10]:
                                self.process_news_item(item, base_url, 'åŒ—äº¬äººç¤¾å±€')
                            break
                time.sleep(0.6)
            except Exception as e:
                print(f"æŠ“å–åŒ—äº¬äººç¤¾å±€ {url_path} æ—¶å‡ºé”™: {e}")
                continue

    def crawl_mohrss(self):
        print("æ­£åœ¨è·å–äººç¤¾éƒ¨ç›¸å…³ä¿¡æ¯ï¼ˆç¤ºä¾‹ï¼‰...")
        mock_news = [
            {
                'title': 'äººåŠ›èµ„æºå’Œç¤¾ä¼šä¿éšœéƒ¨å‘å¸ƒæœ€æ–°å°±ä¸šä¿ƒè¿›æ”¿ç­–',
                'url': 'https://www.mohrss.gov.cn/SYrlzyhshbzb/zwgk/202310/t20231015_123456.html',
                'source': 'äººç¤¾éƒ¨',
                'date': (datetime.now() - timedelta(days=15)).strftime('%Y-%m-%d'),
                'content': 'ä¸ºè¿›ä¸€æ­¥ä¿ƒè¿›å°±ä¸šï¼Œäººç¤¾éƒ¨æ¨å‡ºå°±ä¸šä¿ƒè¿›æªæ–½ï¼ŒåŒ…æ‹¬æŠ€èƒ½åŸ¹è®­è¡¥è´´ã€åˆ›ä¸šæ‰¶æŒæ”¿ç­–ç­‰ã€‚'
            },
            {
                'title': '2023å¹´ç¤¾ä¼šä¿é™©ç¼´è´¹åŸºæ•°è°ƒæ•´é€šçŸ¥',
                'url': 'https://www.mohrss.gov.cn/SYrlzyhshbzb/zwgk/202310/t20231008_123457.html',
                'source': 'äººç¤¾éƒ¨',
                'date': (datetime.now() - timedelta(days=22)).strftime('%Y-%m-%d'),
                'content': 'å„åœ°ç¤¾ä¼šä¿é™©ç¼´è´¹åŸºæ•°å°†æ ¹æ®ä¸Šå¹´åº¦åœ¨å²—èŒå·¥å¹³å‡å·¥èµ„è¿›è¡Œç›¸åº”è°ƒæ•´ã€‚'
            }
        ]
        for news in mock_news:
            if self.is_recent_news(news['date']):
                self.results.append(news)

    def crawl_hr_portals(self):
        print("æ­£åœ¨è·å–äººåŠ›èµ„æºé—¨æˆ·ç½‘ç«™ä¿¡æ¯ï¼ˆç¤ºä¾‹ï¼‰...")
        mock_portal_news = [
            {
                'title': '2023å¹´ç¬¬å››å­£åº¦äººåŠ›èµ„æºå¸‚åœºä¾›éœ€æŠ¥å‘Š',
                'url': 'https://www.chinahr.com/news/202310/123456.html',
                'source': 'ä¸­å›½äººåŠ›èµ„æºç½‘',
                'date': (datetime.now() - timedelta(days=10)).strftime('%Y-%m-%d'),
                'content': 'ä¿¡æ¯æŠ€æœ¯ã€æ–°èƒ½æºç­‰è¡Œä¸šäººæ‰éœ€æ±‚æŒç»­æ—ºç››ï¼Œå¸‚åœºä¾›éœ€åŸºæœ¬å¹³è¡¡ã€‚'
            },
            {
                'title': 'çµæ´»ç”¨å·¥æ”¿ç­–æœ€æ–°è§£è¯»',
                'url': 'https://www.51job.com/news/202310/123457.html',
                'source': 'å‰ç¨‹æ— å¿§',
                'date': (datetime.now() - timedelta(days=25)).strftime('%Y-%m-%d'),
                'content': 'é’ˆå¯¹çµæ´»ç”¨å·¥çš„æœ€æ–°æ”¿ç­–è¦æ±‚ï¼Œä¸“å®¶è§£è¯»ï¼Œå¸®åŠ©ä¼ä¸šåˆè§„ç”¨å·¥ã€‚'
            },
            {
                'title': 'æ•°å­—åŒ–è½¬å‹ä¸­çš„äººåŠ›èµ„æºç®¡ç†å˜é©',
                'url': 'https://www.zhaopin.com/trends/202309/123458.html',
                'source': 'æ™ºè”æ‹›è˜',
                'date': (datetime.now() - timedelta(days=40)).strftime('%Y-%m-%d'),
                'content': 'ä¼ä¸šæ•°å­—åŒ–è½¬å‹å¯¹äººåŠ›èµ„æºç®¡ç†æå‡ºäº†æ–°çš„è¦æ±‚å’ŒæŒ‘æˆ˜ã€‚'
            }
        ]
        for news in mock_portal_news:
            if self.is_recent_news(news['date']):
                self.results.append(news)

    def process_news_item(self, item, base_url, source):
        try:
            link = item.find('a')
            if not link:
                return
            title = link.get_text().strip()
            href = link.get('href', '')
            if href.startswith('/'):
                full_url = base_url + href
            elif href.startswith('http'):
                full_url = href
            else:
                return

            date_text = ""
            date_pattern = r'(\d{4}-\d{2}-\d{2})|(\d{4}/\d{2}/\d{2})|(\d{2}-\d{2}-\d{2})'
            date_match = re.search(date_pattern, item.get_text())
            if date_match:
                date_text = date_match.group()
            if not date_text or len(date_text) < 8:
                date_text = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

            news_item = {
                'title': title,
                'url': full_url,
                'source': source,
                'date': date_text,
                'content': self.extract_content_snippet(item)
            }
            if self.is_recent_news(date_text):
                self.results.append(news_item)
        except Exception as e:
            print(f"å¤„ç†æ–°é—»æ¡ç›®æ—¶å‡ºé”™: {e}")

    def extract_content_snippet(self, item):
        try:
            text = item.get_text(" ", strip=True)
            return (text[:100] + '...') if len(text) > 100 else text
        except:
            return "å†…å®¹è·å–ä¸­..."

    def is_recent_news(self, date_str, days=60):
        try:
            for fmt in ('%Y-%m-%d', '%Y/%m/%d', '%y-%m-%d', '%m-%d'):
                try:
                    news_date = datetime.strptime(date_str, fmt)
                    if news_date.year < 2000:
                        news_date = news_date.replace(year=2000 + news_date.year % 100)
                    break
                except ValueError:
                    continue
            else:
                return True
            return (datetime.now() - news_date).days <= days
        except:
            return True

    def save_results(self):
        if not self.results:
            print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³èµ„è®¯")
            return None, None
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        csvf = f'hr_news_{ts}.csv'
        jsonf = f'hr_news_{ts}.json'
        with open(csvf, 'w', newline='', encoding='utf-8-sig') as f:
            w = csv.DictWriter(f, fieldnames=['title','url','source','date','content'])
            w.writeheader()
            w.writerows(self.results)
        with open(jsonf, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {csvf}, {jsonf}")
        return csvf, jsonf

    def to_markdown(self):
        if not self.results:
            return "ä»Šå¤©æœªæŠ“åˆ°ç¬¦åˆæ¡ä»¶çš„äººç¤¾ç±»èµ„è®¯ã€‚"
        lines = [
            "### ğŸ§© äººåŠ›èµ„æºèµ„è®¯æ¯æ—¥æ±‡æ€»",
            f"**æ±‡æ€»æ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}**",
            f"**ä»Šæ—¥èµ„è®¯ï¼š{len(self.results)} æ¡**",
            "",
            "ğŸ—ï¸ **èµ„è®¯è¯¦æƒ…**"
        ]
        for i, it in enumerate(self.results[:8], 1):
            lines.append(f"{i}. [{it['title']}]({it['url']})")
            lines.append(f"> ğŸ“… {it['date']}ã€€|ã€€ğŸ›ï¸ {it['source']}")
            if it.get("content"):
                lines.append(f"> {it['content'][:120]}")
            lines.append("")
        lines.append("ğŸ’¡ æ—©å®‰ï¼ä»Šæ—¥äººåŠ›èµ„æºèµ„è®¯å·²ä¸ºæ‚¨æ•´ç†å®Œæ¯•")
        return "\n".join(lines)

def main():
    print("äººåŠ›èµ„æºèµ„è®¯è‡ªåŠ¨æŠ“å–å·¥å…·")
    print("=" * 50)
    crawler = HRNewsCrawler()
    crawler.get_recent_hr_news()
    # æ‰“å° & ä¿å­˜
    if crawler.results:
        print(f"\næ‰¾åˆ° {len(crawler.results)} æ¡èµ„è®¯ï¼š\n" + "-"*80)
        for i, it in enumerate(crawler.results, 1):
            print(f"{i}. {it['title']} | {it['source']} | {it['date']}")
        crawler.save_results()
    else:
        print("æ²¡æœ‰æŠ“åˆ°èµ„è®¯ã€‚")
    # æ¨é€é’‰é’‰
    md = crawler.to_markdown()
    ok = send_dingtalk_markdown("äººåŠ›èµ„æºèµ„è®¯æ¯æ—¥æ±‡æ€»", md)
    print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥ âŒ")

if __name__ == "__main__":
    main()
