# -*- coding: utf-8 -*-
"""
ä¸‰èŒ…æ—¥æŠ¥ + è´¢å¯Œä¸­æ–‡ç½‘ï¼ˆå•†ä¸š/ä¸“æ ï¼‰åˆå¹¶çˆ¬è™«
------------------------------------------------
åŠŸèƒ½ï¼š
1. ä¸‰èŒ…äººåŠ›èµ„æºç½‘ï¼šæŠ“å–æŒ‡å®šæ—¥æœŸçš„ã€Šä¸‰èŒ…æ—¥æŠ¥ã€‹æ ‡é¢˜åˆ—è¡¨ï¼›
2. è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ï¼šæŠ“å–æŒ‡å®šæ—¥æœŸçš„æ–°é—» / ä¸“æ æ­£æ–‡ï¼›
3. è°ƒç”¨ç¡…åŸºæµåŠ¨ï¼ˆOpenAI å…¼å®¹ï¼‰ç”Ÿæˆä¸€å¥è¯ä¸­æ–‡æ‘˜è¦ï¼ˆå¸¦â€œé˜²å¹»è§‰â€å…œåº•ï¼‰ï¼›
4. åˆå¹¶æˆä¸€æ¡é’‰é’‰ Markdown æ¶ˆæ¯ï¼ˆç¼–å·è¿ç»­ã€å¯ç‚¹å‡»è·³è½¬ï¼‰ï¼›
5. é€šè¿‡ä¸€ä¸ªæˆ–å¤šä¸ªé’‰é’‰æœºå™¨äººæ¨é€ã€‚

ä¾èµ–ï¼ˆrequirements.txtï¼‰ï¼š
- requests
- beautifulsoup4
"""

import os
import re
import time
import hmac
import ssl
import base64
import hashlib
import csv
from datetime import datetime, date, timedelta, timezone
from urllib.parse import urljoin, quote_plus

import requests
from bs4 import BeautifulSoup, Tag

# ========== é€šç”¨å·¥å…· ==========
try:
    from zoneinfo import ZoneInfo
except Exception:  # py<3.9
    from backports.zoneinfo import ZoneInfo  # type: ignore


def _tz():
    return ZoneInfo("Asia/Shanghai")


def now_tz():
    return datetime.now(_tz())


def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def zh_weekday(dt: datetime) -> str:
    return ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][dt.weekday()]


# ========== é’‰é’‰å·¥å…·ï¼ˆå¤šæœºå™¨äººï¼‰ ==========

def sign_dingtalk(secret: str, timestamp_ms: int) -> str:
    string_to_sign = f"{timestamp_ms}\n{secret}"
    hmac_code = hmac.new(
        secret.encode("utf-8"),
        string_to_sign.encode("utf-8"),
        digestmod=hashlib.sha256,
    ).digest()
    return quote_plus(base64.b64encode(hmac_code))


def send_dingtalk_markdown(title: str, text: str):
    """
    å°† Markdown æ–‡æœ¬å‘é€åˆ°ä¸€ä¸ªæˆ–å¤šä¸ªé’‰é’‰æœºå™¨äººã€‚
    éœ€è¦ç¯å¢ƒå˜é‡ï¼š
    - DINGTALK_BASES   : webhook åŸºç¡€ URLï¼Œå¤šä¸ªç”¨è‹±æ–‡é€—å·åˆ†éš”
    - DINGTALK_SECRETS : å¯¹åº”çš„ secretï¼Œå¤šä¸ªç”¨è‹±æ–‡é€—å·åˆ†éš”
    """
    bases_raw = os.getenv("DINGTALK_BASES", "").strip()
    secrets_raw = os.getenv("DINGTALK_SECRETS", "").strip()

    if not bases_raw or not secrets_raw:
        print("ğŸ’¡ æœªé…ç½® DINGTALK_BASES / DINGTALK_SECRETSï¼Œè·³è¿‡é’‰é’‰æ¨é€ã€‚")
        return

    bases = [b.strip() for b in bases_raw.split(",") if b.strip()]
    secrets = [s.strip() for s in secrets_raw.split(",") if s.strip()]

    if not bases or len(bases) != len(secrets):
        print("âš ï¸ DINGTALK_BASES ä¸ DINGTALK_SECRETS æ•°é‡ä¸ä¸€è‡´ï¼Œè·³è¿‡é’‰é’‰æ¨é€ã€‚")
        return

    for idx, (base_url, secret) in enumerate(zip(bases, secrets), start=1):
        try:
            ts = int(time.time() * 1000)
            sign = sign_dingtalk(secret, ts)
            full_url = f"{base_url}&timestamp={ts}&sign={sign}"

            payload = {
                "msgtype": "markdown",
                "markdown": {"title": title, "text": text},
                "at": {"isAtAll": False},
            }

            print(f"\nğŸ“¨ æ­£åœ¨å‘ç¬¬ {idx} ä¸ªé’‰é’‰æœºå™¨äººå‘é€æ¶ˆæ¯...")
            resp = requests.post(full_url, json=payload, timeout=10)
            print(f"  é’‰é’‰è¿”å›çŠ¶æ€ç ï¼š{resp.status_code}")
            try:
                print("  é’‰é’‰è¿”å›ï¼š", resp.text[:300])
            except Exception:
                pass

        except Exception as e:
            print(f"  âš ï¸ ç¬¬ {idx} ä¸ªé’‰é’‰æœºå™¨äººå‘é€å¤±è´¥ï¼š{e}")


# ========== requests Sessionï¼ˆå«è€ TLS å…¼å®¹ï¼‰ ==========

class LegacyTLSAdapter(requests.adapters.HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)


def make_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120 Safari/537.36"
        ),
        "Accept-Language": "zh-CN,zh;q=0.9",
    })
    s.mount("https://", LegacyTLSAdapter())
    return s


# ========== ä¸€ã€ä¸‰èŒ…æ—¥æŠ¥çˆ¬è™« ==========

CN_TITLE_DATE = re.compile(
    r"[ï¼ˆ(]\s*(20\d{2})\s*[å¹´\-/.]\s*(\d{1,2})\s*[æœˆ\-/.]\s*(\d{1,2})\s*[)ï¼‰]"
)


def date_from_bracket_title(text: str):
    m = CN_TITLE_DATE.search(text or "")
    if not m:
        return None
    try:
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        return date(y, mo, d)
    except Exception:
        return None


def looks_like_numbered(text: str) -> bool:
    return bool(
        re.match(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*\S+", text or "")
    )


CIRCLED = "â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©"


def strip_leading_num(t: str) -> str:
    t = re.sub(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*", "", t)
    t = re.sub(r"^\s*[" + CIRCLED + r"]\s*", "", t)
    t = re.sub(r"^\s*[ï¼-ï¼™]+\s*[ã€.ï¼]\s*", "", t)
    return t.strip()


class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = 1

        t = (os.getenv("HR_TARGET_DATE") or "").strip()
        if t:
            try:
                y, m, d = map(int, re.split(r"[-/\.]", t))
                self.target_date = date(y, m, d)
            except Exception:
                print("âš ï¸ HR_TARGET_DATE è§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šæ—¥ã€‚")
                self.target_date = now_tz().date()
        else:
            self.target_date = now_tz().date()

        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        self.sources = [
            u.strip()
            for u in os.getenv(
                "SRC_HRLOO_URLS",
                "https://www.hrloo.com/,https://www.hrloo.com/news/hr",
            ).split(",")
            if u.strip()
        ]
        print(
            f"[HRLOO CFG] target_date={self.target_date} "
            f"{zh_weekday(now_tz())} sources={self.sources}"
        )

    # ---- å¯¹å¤–å…¥å£ ----
    def crawl(self):
        for base in self.sources:
            if self._crawl_source(base):
                break

    # ---- æŠ“é¦–é¡µï¼Œæ‰¾åˆ°ã€Šä¸‰èŒ…æ—¥æŠ¥ã€‹ ----
    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=20)
        except Exception as e:
            print("é¦–é¡µè¯·æ±‚å¼‚å¸¸ï¼š", base, e)
            return False
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥ï¼š", base, r.status_code)
            return False

        soup = BeautifulSoup(r.text, "html.parser")

        # 1ï¼‰ä¼˜å…ˆèµ°â€œå®¹å™¨åˆ—è¡¨â€é€šé“
        items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
        if items:
            for div in items:
                dts = (div.get("dwdata-time") or "").strip()
                if dts:
                    try:
                        pub_d = datetime.strptime(
                            dts.split()[0], "%Y-%m-%d"
                        ).date()
                        if pub_d != self.target_date:
                            continue
                    except Exception:
                        pass
                a = div.find("a", href=True)
                if not a:
                    continue
                title_text = norm(a.get_text())
                if not self.daily_title_pat.search(title_text):
                    continue
                t2 = date_from_bracket_title(title_text)
                if t2 and t2 != self.target_date:
                    continue

                abs_url = urljoin(base, a["href"])
                if self._try_detail(abs_url):
                    return True
            print("[HRLOO MISS] å®¹å™¨é€šé“æœªå‘½ä¸­ï¼š", base)

        # 2ï¼‰å…œåº•ï¼šéå†æ‰€æœ‰ /news/xxx.html
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href", "")
            if not re.search(r"/news/\d+\.html$", href):
                continue
            text = norm(a.get_text())
            if not self.daily_title_pat.search(text):
                continue
            t2 = date_from_bracket_title(text)
            if t2 and t2 != self.target_date:
                continue
            links.append(urljoin(base, href))

        seen = set()
        for url in links:
            if url in seen:
                continue
            seen.add(url)
            if self._try_detail(url):
                return True

        print("[HRLOO MISS] æœ¬æºæœªå‘½ä¸­ç›®æ ‡æ—¥æœŸï¼š", base)
        return False

    def _try_detail(self, abs_url):
        pub_dt, titles, page_title = self._fetch_detail_clean(abs_url)
        if not page_title or not self.daily_title_pat.search(page_title):
            return False
        t3 = date_from_bracket_title(page_title)
        if t3 and t3 != self.target_date:
            return False
        if pub_dt and pub_dt.date() != self.target_date and not t3:
            return False
        if not titles:
            return False

        self.results.append(
            {
                "title": page_title,
                "url": abs_url,
                "date": (
                    pub_dt.strftime("%Y-%m-%d %H:%M")
                    if pub_dt
                    else f"{self.target_date} 09:00"
                ),
                "titles": titles,
            }
        )
        print(f"[HRLOO HIT] {abs_url} -> {len(titles)} æ¡")
        return True

    # ---- ç»†èŠ‚é¡µè§£æ ----
    def _extract_pub_time(self, soup: BeautifulSoup):
        cand = []
        for t in soup.select("time[datetime]"):
            cand.append(t.get("datetime", ""))
        for m in soup.select(
            "meta[property='article:published_time'],"
            "meta[name='pubdate'],meta[name='publishdate']"
        ):
            cand.append(m.get("content", ""))
        for sel in [
            ".time",
            ".date",
            ".pubtime",
            ".publish-time",
            ".post-time",
            ".info",
            "meta[itemprop='datePublished']",
        ]:
            for x in soup.select(sel):
                if isinstance(x, Tag):
                    cand.append(x.get_text(" ", strip=True))

        pat = re.compile(
            r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?"
        )

        def parse_one(s):
            m = pat.search(s or "")
            if not m:
                return None
            try:
                y, mo, d = int(m[1]), int(m[2]), int(m[3])
                hh = int(m[4]) if m[4] else 9
                mm = int(m[5]) if m[5] else 0
                return datetime(y, mo, d, hh, mm, tzinfo=_tz())
            except Exception:
                return None

        dts = [dt for dt in map(parse_one, cand) if dt]
        if dts:
            now = now_tz()
            past = [dt for dt in dts if dt <= now]
            return min(past or dts, key=lambda dt: abs((now - dt).total_seconds()))
        return None

    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=(6, 20))
            if r.status_code != 200:
                print("[HRLOO DetailFail]", url, r.status_code)
                return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            title_tag = soup.find(["h1", "h2"])
            page_title = norm(title_tag.get_text()) if title_tag else ""
            pub_dt = self._extract_pub_time(soup)

            container = soup.select_one(
                ".content-con.hr-rich-text.fn-wenda-detail-infomation.fn-hr-rich-text.custom-style-w"
            ) or soup

            for sel in [
                ".other-wrap",
                ".txt",
                "a.prev.fn-dataStatistics-btn",
                "a.next.fn-dataStatistics-btn",
                ".footer",
                ".bottom",
            ]:
                for bad in container.select(sel):
                    bad.decompose()

            titles = self._extract_strong_titles(container)
            if not titles:
                titles = self._extract_numbered_titles(container)
            return pub_dt, titles, page_title
        except Exception as e:
            print("[HRLOO DetailError]", url, e)
            return None, [], ""

    def _extract_strong_titles(self, root: Tag):
        keep = []
        for st in root.select("strong"):
            text = norm(st.get_text())
            if not text:
                continue
            if len(text) < 4:
                continue
            text = re.split(
                r"[ï¼ˆ(]?(é˜…è¯»|é˜…è¯»é‡|æµè§ˆ|æ¥æº)[:ï¼š]\s*\d+.*$", text
            )[0].strip()
            if not text:
                continue
            text = strip_leading_num(text)
            if text:
                keep.append(text)
        seen, out = set(), []
        for t in keep:
            if t in seen:
                continue
            seen.add(t)
            out.append(t)
        return out

    def _extract_numbered_titles(self, root: Tag):
        out = []
        for p in root.find_all(["p", "h2", "h3", "div", "span", "li"]):
            text = norm(p.get_text())
            if looks_like_numbered(text):
                text = strip_leading_num(text)
                text = re.split(r"[ï¼ˆ(]", text)[0].strip()
                if text and len(text) >= 4:
                    out.append(text)
        seen, final = set(), []
        for t in out:
            if t in seen:
                continue
            seen.add(t)
            final.append(t)
        return final


# ========== äºŒã€è´¢å¯Œä¸­æ–‡ç½‘çˆ¬è™« + AI æ‘˜è¦ ==========

BASE = "https://www.fortunechina.com"
LIST_URL_BASE = "https://www.fortunechina.com/shangye/"
MAX_PAGES = 1
MAX_RETRY = 3

OUTPUT_CSV = "fortunechina_articles_with_ai_title.csv"
OUTPUT_MD = "fortunechina_articles_with_ai_title.md"


def get_target_date() -> str:
    env_date = os.getenv("TARGET_DATE", "").strip()
    if env_date:
        return env_date
    tz_cn = timezone(timedelta(hours=8))
    yesterday_cn = (datetime.now(tz_cn) - timedelta(days=1)).strftime("%Y-%m-%d")
    return yesterday_cn


TARGET_DATE = get_target_date()

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": "no-cache",
}

AI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
AI_API_BASE = os.getenv("AI_API_BASE", "https://api.siliconflow.cn/v1").rstrip("/")
AI_CHAT_URL = f"{AI_API_BASE}/chat/completions"
AI_MODEL = os.getenv("AI_MODEL", "Qwen/Qwen2.5-7B-Instruct")


def _title_keywords(title: str):
    parts = re.split(r"[ï¼š:ï¼Œ,ã€‚ï¼›;ã€ï¼Ÿ?ï¼!ï¼ˆï¼‰()ã€ã€‘\s]+", title or "")
    return [p for p in parts if len(p) >= 2]


def _summary_passes_check(summary: str, title: str, body: str) -> bool:
    """
    ç®€å•â€œä¸€è‡´æ€§æ£€æŸ¥â€ï¼š
    - æ ‡é¢˜æ‹†å‡ºå…³é”®è¯ï¼Œè‡³å°‘æœ‰ä¸€ä¸ªå‡ºç°åœ¨æ‘˜è¦ä¸­ï¼Œå¦åˆ™è®¤ä¸ºæ¨¡å‹åœ¨èƒ¡ç¼–ã€‚
    """
    if not summary:
        return False
    kws = _title_keywords(title)
    if not kws:
        return True  # æ²¡æ³•æ ¡éªŒå°±æ”¾è¡Œ
    for k in kws:
        if k in summary:
            return True
    # å†å®½æ¾ä¸€ç‚¹ï¼šæ­£æ–‡å‰ 200 å­—é‡Œï¼Œæ˜¯å¦å‡ºç°äº†æ‘˜è¦é‡Œçš„æ ¸å¿ƒè¯ï¼Ÿ
    body_short = (body or "")[:200]
    for w in _title_keywords(summary):
        if w in body_short:
            return True
    return False


def get_ai_summary(content: str, title: str = "") -> str:
    """
    ä½¿ç”¨ç¡…åŸºæµåŠ¨ç”Ÿæˆä¸€å¥è¯æ‘˜è¦ï¼Œå¹¶åšâ€œé˜²å¹»è§‰â€å…œåº•ï¼š
    - è¦æ±‚å›´ç»•ã€æ­£æ–‡ + æ ‡é¢˜ã€‘æ¦‚æ‹¬ï¼›
    - è‹¥æ‘˜è¦é‡Œå®Œå…¨ä¸å«æ ‡é¢˜å…³é”®è¯ï¼Œåˆ™å›é€€ä¸ºåŸæ ‡é¢˜ã€‚
    """
    fallback_title = title or "ï¼ˆæœªå‘½åï¼‰"

    if not content or len(content) < 30:
        return fallback_title

    if not AI_API_KEY:
        print("  âš ï¸ æœªé…ç½® OPENAI_API_KEYï¼Œè·³è¿‡ AI æ‘˜è¦ã€‚")
        return fallback_title

    headers = {
        "Accept": "application/json",
        "Authorization": f"Bearer {AI_API_KEY}",
        "Content-Type": "application/json",
    }

    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¸­æ–‡æ–°é—»ç¼–è¾‘ã€‚è¯·æ ¹æ®ä¸‹é¢ç»™å‡ºçš„ã€æ ‡é¢˜ã€‘å’Œã€æ­£æ–‡ã€‘"
        "å†™å‡ºä¸€æ¡ä¸è¶…è¿‡ 25 ä¸ªå­—çš„ä¸€å¥è¯æ‘˜è¦ï¼š\n"
        "1. åªåŸºäºæä¾›çš„å†…å®¹ï¼Œä¸å¾—æé€ æ–°çš„äº‹å®æˆ–äº‹ä»¶ï¼›\n"
        "2. æ‘˜è¦å¿…é¡»ä¸æ ‡é¢˜ä¸»é¢˜é«˜åº¦ä¸€è‡´ï¼Œä¸èƒ½æŠŠåˆ«çš„æ–°é—»å†™è¿›æ¥ï¼›\n"
        "3. ä¿æŒå®¢è§‚ã€ä¸­æ€§ï¼Œä¸æ ‡é¢˜å…šï¼›\n"
        "4. å°½é‡åŒ…å«æ ‡é¢˜ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆå¦‚äººåã€æœºæ„åã€å›½å®¶ç­‰ï¼‰ã€‚"
    )

    user_content = f"ã€æ ‡é¢˜ã€‘{fallback_title}\n\nã€æ­£æ–‡ã€‘\n{content[:2000]}"

    payload = {
        "model": AI_MODEL,
        "messages": [
            {"role": "system", "content": prompt},
            {"role": "user", "content": user_content},
        ],
        "max_tokens": 120,
        "temperature": 0.3,
    }

    print(f"  ğŸ¤– æ­£åœ¨è°ƒç”¨ AI ç”Ÿæˆæ‘˜è¦ï¼ˆæ¨¡å‹={AI_MODEL}ï¼‰...")

    try:
        resp = requests.post(AI_CHAT_URL, headers=headers, json=payload, timeout=40)
        if resp.status_code != 200:
            print(f"  âŒ AI çŠ¶æ€ç ï¼š{resp.status_code}")
            try:
                print("  âŒ AI è¿”å›å†…å®¹ï¼š", resp.text[:300])
            except Exception:
                pass
            return fallback_title

        data = resp.json()
        summary = data["choices"][0]["message"]["content"].strip()
        summary = summary.splitlines()[0].strip()

        # â€”â€” ä¸€è‡´æ€§æ£€æŸ¥ï¼šé˜²æ­¢â€œå°ç±³æ–°é—»è·‘åˆ°é©¬å…‹é¾™ä¸“æ ä¸Šâ€ â€”â€” 
        if not _summary_passes_check(summary, fallback_title, content):
            print("  âš ï¸ AI æ‘˜è¦ä¸æ ‡é¢˜ä¸ä¸€è‡´ï¼Œå·²å›é€€ä¸ºåŸæ ‡é¢˜ã€‚")
            return fallback_title

        print(f"  âœ¨ AI æ‘˜è¦ï¼š{summary}")
        return summary or fallback_title

    except Exception as e:
        print(f"  âš ï¸ AI è°ƒç”¨å¤±è´¥ï¼š{e}")
        return fallback_title


def fetch_list(page: int = 1):
    if page == 1:
        current_list_url = LIST_URL_BASE
    else:
        current_list_url = f"{LIST_URL_BASE}?page={page}"

    print(f"\n--- æ­£åœ¨è¯·æ±‚è´¢å¯Œåˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ({current_list_url}) ---")

    try:
        r = requests.get(current_list_url, headers=DEFAULT_HEADERS, timeout=15)
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []

    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")

        if not (h2 and a and date_div):
            continue

        href = a["href"].strip()
        pub_date = date_div.get_text(strip=True) if date_div else ""

        if pub_date != TARGET_DATE:
            continue

        if not re.search(r"content_\d+\.htm", href):
            continue

        url_full = urljoin(current_list_url, href)

        items.append(
            {
                "title": h2.get_text(strip=True),
                "url": url_full,
                "date": pub_date,
                "content": "",
                "ai_summary": "",
            }
        )

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°ç›®æ ‡æ—¥æœŸ({TARGET_DATE})æ–‡ç« æ•°ï¼š{len(items)}")
    return items


def fetch_article_content(item: dict):
    url = item["url"]
    headers = DEFAULT_HEADERS.copy()
    headers["Referer"] = LIST_URL_BASE

    for attempt in range(MAX_RETRY):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()

            soup = BeautifulSoup(r.text, "html.parser")
            container = soup.select_one("div.article-mod div.word-text-con")
            if not container:
                container = soup.select_one("div.article-content")
            if not container:
                # Plus ä¸“æ æœ‰æ—¶å€™ç»“æ„ä¸åŒï¼Œå†å…œåº•æ‰¾ä¸»å†…å®¹åŒº
                container = soup.find("article") or soup

            paras = [
                p.get_text(strip=True)
                for p in container.find_all("p")
                if p.get_text(strip=True)
            ]
            if not paras:
                # å†å…œåº•ï¼šæŠŠ container æ–‡æœ¬å…¨æŠ“äº†
                text_all = container.get_text("\n", strip=True)
                item["content"] = text_all
            else:
                item["content"] = "\n".join(paras)

            time.sleep(0.5)
            return

        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRY - 1:
                print(
                    f"  âŒ è¯·æ±‚å¤±è´¥ ({r.status_code if 'r' in locals() else 'Error'}), é‡è¯•ä¸­...: {url}"
                )
                time.sleep(1)
            else:
                print(f"  â›”ï¸ æœ€ç»ˆå¤±è´¥: {url} | é”™è¯¯: {e}")
                item["content"] = f"[è·å–å¤±è´¥: {e}]"


def save_to_csv(data: list, filename: str):
    if not data:
        print("ğŸ’¡ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ã€‚")
        return

    fieldnames = ["title", "ai_summary", "date", "url", "content"]
    try:
        with open(filename, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"\nğŸ‰ æˆåŠŸä¿å­˜åˆ° CSVï¼š{filename}ï¼Œå…± {len(data)} æ¡ã€‚")
    except Exception as e:
        print(f"\nâŒ CSV ä¿å­˜å¤±è´¥ï¼š{e}")


# ========== ä¸‰ã€åˆå¹¶ Markdown è¾“å‡º ==========

def _strip_trailing_punc(s: str) -> str:
    return re.sub(r"[ï¼›;ã€‚.\s]+$", "", s or "")


def build_clean_markdown(hr_items: list, fc_items: list) -> str:
    now_cn = now_tz()
    today_str = now_cn.strftime("%Y-%m-%d")
    weekday_str = zh_weekday(now_cn)

    merged_items = []

    # â€”â€” ä¸‰èŒ… titles â€”â€”
    if hr_items and hr_items[0].get("titles"):
        it = hr_items[0]
        detail_url = it.get("url", "")
        for t in it["titles"]:
            title = _strip_trailing_punc(t)
            if not title:
                continue
            merged_items.append(
                {
                    "title": title,
                    "url": detail_url or "#",
                }
            )

    # â€”â€” è´¢å¯Œ AI æ‘˜è¦ â€”â€”
    for art in fc_items or []:
        raw_title = art.get("ai_summary") or art.get("title") or ""
        title = _strip_trailing_punc(raw_title)
        if not title:
            continue
        merged_items.append(
            {
                "title": title,
                "url": art.get("url", "#"),
            }
        )

    if not merged_items:
        return f"**æ—¥æœŸï¼š{today_str}ï¼ˆ{weekday_str}ï¼‰**  \n**æ ‡é¢˜ï¼šäººèµ„æ—¥æŠ¥ | æ¯æ—¥è¦ç‚¹**  \n\n> ä»Šæ—¥æœªæŠ“å–åˆ°æœ‰æ•ˆèµ„è®¯ã€‚"

    lines = [
        f"**æ—¥æœŸï¼š{today_str}ï¼ˆ{weekday_str}ï¼‰**  ",
        f"**æ ‡é¢˜ï¼šäººèµ„æ—¥æŠ¥ | æ¯æ—¥è¦ç‚¹**  ",
        "",
    ]

    # â€”â€” ç¼–å· + æ ‡ç‚¹ï¼šæœ€åä¸€æ¡å¥å·ï¼Œå…¶ä½™åˆ†å· â€”â€”
    for idx, item in enumerate(merged_items, start=1):
        title = item["title"]
        url = item["url"]

        if idx == len(merged_items):
            # æœ€åä¸€æ¡ï¼šå¥å·
            lines.append(f"{idx}. [{title}]({url})ã€‚")
        else:
            lines.append(f"{idx}. [{title}]({url})ï¼›")

    return "\n".join(lines)


# ========== å››ã€ä¸»æµç¨‹ ==========

def main():
    print("=== ğŸš€ åˆå¹¶çˆ¬è™«å¯åŠ¨ï¼ˆHR ä¸‰èŒ… + è´¢å¯Œä¸­æ–‡ç½‘ï¼‰ ===")

    # 1. ä¸‰èŒ…æ—¥æŠ¥
    hr_crawler = HRLooCrawler()
    hr_crawler.crawl()
    hr_results = hr_crawler.results

    # 2. è´¢å¯Œä¸­æ–‡ç½‘åˆ—è¡¨
    all_articles = []
    print(
        f"\n=== ğŸ“… è´¢å¯Œä¸­æ–‡ç½‘ç›®æ ‡æ—¥æœŸ: {TARGET_DATE} "
        f"ï¼ˆåˆ—è¡¨å…¥å£: {LIST_URL_BASE}ï¼‰ ==="
    )

    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)
        if not list_items:
            if page == 1:
                print(f"âš ï¸ ç¬¬ 1 é¡µæœªæ‰¾åˆ° {TARGET_DATE} çš„æ–‡ç« ï¼Œè¯·ç¡®è®¤ç½‘ç«™ä¸Šç¡®å®æœ‰è¯¥æ—¥æœŸçš„å†…å®¹ã€‚")
            break
        all_articles.extend(list_items)
        time.sleep(1)

    print(
        f"\n=== ğŸ“¥ è´¢å¯Œé“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡ã€‚å¼€å§‹æŠ“å–æ­£æ–‡ + ç”Ÿæˆ AI æ‘˜è¦... ==="
    )

    for idx, item in enumerate(all_articles, start=1):
        print(f"\nğŸ”¥ è´¢å¯Œ ({idx}/{len(all_articles)}) å¤„ç†: {item['title']}")
        fetch_article_content(item)
        item["ai_summary"] = get_ai_summary(item["content"], item["title"])

    success_count = sum(
        1
        for item in all_articles
        if "è·å–å¤±è´¥" not in item["content"] and item["content"]
    )
    print(
        f"\n=== è´¢å¯Œç»Ÿè®¡: æˆåŠŸ {success_count} ç¯‡ï¼Œå¤±è´¥ {len(all_articles) - success_count} ç¯‡ ==="
    )
    save_to_csv(all_articles, OUTPUT_CSV)

    # 3. ç”Ÿæˆå•ç‹¬çš„è´¢å¯Œ Markdownï¼ˆå¯é€‰ï¼‰
    fc_md_lines = []
    if all_articles:
        fc_md_lines.append(f"### è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{TARGET_DATE}ï¼‰")
        fc_md_lines.append("")
        for i, art in enumerate(all_articles, start=1):
            t = art.get("ai_summary") or art.get("title") or "ï¼ˆæ— æ ‡é¢˜ï¼‰"
            u = art.get("url", "")
            fc_md_lines.append(f"{i}. [{t}]({u})")
    fc_md = "\n".join(fc_md_lines) if fc_md_lines else ""
    if fc_md:
        try:
            with open(OUTPUT_MD, "w", encoding="utf-8") as f:
                f.write(fc_md)
            print(f"\nğŸ“„ å·²ä¿å­˜è´¢å¯Œ Markdown æ–‡ä»¶ï¼š{OUTPUT_MD}")
        except Exception as e:
            print(f"\nâŒ è´¢å¯Œ Markdown ä¿å­˜å¤±è´¥ï¼š{e}")

    # 4. åˆå¹¶ä¸‰èŒ… + è´¢å¯Œï¼Œç”Ÿæˆæ€» Markdownï¼Œå¹¶æ¨é€é’‰é’‰
    md_merged = build_clean_markdown(hr_results, all_articles)
    print("\n===== åˆå¹¶ Markdown é¢„è§ˆ =====\n")
    print(md_merged)

    print("\n>>> [æ­¥éª¤4] æ¨é€åˆ°é’‰é’‰æœºå™¨äºº")
    send_dingtalk_markdown("äººèµ„æ—¥æŠ¥ | æ¯æ—¥è¦ç‚¹", md_merged)


if __name__ == "__main__":
    main()
