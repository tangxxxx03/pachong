# -*- coding: utf-8 -*-
"""
ä¸‰èŒ…ç½‘ + è´¢å¯Œä¸­æ–‡ç½‘ åˆå¹¶çˆ¬è™« V23 (æ¸…å•æ—©æŠ¥é£)
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
æ ¸å¿ƒæ›´æ–°ï¼š
1. å½»åº•å»æ ‡ç­¾åŒ–ï¼šä¸å†å¼ºè¡Œå®šä¹‰â€œHRâ€æˆ–â€œå•†ä¸šâ€ï¼Œå›å½’æ¥æºå±æ€§ã€‚
2. è§†è§‰ä¼˜åŒ–ï¼šEmoji å¼•å¯¼ï¼Œé“¾æ¥è¡Œå†…åŒ–ï¼ˆæ›´çœç©ºé—´ï¼‰ã€‚
"""

import os
import re
import ssl
import time
import hmac
import base64
import hashlib
import urllib.parse
from datetime import datetime, date
from urllib.parse import urljoin, quote

import requests
from bs4 import BeautifulSoup, Tag
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# --- AI ä¾èµ– ---
try:
    from openai import OpenAI
    HAS_OPENAI_LIB = True
except ImportError:
    HAS_OPENAI_LIB = False

try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo


# ================== åŸºç¡€å·¥å…· ==================

def _tz():
    return ZoneInfo("Asia/Shanghai")

def now_tz():
    return datetime.now(_tz())

def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def zh_weekday(dt: datetime) -> str:
    return ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][dt.weekday()]

def safe_url(url: str) -> str:
    if not url: return ""
    return quote(url.strip(), safe=":/?&amp;=#%")


# ================== AI æ€»ç»“æ¨¡å— ==================

AI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
AI_API_BASE = os.getenv("AI_API_BASE", "https://api.siliconflow.cn/v1").rstrip("/")
AI_MODEL = os.getenv("AI_MODEL", "Qwen/Qwen2.5-7B-Instruct")

AI_CLIENT = None
if HAS_OPENAI_LIB and AI_API_KEY:
    try:
        AI_CLIENT = OpenAI(api_key=AI_API_KEY, base_url=AI_API_BASE)
    except: pass

def get_ai_summary(content: str, title: str = "") -> str:
    """30å­—æé™æ€»ç»“"""
    if not AI_CLIENT: return title 
    if not content or len(content) < 50: return title

    print(f"  ğŸ¤– æ­£åœ¨ AI æ€»ç»“: {title[:10]}...")
    
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå­—å­—åƒé‡‘çš„å¿«è®¯ç¼–è¾‘ã€‚è¯·å°†æ–°é—»å‹ç¼©ä¸ºä¸€å¥**30å­—ä»¥å†…**çš„æç®€çŸ­è¯­ã€‚\n"
        "è§„åˆ™ï¼š1.å­—æ•°é”æ­»30å­—å†…ã€‚2.å»åºŸè¯ï¼Œç›´æ¥è¯´ç»“è®ºã€‚3.ç¦æ­¢ä»»ä½•æ ‡ç­¾å‰ç¼€ã€‚"
    )
    user_prompt = f"æ ‡é¢˜ï¼š{title}\næ­£æ–‡ï¼š{content[:2000]}"

    try:
        resp = AI_CLIENT.chat.completions.create(
            model=AI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=60, 
            temperature=0.3 
        )
        summary = resp.choices[0].message.content.strip()
        summary = summary.replace('"', '').replace("'", "").replace("\n", " ")
        summary = re.sub(r"^(æ‘˜è¦|ç»“è®º|æ ¸å¿ƒ|èƒŒæ™¯)[/:]\s*", "", summary)
        
        if "åŸæ ‡é¢˜" in summary and len(summary) < 10: return title
        print(f"  âœ¨ æ‘˜è¦æˆåŠŸ: {summary[:20]}...")
        return summary
    except:
        return title


# ================== HTTP Session ==================

class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9",
    })
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500, 502, 503, 504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s


# ================== ä¸‰èŒ…æ—¥æŠ¥çˆ¬è™« ==================

class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.target_date = now_tz().date()
        t = os.getenv("HR_TARGET_DATE", "")
        if t:
            try:
                y, m, d = map(int, t.split("-"))
                self.target_date = date(y, m, d)
            except: pass
        self.sources = ["https://www.hrloo.com/", "https://www.hrloo.com/news/hr"]
        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")

    def run(self):
        for base in self.sources:
            if self._crawl_source(base): break

    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=15)
            r.encoding = "utf-8"
            if r.status_code != 200: return False
            soup = BeautifulSoup(r.text, "html.parser")
            
            items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
            if items:
                for div in items:
                    dts = (div.get("dwdata-time") or "").strip()
                    if dts and str(self.target_date) not in dts: continue
                    a = div.find("a", href=True)
                    if not a: continue
                    if self._check_and_fetch(base, a): return True

            for a in soup.select("a[href*='/news/']"):
                if self._check_and_fetch(base, a): return True
        except: pass
        return False

    def _check_and_fetch(self, base, a):
        text = norm(a.get_text())
        href = a["href"]
        if not self.daily_title_pat.search(text): return False
        abs_url = urljoin(base, href)
        return self._fetch_detail(abs_url)

    def _fetch_detail(self, url):
        try:
            r = self.session.get(url, timeout=15)
            r.encoding = "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            container = soup.select_one(".content-con.hr-rich-text") or soup
            titles = []
            for st in container.select("strong"):
                t = norm(st.get_text())
                t = re.sub(r"^\d+[.ã€]\s*", "", t)
                if len(t) > 5 and "é˜…è¯»" not in t:
                    titles.append(t)
            if not titles:
                for p in container.select("p"):
                    t = norm(p.get_text())
                    if re.match(r"^\d+[.ã€]", t) and len(t) > 5:
                        titles.append(re.sub(r"^\d+[.ã€]\s*", "", t))
            titles = list(dict.fromkeys(titles))

            if titles:
                self.results.append({
                    "title": "ä¸‰èŒ…æ—¥æŠ¥",
                    "url": safe_url(url),
                    "titles": titles
                })
                return True
        except: pass
        return False

def build_hr_md(crawler):
    if not crawler.results: return "> ä»Šæ—¥ä¸‰èŒ…æš‚æ— æ›´æ–°ã€‚\n"
    it = crawler.results[0]
    # ä¸­æ€§æ ‡é¢˜ï¼Œä¸å¼ºè°ƒâ€œHRâ€
    md = [f"**ğŸ“° ä¸‰èŒ…èµ„è®¯**"]
    for i, t in enumerate(it['titles'], 1):
        # é“¾æ¥ç¼©å°æˆä¸€ä¸ªå°å›¾æ ‡æ”¾åœ¨å¥å°¾ï¼Œæ˜¾å¾—æ›´æ•´æ´
        md.append(f"{i}. {t} [ğŸ”—]({it['url']})")
    return "\n".join(md) + "\n"


# ================== è´¢å¯Œä¸­æ–‡ç½‘çˆ¬è™« ==================

BASE_FORTUNE = "https://www.fortunechina.com"
LIST_URL = "https://www.fortunechina.com/shangye/"

class FortuneCrawler:
    def __init__(self, max_items=5):
        self.session = make_session()
        self.max_items = max_items
        self.items = []

    def run(self):
        print(f"[Fortune] å¼€å§‹æŠ“å–...")
        try:
            r = self.session.get(LIST_URL, timeout=15)
            r.encoding = "utf-8" 
            soup = BeautifulSoup(r.text, "html.parser")
            
            cnt = 0
            for li in soup.select("ul.news-list li.news-item"):
                if cnt >= self.max_items: break
                
                h2 = li.find("h2")
                a = li.find("a", href=True)
                
                if not (h2 and a): continue
                
                href = a["href"].strip()
                if "content_" not in href: continue 
                
                title = norm(h2.get_text())
                full_url = urljoin(LIST_URL, href)
                
                content = self._fetch_content(full_url)
                ai_summary = get_ai_summary(content, title)
                
                self.items.append({
                    "title": title,
                    "summary": ai_summary, 
                    "url": safe_url(full_url)
                })
                cnt += 1
                
        except Exception as e:
            print(f"[Fortune Error] {e}")

    def _fetch_content(self, url):
        try:
            r = self.session.get(url, timeout=10)
            r.encoding = "utf-8" 
            soup = BeautifulSoup(r.text, "html.parser")
            container = soup.select_one("div.article-mod div.word-text-con") or \
                        soup.select_one("div.article-content")
            
            if container: return norm(container.get_text())
        except: pass
        return ""

def build_fortune_md(crawler):
    if not crawler.items: return "> ä»Šæ—¥è´¢å¯Œæš‚æ— æ›´æ–°ã€‚\n"
    # ä¸­æ€§æ ‡é¢˜ï¼Œä¸å¼ºè°ƒâ€œå•†ä¸šçƒ­ç‚¹â€
    md = ["**ğŸš€ è´¢å¯Œå•†ä¸š**"]
    for i, it in enumerate(crawler.items, 1):
        display_text = it['summary']
        # é“¾æ¥æ”¾åœ¨å¥å°¾
        md.append(f"{i}. {display_text} [ğŸ”—]({it['url']})")
    return "\n".join(md) + "\n"


# ================== æ¨é€ä¸å…¥å£ ==================

def send_dingtalk(title, text):
    bases = (os.getenv("DINGTALK_BASES") or "").split(",")
    secrets = (os.getenv("DINGTALK_SECRETS") or "").split(",")
    
    if not bases or not bases[0]:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASES")
        return

    for i, base in enumerate(bases):
        base = base.strip()
        if not base: continue
        secret = secrets[i].strip() if i < len(secrets) else ""
        
        if secret:
            ts = str(round(time.time() * 1000))
            s = f"{ts}\n{secret}".encode("utf-8")
            sign = urllib.parse.quote_plus(base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()))
            url = f"{base}&timestamp={ts}&sign={sign}"
        else:
            url = base
            
        try:
            requests.post(url, json={
                "msgtype": "markdown",
                "markdown": {"title": title, "text": text}
            }, timeout=10)
            print(f"âœ… æ¨é€æˆåŠŸ: æœºå™¨äºº {i+1}")
        except Exception as e:
            print(f"âŒ æ¨é€å¤±è´¥: {e}")

def main():
    print("=== å¯åŠ¨åˆå¹¶çˆ¬è™« V23 (æ¸…å•æ—©æŠ¥ç‰ˆ) ===")
    
    # 1. ä¸‰èŒ…
    hr = HRLooCrawler()
    hr.run()
    hr_md = build_hr_md(hr)
    
    # 2. è´¢å¯Œ
    fc = FortuneCrawler(max_items=int(os.getenv("FORTUNE_MAX_ITEMS") or 5))
    fc.run()
    fc_md = build_fortune_md(fc)
    
    # 3. åˆå¹¶ - æç®€æ ‡é¢˜
    final_md = (
        f"**ğŸ“… {now_tz().strftime('%m-%d')} æ¯æ—¥æ—©æŠ¥** \n\n"
        f"{hr_md}\n"
        f"{fc_md}"
    )
    
    print("\n--- Markdown é¢„è§ˆ ---\n")
    print(final_md)
    
    send_dingtalk("æ¯æ—¥æ—©æŠ¥", final_md)

if __name__ == "__main__":
    main()
