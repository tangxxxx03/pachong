# -*- coding: utf-8 -*-
"""
ä¸‰èŒ…ç½‘ + è´¢å¯Œä¸­æ–‡ç½‘ åˆå¹¶çˆ¬è™« V20 (æé€Ÿç²¾ç®€ç‰ˆ)
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
æ ¸å¿ƒæ›´æ–°ï¼š
1. æè‡´å‹ç¼©ï¼šPrompt å¼ºåˆ¶è¦æ±‚â€œå»æ ‡ç­¾â€ã€â€œè®²äººè¯â€ã€â€œä¸€å¥è¯è¯´å®Œâ€ã€‚
2. èšç„¦ç»“æœï¼šçœç•¥é“ºå«ï¼Œç›´æ¥æŠŠæ–°é—»æœ€æ ¸å¿ƒçš„å†²çªæˆ–ç»“è®ºæŠ›å‡ºæ¥ã€‚
3. è§‚æ„Ÿä¼˜åŒ–ï¼šé€‚åˆæ‰‹æœºå¿«é€Ÿæ‰«è¯»ï¼Œæ¯æ¡æ–°é—»ä¸è¶…è¿‡ 2 è¡Œã€‚
"""

import os
import re
import ssl
import time
import hmac
import base64
import hashlib
import urllib.parse
from datetime import datetime, date
from urllib.parse import urljoin, quote

import requests
from bs4 import BeautifulSoup, Tag
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# --- AI ä¾èµ– ---
AI_DEBUG_MSG = "" 
try:
    from openai import OpenAI
    HAS_OPENAI_LIB = True
except ImportError:
    print("âš ï¸ è­¦å‘Šï¼šç¼ºå°‘ openai åº“ï¼Œè¯·åœ¨ yml æ–‡ä»¶ä¸­è¿è¡Œ pip install openai")
    HAS_OPENAI_LIB = False
    AI_DEBUG_MSG = "(AIåº“ç¼ºå¤±)"

try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo


# ================== åŸºç¡€å·¥å…· ==================

def _tz():
    return ZoneInfo("Asia/Shanghai")

def now_tz():
    return datetime.now(_tz())

def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def zh_weekday(dt: datetime) -> str:
    return ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][dt.weekday()]

def safe_url(url: str) -> str:
    if not url: return ""
    return quote(url.strip(), safe=":/?&amp;=#%")


# ================== AI æ€»ç»“æ¨¡å— (ç²¾ç®€ Prompt) ==================

AI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
AI_API_BASE = os.getenv("AI_API_BASE", "https://api.siliconflow.cn/v1").rstrip("/")
AI_MODEL = os.getenv("AI_MODEL", "Qwen/Qwen2.5-7B-Instruct")

AI_CLIENT = None
if HAS_OPENAI_LIB:
    if AI_API_KEY:
        try:
            AI_CLIENT = OpenAI(api_key=AI_API_KEY, base_url=AI_API_BASE)
        except Exception as e:
            print(f"[AI Init Error] {e}")
            AI_DEBUG_MSG = f"(AIé…ç½®é”™è¯¯)"
    else:
        AI_DEBUG_MSG = "(AI Keyç¼ºå¤±)"
elif not AI_DEBUG_MSG:
    AI_DEBUG_MSG = "(AIåº“ç¼ºå¤±)"

def get_ai_summary(content: str, title: str = "") -> str:
    """è°ƒç”¨ AI ç”Ÿæˆæç®€çŸ­è¯„"""
    if not AI_CLIENT:
        return f"{title} {AI_DEBUG_MSG}"

    if not content or len(content) < 50:
        return title

    print(f"  ğŸ¤– æ­£åœ¨ AI æ€»ç»“: {title[:10]}...")
    
    # --- âš¡ï¸ æé€Ÿç²¾ç®€ Prompt ---
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ª**å­—å­—ç ç‘**çš„å¿«è®¯ç¼–è¾‘ã€‚è¯·æ ¹æ®æ–°é—»æ­£æ–‡ï¼Œæç‚¼å‡ºä¸€å¥**æç®€çŸ­è¯„**ã€‚\n\n"
        "**ç»å¯¹è§„åˆ™ï¼š**\n"
        "1. **ç¦æ­¢æ ‡ç­¾**ï¼šä¸¥ç¦å‡ºç°â€œèƒŒæ™¯ï¼šâ€ã€â€œè§‚ç‚¹ï¼šâ€ã€â€œç»“å±€ï¼šâ€ç­‰ä»»ä½•å‰ç¼€è¯ï¼ç›´æ¥è¯´å†…å®¹ã€‚\n"
        "2. **ç»“æœå‰ç½®**ï¼šç›´æ¥æŠŠæœ€é‡è¦çš„ç»“è®ºæˆ–å†²çªæ”¾åœ¨æœ€å‰é¢ï¼Œä¸è¦åšé“ºå«ã€‚\n"
        "3. **ä¸€è¯­é“ç ´**ï¼šå°†å¤æ‚çš„å› æœå…³ç³»å‹ç¼©æˆä¸€å¥è¯ï¼Œä½¿ç”¨â€œå¯¼è‡´â€ã€â€œæ„å‘³ç€â€ã€â€œè­¦ç¤ºâ€ç­‰è¿æ¥è¯ã€‚\n"
        "4. **æ‹’ç»å¤è¯»**ï¼šä¸è¦æ€»æ˜¯ä»¥â€œæŸæŸè¡¨ç¤ºâ€å¼€å¤´ï¼Œå°è¯•ç”¨â€œéšç€...â€ã€â€œ...å¼•å‘å…³æ³¨â€ç­‰å¥å¼ã€‚\n"
        "5. **å­—æ•°é™åˆ¶**ï¼šä¸¥æ ¼æ§åˆ¶åœ¨ **40-60ä¸ªä¸­æ–‡å­—** ä»¥å†…ï¼Œè¶ŠçŸ­è¶Šå¥½ã€‚"
    )
    
    user_prompt = f"ã€æ–°é—»æ ‡é¢˜ã€‘ï¼š{title}\n\nã€æ–°é—»æ­£æ–‡ã€‘ï¼š\n{content[:2000]}"
    # ---------------------------

    try:
        resp = AI_CLIENT.chat.completions.create(
            model=AI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=80, # ç¼©å‡ tokenï¼Œé€¼è¿« AI å°‘è¯´è¯
            temperature=0.3 
        )
        summary = resp.choices[0].message.content.strip()
        summary = summary.replace('"', '').replace("'", "").replace("\n", " ")
        
        # å†æ¬¡æ¸…æ´—ï¼šå¦‚æœ AI ä¸å¬è¯åŠ äº†æ ‡ç­¾ï¼Œæ‰‹åŠ¨åˆ æ‰
        summary = re.sub(r"^(èƒŒæ™¯|è§‚ç‚¹|ç»“è®º|ç»“å±€|æ ¸å¿ƒäº‹ä»¶)[/:]\s*", "", summary)
        
        if "åŸæ ‡é¢˜" in summary and len(summary) < 10:
            return title
            
        print(f"  âœ¨ æ‘˜è¦æˆåŠŸ: {summary[:20]}...")
        return summary
    except Exception as e:
        print(f"  âš ï¸ AI è°ƒç”¨å¤±è´¥: {e}")
        return f"{title} (AIå¤±è´¥)"


# ================== HTTP Session ==================

class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9",
    })
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500, 502, 503, 504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s


# ================== ä¸‰èŒ…æ—¥æŠ¥çˆ¬è™« ==================

class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.target_date = now_tz().date()
        t = os.getenv("HR_TARGET_DATE", "")
        if t:
            try:
                y, m, d = map(int, t.split("-"))
                self.target_date = date(y, m, d)
            except: pass
        self.sources = ["https://www.hrloo.com/", "https://www.hrloo.com/news/hr"]
        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")

    def run(self):
        for base in self.sources:
            if self._crawl_source(base): break

    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=15)
            r.encoding = "utf-8"
            if r.status_code != 200: return False
            soup = BeautifulSoup(r.text, "html.parser")
            
            items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
            if items:
                for div in items:
                    dts = (div.get("dwdata-time") or "").strip()
                    if dts and str(self.target_date) not in dts: continue
                    a = div.find("a", href=True)
                    if not a: continue
                    if self._check_and_fetch(base, a): return True

            for a in soup.select("a[href*='/news/']"):
                if self._check_and_fetch(base, a): return True
        except Exception as e:
            print(f"[HR Error] {base}: {e}")
        return False

    def _check_and_fetch(self, base, a):
        text = norm(a.get_text())
        href = a["href"]
        if not self.daily_title_pat.search(text): return False
        abs_url = urljoin(base, href)
        return self._fetch_detail(abs_url)

    def _fetch_detail(self, url):
        try:
            r = self.session.get(url, timeout=15)
            r.encoding = "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            
            container = soup.select_one(".content-con.hr-rich-text") or soup
            titles = []
            for st in container.select("strong"):
                t = norm(st.get_text())
                t = re.sub(r"^\d+[.ã€]\s*", "", t)
                if len(t) > 5 and "é˜…è¯»" not in t:
                    titles.append(t)
            if not titles:
                for p in container.select("p"):
                    t = norm(p.get_text())
                    if re.match(r"^\d+[.ã€]", t) and len(t) > 5:
                        titles.append(re.sub(r"^\d+[.ã€]\s*", "", t))
            titles = list(dict.fromkeys(titles))

            if titles:
                self.results.append({
                    "title": "ä¸‰èŒ…æ—¥æŠ¥",
                    "url": safe_url(url),
                    "date": str(self.target_date),
                    "titles": titles
                })
                return True
        except: pass
        return False

def build_hr_md(crawler):
    if not crawler.results: return "> ä»Šæ—¥æœªæŠ“å–åˆ°ä¸‰èŒ…æ—¥æŠ¥ã€‚\n"
    it = crawler.results[0]
    md = [f"**ä¸‰èŒ…æ—¥æŠ¥ Â· {it['date']}** \n"]
    for i, t in enumerate(it['titles'], 1):
        md.append(f"{i}. {t}")
    md.append(f"\n[ğŸ‘‰ æŸ¥çœ‹åŸæ–‡]({it['url']})\n")
    return "\n".join(md)


# ================== è´¢å¯Œä¸­æ–‡ç½‘çˆ¬è™« ==================

BASE_FORTUNE = "https://www.fortunechina.com"
LIST_URL = "https://www.fortunechina.com/shangye/"

class FortuneCrawler:
    def __init__(self, max_items=5):
        self.session = make_session()
        self.max_items = max_items
        self.items = []

    def run(self):
        print(f"[Fortune] å¼€å§‹æŠ“å–åˆ—è¡¨ (Max: {self.max_items})...")
        try:
            r = self.session.get(LIST_URL, timeout=15)
            r.encoding = "utf-8" 
            soup = BeautifulSoup(r.text, "html.parser")
            
            cnt = 0
            for li in soup.select("ul.news-list li.news-item"):
                if cnt >= self.max_items: break
                
                h2 = li.find("h2")
                a = li.find("a", href=True)
                date_div = li.find("div", class_="date")
                
                if not (h2 and a): continue
                
                href = a["href"].strip()
                if "content_" not in href: continue 
                
                title = norm(h2.get_text())
                pub_date = norm(date_div.get_text()) if date_div else ""
                full_url = urljoin(LIST_URL, href)
                
                content = self._fetch_content(full_url)
                ai_summary = get_ai_summary(content, title)
                
                self.items.append({
                    "title": title,
                    "summary": ai_summary, 
                    "url": safe_url(full_url),
                    "date": pub_date
                })
                cnt += 1
                
        except Exception as e:
            print(f"[Fortune Error] {e}")

    def _fetch_content(self, url):
        """æŠ“å–æ­£æ–‡ç”¨äº AI æ€»ç»“"""
        try:
            r = self.session.get(url, timeout=10)
            r.encoding = "utf-8" 
            soup = BeautifulSoup(r.text, "html.parser")
            container = soup.select_one("div.article-mod div.word-text-con") or \
                        soup.select_one("div.article-content")
            
            if container:
                return norm(container.get_text())
        except:
            pass
        return ""

def build_fortune_md(crawler):
    if not crawler.items: return "> è´¢å¯Œä¸­æ–‡ç½‘æš‚æ— å†…å®¹ã€‚\n"
    md = ["**è´¢å¯Œä¸­æ–‡ç½‘ Â· å•†ä¸šç²¾é€‰ (AI æ‘˜è¦)** \n"]
    for i, it in enumerate(crawler.items, 1):
        display_text = it['summary']
        md.append(f"{i}. [{display_text}]({it['url']})")
    return "\n".join(md) + "\n"


# ================== æ¨é€ä¸å…¥å£ ==================

def send_dingtalk(title, text):
    bases = (os.getenv("DINGTALK_BASES") or "").split(",")
    secrets = (os.getenv("DINGTALK_SECRETS") or "").split(",")
    
    if not bases or not bases[0]:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASES")
        return

    for i, base in enumerate(bases):
        base = base.strip()
        if not base: continue
        secret = secrets[i].strip() if i < len(secrets) else ""
        
        if secret:
            ts = str(round(time.time() * 1000))
            s = f"{ts}\n{secret}".encode("utf-8")
            sign = urllib.parse.quote_plus(base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest()))
            url = f"{base}&timestamp={ts}&sign={sign}"
        else:
            url = base
            
        try:
            requests.post(url, json={
                "msgtype": "markdown",
                "markdown": {"title": title, "text": text}
            }, timeout=10)
            print(f"âœ… æ¨é€æˆåŠŸ: æœºå™¨äºº {i+1}")
        except Exception as e:
            print(f"âŒ æ¨é€å¤±è´¥: {e}")

def main():
    print("=== å¯åŠ¨åˆå¹¶çˆ¬è™« V20 (ç²¾ç®€ç‰ˆ) ===")
    
    # 1. ä¸‰èŒ…
    hr = HRLooCrawler()
    hr.run()
    hr_md = build_hr_md(hr)
    
    # 2. è´¢å¯Œ
    fc = FortuneCrawler(max_items=int(os.getenv("FORTUNE_MAX_ITEMS") or 5))
    fc.run()
    fc_md = build_fortune_md(fc)
    
    # 3. åˆå¹¶
    final_md = (
        f"**äººèµ„ & å•†ä¸šæ—©æŠ¥ ({now_tz().strftime('%Y-%m-%d')})** \n\n"
        "### ä¸€ã€HR çƒ­ç‚¹ (ä¸‰èŒ…ç½‘)\n"
        f"{hr_md}\n"
        "### äºŒã€å•†ä¸šçƒ­ç‚¹ (è´¢å¯Œä¸­æ–‡ç½‘)\n"
        f"{fc_md}"
    )
    
    print("\n--- Markdown é¢„è§ˆ ---\n")
    print(final_md)
    
    send_dingtalk("äººèµ„&å•†ä¸šæ—©æŠ¥", final_md)

if __name__ == "__main__":
    main()
