# -*- coding: utf-8 -*-
"""
ä¸‰èŒ…ç½‘ã€Œä¸‰èŒ…æ—¥æŠ¥ã€ + è´¢å¯Œä¸­æ–‡ç½‘ã€Œå•†ä¸šé¢‘é“ã€åˆå¹¶çˆ¬è™« V12 (ä¿®å¤é“¾æ¥ä¸ä¹±ç )
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
åŠŸèƒ½ï¼š
1ï¼‰ä»ä¸‰èŒ…ç½‘æŠ“å–å½“å¤©çš„ã€Œä¸‰èŒ…æ—¥æŠ¥ã€å¹¶æŠ½å–è¦ç‚¹æ ‡é¢˜
2ï¼‰ä»è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“ï¼ˆPC ç‰ˆ /shangye/ï¼‰æŠ“å–æœ€æ–°è‹¥å¹²æ¡æ–‡ç« 
3ï¼‰æŠŠä¸¤ä¸ªæ¥æºåˆæˆä¸€æ¡ Markdown æ¶ˆæ¯æ¨é€åˆ°é’‰é’‰

ç¯å¢ƒå˜é‡ï¼ˆæ¨èåœ¨ GitHub Actions Secrets é‡Œé…ç½®ï¼‰ï¼š
  # é’‰é’‰ç¾¤æœºå™¨äººï¼ˆå¯å¤šä¸ªï¼Œç”¨é€—å·åˆ†éš”ï¼‰
  DINGTALK_BASES   = https://oapi.dingtalk.com/robot/send?access_token=xxx,https://...
  DINGTALK_SECRETS = xxx,yyy

  # ä¸‰èŒ…ç›®æ ‡æ—¥æœŸï¼ˆå¯é€‰ï¼Œä¸è®¾åˆ™é»˜è®¤â€œä»Šå¤©â€ï¼‰
  HR_TARGET_DATE   = 2025-12-04

  # ä¸‰èŒ…æŠ“å–å…¥å£ï¼ˆå¯é€‰ï¼Œä¸€èˆ¬ä¸ç”¨æ”¹ï¼‰
  SRC_HRLOO_URLS   = https://www.hrloo.com/,https://www.hrloo.com/news/hr

  # è´¢å¯ŒæŠ“å–ç¯‡æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 5ï¼‰
  FORTUNE_MAX_ITEMS = 5
"""

import os
import re
import ssl
import time
import hmac
import base64
import hashlib
import urllib.parse
from datetime import datetime, date
from urllib.parse import urljoin, quote

import requests
from bs4 import BeautifulSoup, Tag
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ================== é€šç”¨å°å·¥å…· ==================

try:
    from zoneinfo import ZoneInfo
except:  # Python<3.9
    from backports.zoneinfo import ZoneInfo


def _tz():
    return ZoneInfo("Asia/Shanghai")


def now_tz():
    return datetime.now(_tz())


def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def zh_weekday(dt: datetime) -> str:
    return ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][dt.weekday()]

# --- ğŸ¯ æ ¸å¿ƒä¿®å¤ï¼šURL ç¼–ç ä¸æ¸…æ´— ---
def safe_url(url: str) -> str:
    """
    å¯¹ URL è¿›è¡Œæ¸…æ´—å’Œç¼–ç ï¼Œé˜²æ­¢ Markdown è§£æé”™è¯¯æˆ– 404ã€‚
    åªå¯¹è·¯å¾„éƒ¨åˆ†ç¼–ç ï¼Œä¿ç•™ :// ç­‰ç¬¦å·ã€‚
    """
    if not url: return ""
    # å…ˆå»é™¤é¦–å°¾ç©ºç™½
    url = url.strip()
    # ç®€å•ç¼–ç ï¼Œsafe å­—ç¬¦ä¸ç¼–ç 
    return quote(url, safe=":/?&amp;=#%")
# -----------------------------------

# ================== DingTalk æ¨é€ ==================

def _sign_webhook(base: str, secret: str) -> str:
    """é’‰é’‰è‡ªå¸¦åŠ ç­¾é€»è¾‘"""
    if not base:
        return ""
    if not secret:
        return base

    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(
        base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest())
    )
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"


def send_dingtalk_markdown_all(title: str, text: str) -> None:
    """
    åŒæ—¶å¾€å¤šä¸ªç¾¤æ¨é€
    """
    bases = (os.getenv("DINGTALK_BASES") or "").split(",")
    secrets = (os.getenv("DINGTALK_SECRETS") or "").split(",")

    bases = [b.strip() for b in bases if b.strip()]
    secrets = [s.strip() for s in secrets if s.strip()]

    if not bases:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASESï¼Œè·³è¿‡æ¨é€ã€‚")
        return

    for i, base in enumerate(bases):
        secret = secrets[i] if i < len(secrets) else ""
        try:
            url = _sign_webhook(base, secret)
            resp = requests.post(
                url,
                json={"msgtype": "markdown", "markdown": {"title": title, "text": text}},
                timeout=20,
            )
            ok = resp.status_code == 200 and resp.json().get("errcode") == 0
            print(f"[DingTalk #{i}] push={ok} code={resp.status_code}")
            if not ok:
                print("  resp:", resp.text[:300])
        except Exception as e:
            print(f"[DingTalk #{i}] error:", e)


# ================== HTTP Sessionï¼ˆæ”¯æŒæ—§ TLSï¼‰ ==================

class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)


def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120 Safari/537.36"
        ),
        "Accept-Language": "zh-CN,zh;q=0.9",
    })
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500, 502, 503, 504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s


# ================== ä¸‰èŒ…ç½‘ï¼šäººèµ„æ—¥æŠ¥ ==================
# (ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜ï¼Œåªåœ¨ç”Ÿæˆ URL æ—¶è°ƒç”¨ safe_url)

CN_TITLE_DATE = re.compile(r"[ï¼ˆ(]\s*(20\d{2})\s*[å¹´\-/.]\s*(\d{1,2})\s*[æœˆ\-/.]\s*(\d{1,2})\s*[)ï¼‰]")

def date_from_bracket_title(text: str):
    m = CN_TITLE_DATE.search(text or "")
    if not m: return None
    try:
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        return date(y, mo, d)
    except Exception: return None

def looks_like_numbered(text: str) -> bool:
    return bool(re.match(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*\S+", text or ""))

CIRCLED = "â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©"
def strip_leading_num(t: str) -> str:
    t = re.sub(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*", "", t)
    t = re.sub(r"^\s*[" + CIRCLED + r"]\s*", "", t)
    t = re.sub(r"^\s*[ï¼-ï¼™]+\s*[ã€.ï¼]\s*", "", t)
    return t.strip()

class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        t = (os.getenv("HR_TARGET_DATE") or "").strip()
        if t:
            try:
                y, m, d = map(int, re.split(r"[-/\.]", t))
                self.target_date = date(y, m, d)
            except Exception:
                self.target_date = now_tz().date()
        else:
            self.target_date = now_tz().date()
        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        self.sources = [u.strip() for u in os.getenv("SRC_HRLOO_URLS", "https://www.hrloo.com/,https://www.hrloo.com/news/hr").split(",") if u.strip()]

    def run(self):
        for base in self.sources:
            if self._crawl_source(base): break

    def _crawl_source(self, base: str) -> bool:
        try: r = self.session.get(base, timeout=20)
        except Exception: return False
        if r.status_code != 200: return False
        soup = BeautifulSoup(r.text, "html.parser")
        
        # 1) dwxfd-list
        items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
        if items:
            for div in items:
                dts = (div.get("dwdata-time") or "").strip()
                if dts:
                    try:
                        pub_d = datetime.strptime(dts.split()[0], "%Y-%m-%d").date()
                        if pub_d != self.target_date: continue
                    except Exception: pass
                a = div.find("a", href=True)
                if not a: continue
                title_text = norm(a.get_text())
                if not self.daily_title_pat.search(title_text): continue
                t2 = date_from_bracket_title(title_text)
                if t2 and t2 != self.target_date: continue
                abs_url = urljoin(base, a["href"])
                if self._try_detail(abs_url): return True

        # 2) /news/
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href", "")
            if not re.search(r"/news/\d+\.html$", href): continue
            text = norm(a.get_text())
            if not self.daily_title_pat.search(text): continue
            t2 = date_from_bracket_title(text)
            if t2 and t2 != self.target_date: continue
            links.append(urljoin(base, href))
        seen = set()
        for url in links:
            if url in seen: continue
            seen.add(url)
            if self._try_detail(url): return True
        return False

    def _try_detail(self, abs_url: str) -> bool:
        pub_dt, titles, page_title = self._fetch_detail_clean(abs_url)
        if not page_title or not self.daily_title_pat.search(page_title): return False
        t3 = date_from_bracket_title(page_title)
        if t3 and t3 != self.target_date: return False
        if pub_dt and pub_dt.date() != self.target_date and not t3: return False
        if not titles: return False
        self.results.append({
            "title": page_title,
            "url": safe_url(abs_url), # ä½¿ç”¨ safe_url æ¸…æ´—é“¾æ¥
            "date": pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else f"{self.target_date} 09:00",
            "titles": titles,
        })
        return True

    def _extract_pub_time(self, soup: BeautifulSoup):
        cand = []
        for t in soup.select("time[datetime]"): cand.append(t.get("datetime", ""))
        for m in soup.select("meta[property='article:published_time'],meta[name='pubdate'],meta[name='publishdate']"): cand.append(m.get("content", ""))
        for sel in [".time", ".date", ".pubtime", ".publish-time", ".post-time", ".info", "meta[itemprop='datePublished']"]:
            for x in soup.select(sel):
                if isinstance(x, Tag): cand.append(x.get_text(" ", strip=True))
        pat = re.compile(r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?")
        dts = []
        for s in cand:
            m = pat.search(s or "")
            if m:
                try: dts.append(datetime(int(m[1]), int(m[2]), int(m[3]), int(m[4]) if m[4] else 9, int(m[5]) if m[5] else 0, tzinfo=_tz()))
                except Exception: pass
        if dts:
            now = now_tz()
            past = [dt for dt in dts if dt <= now]
            return min(past or dts, key=lambda dt: abs((now - dt).total_seconds()))
        return None

    def _fetch_detail_clean(self, url: str):
        try:
            r = self.session.get(url, timeout=(6, 20))
            if r.status_code != 200: return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            title_tag = soup.find(["h1", "h2"])
            page_title = norm(title_tag.get_text()) if title_tag else ""
            pub_dt = self._extract_pub_time(soup)
            container = soup.select_one(".content-con.hr-rich-text.fn-wenda-detail-infomation.fn-hr-rich-text.custom-style-w") or soup
            for sel in [".other-wrap", ".txt", "a.prev.fn-dataStatistics-btn", "a.next.fn-dataStatistics-btn", ".footer", ".bottom"]:
                for bad in container.select(sel): bad.decompose()
            titles = self._extract_strong_titles(container)
            if not titles: titles = self._extract_numbered_titles(container)
            return pub_dt, titles, page_title
        except Exception: return None, [], ""

    def _extract_strong_titles(self, root: Tag):
        keep = []
        for st in root.select("strong"):
            text = norm(st.get_text())
            if not text or len(text) < 4: continue
            text = re.split(r"[ï¼ˆ(]?(é˜…è¯»|é˜…è¯»é‡|æµè§ˆ|æ¥æº)[:ï¼š]\s*\d+.*$", text)[0].strip()
            if not text: continue
            text = strip_leading_num(text)
            if text: keep.append(text)
        seen, out = set(), []
        for t in keep:
            if t in seen: continue
            seen.add(t)
            out.append(t)
        return out

    def _extract_numbered_titles(self, root: Tag):
        out = []
        for p in root.find_all(["p", "h2", "h3", "div", "span", "li"]):
            text = norm(p.get_text())
            if looks_like_numbered(text):
                text = strip_leading_num(text)
                text = re.split(r"[ï¼ˆ(]", text)[0].strip()
                if text and len(text) >= 4: out.append(text)
        seen, final = set(), []
        for t in out:
            if t in seen: continue
            seen.add(t)
            final.append(t)
        return final

def build_hrloo_md_block(crawler: HRLooCrawler) -> str:
    if not crawler.results: return "> ä»Šå¤©æœªå‘ç°ä¸‰èŒ…ç½‘å‘å¸ƒâ€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚\n"
    it = crawler.results[0]
    out = [f"**ä¸‰èŒ…æ—¥æŠ¥ Â· {it['date']}** \n"]
    for idx, t in enumerate(it["titles"], 1): out.append(f"{idx}. {t}  ")
    out.append(f"[ğŸ‘‰ æŸ¥çœ‹åŸæ–‡]({it['url']})  ")
    return "\n".join(out) + "\n"


# ================== è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“ ==================

BASE_FORTUNE = "https://www.fortunechina.com"
# åˆ—è¡¨é¡µ URLï¼Œç”¨äºæ­£ç¡®æ‹¼æ¥ç›¸å¯¹è·¯å¾„
LIST_URL_FORTUNE = "https://www.fortunechina.com/shangye/"

class FortuneChinaCrawler:
    def __init__(self, max_items: int = 5):
        self.session = make_session()
        self.max_items = max_items

    def fetch_list_page(self, page: int = 1):
        url = f"{BASE_FORTUNE}/shangye/" if page == 1 else f"{BASE_FORTUNE}/shangye/?page={page}"
        print(f"[Fortune] è¯·æ±‚åˆ—è¡¨é¡µï¼š{url}")
        try:
            r = self.session.get(url, timeout=15)
            r.raise_for_status()
        except Exception as e:
            print("[Fortune] åˆ—è¡¨é¡µå¼‚å¸¸ï¼š", e)
            return []

        soup = BeautifulSoup(r.text, "html.parser")
        items = []

        for li in soup.select("ul.news-list li.news-item"):
            h2 = li.find("h2")
            a = li.find("a", href=True)
            date_div = li.find("div", class_="date")

            if not (h2 and a): continue

            href = a["href"].strip()
            # ç®€å•æ ¡éªŒ
            if not re.search(r"content_\d+\.htm", href): continue

            title = norm(h2.get_text())
            
            # --- ğŸ¯ æ ¸å¿ƒä¿®å¤ï¼šURL æ‹¼æ¥ ---
            # ä½¿ç”¨åˆ—è¡¨é¡µ LIST_URL_FORTUNE ä½œä¸ºåŸºå‡†ï¼Œè§£å†³ç›¸å¯¹è·¯å¾„ 404 é—®é¢˜
            full_url = urljoin(LIST_URL_FORTUNE, href)
            # ---------------------------

            pub_date = norm(date_div.get_text()) if date_div else ""

            items.append({
                "title": title,
                "url": safe_url(full_url), # æ¸…æ´— URL
                "date": pub_date,
            })

        print(f"[Fortune] æŠ“åˆ° {len(items)} æ¡ã€‚")
        return items

    def run(self):
        items = self.fetch_list_page(page=1)
        return items[: self.max_items]


def build_fortune_md_block(items) -> str:
    if not items: return "> è´¢å¯Œä¸­æ–‡ç½‘å•†ä¸šé¢‘é“æš‚æ— æŠ“å–åˆ°å†…å®¹ã€‚\n"
    out = ["**è´¢å¯Œä¸­æ–‡ç½‘ Â· å•†ä¸šé¢‘é“ç²¾é€‰** ", ""]
    for i, it in enumerate(items, 1):
        # é’‰é’‰é“¾æ¥æ ¼å¼ï¼š[æ ‡é¢˜](é“¾æ¥)
        out.append(f"{i}. [{it['title']}]({it['url']})  ï¼ˆ{it['date']}ï¼‰")
    return "\n".join(out) + "\n"


# ================== ä¸»æµç¨‹ï¼šåˆå¹¶æ¨é€ ==================

def build_final_markdown(hr_md: str, fortune_md: str) -> str:
    n = now_tz()
    head = f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰** \n\n"
    head += "**äººèµ„ & å•†ä¸šæƒ…æŠ¥ Â· æ¯æ—¥ç®€æŠ¥** \n\n"
    parts = [
        "### ä¸€ã€HR äººèµ„çƒ­ç‚¹ï¼ˆæ¥è‡ªä¸‰èŒ…ç½‘ï¼‰",
        "",
        hr_md,
        "",
        "### äºŒã€å•†ä¸šè´¢ç»çƒ­ç‚¹ï¼ˆæ¥è‡ªè´¢å¯Œä¸­æ–‡ç½‘ï¼‰",
        "",
        fortune_md,
    ]
    return head + "\n".join(parts)


def main():
    print("=== åˆå¹¶çˆ¬è™«å¼€å§‹æ‰§è¡Œï¼ˆä¸‰èŒ… + è´¢å¯Œä¸­æ–‡ç½‘ï¼‰V12 ===")

    # 1) ä¸‰èŒ…æ—¥æŠ¥
    hr = HRLooCrawler()
    hr.run()
    hr_block = build_hrloo_md_block(hr)

    # 2) è´¢å¯Œå•†ä¸šé¢‘é“
    max_items = int(os.getenv("FORTUNE_MAX_ITEMS") or 5)
    fortune_crawler = FortuneChinaCrawler(max_items=max_items)
    fortune_items = fortune_crawler.run()
    fortune_block = build_fortune_md_block(fortune_items)

    # 3) æ‹¼ Markdown & æ¨é’‰é’‰
    md = build_final_markdown(hr_block, fortune_block)
    print("\n===== Markdown Preview =====\n")
    print(md)

    send_dingtalk_markdown_all("äººèµ„ & å•†ä¸šç®€æŠ¥", md)


if __name__ == "__main__":
    main()
