# -*- coding: utf-8 -*-
"""
ä¸‰èŒ…äººèµ„æ—¥æŠ¥ + è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“
â€”â€” åˆå¹¶ç‰ˆçˆ¬è™« + SiliconFlow AI æ‘˜è¦ + é’‰é’‰ Markdown ä¸€æ¬¡æ¨é€

åŠŸèƒ½æ¦‚è§ˆï¼š
1ï¼‰æŠ“å–ä¸‰èŒ…äººåŠ›èµ„æºç½‘çš„ã€Œä¸‰èŒ…æ—¥æŠ¥ã€è¦ç‚¹åˆ—è¡¨ï¼›
2ï¼‰æŠ“å–è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“æŒ‡å®šæ—¥æœŸæ–°é—»ï¼ˆé»˜è®¤åŒ—äº¬æ—¶é—´æ˜¨å¤©ï¼‰ï¼›
3ï¼‰å¯¹è´¢å¯Œæ–°é—»æ­£æ–‡è°ƒç”¨ SiliconFlowï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰ç”Ÿæˆä¸€å¥è¯ä¸­æ–‡æ‘˜è¦ï¼›
4ï¼‰ä¸‰èŒ… + è´¢å¯Œ ç»“æœåˆå¹¶æˆä¸€æ¡ Markdown æ¶ˆæ¯ï¼›
5ï¼‰é€šè¿‡é’‰é’‰æœºå™¨äººï¼ˆæ”¯æŒå¤šæœºå™¨äººæˆ–å•æœºå™¨äººï¼‰ä¸€æ¬¡æ€§å‘é€ã€‚

ç¯å¢ƒå˜é‡çº¦å®šï¼ˆæŒ‰éœ€é…ç½®ï¼‰ï¼š
- HR_TARGET_DATE          ï¼šä¸‰èŒ…æ—¥æŠ¥ç›®æ ‡æ—¥æœŸï¼ˆYYYY-MM-DDï¼Œä¸å¡«åˆ™é»˜è®¤ä»Šå¤©ï¼‰
- SRC_HRLOO_URLS          ï¼šä¸‰èŒ…æŠ“å–å…¥å£ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼ˆé»˜è®¤ï¼šå®˜ç½‘ + æ–°é—»é¡µï¼‰

- TARGET_DATE             ï¼šè´¢å¯Œä¸­æ–‡ç½‘ç›®æ ‡æ—¥æœŸï¼ˆYYYY-MM-DDï¼Œä¸å¡«åˆ™é»˜è®¤â€œåŒ—äº¬æ—¶é—´æ˜¨å¤©â€ï¼‰
- OPENAI_API_KEY          ï¼šSiliconFlow / OpenAI å…¼å®¹ Keyï¼ˆå½¢å¦‚ sk-xxxï¼‰
- AI_API_BASE             ï¼šSiliconFlow Base URLï¼ˆé»˜è®¤ https://api.siliconflow.cn/v1ï¼‰
- AI_MODEL                ï¼šæ¨¡å‹åï¼ˆé»˜è®¤ Qwen/Qwen2.5-7B-Instructï¼‰

- DINGTALK_BASES          ï¼šé’‰é’‰ webhook åŸºç¡€ URLï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼ˆå« access_tokenï¼‰
- DINGTALK_SECRETS        ï¼šå¯¹åº”çš„ secretï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”
    â€”â€” æˆ–è€…ä½¿ç”¨å•æœºå™¨äººè€é…ç½®ï¼š
- DINGTALK_BASE / DINGTALK_BASEA
- DINGTALK_SECRET / DINGTALK_SECRETA
"""

import os
import re
import time
import csv
import hmac
import ssl
import base64
import hashlib
import urllib.parse
from datetime import datetime, date, timedelta, timezone
from urllib.parse import urljoin, quote_plus

import requests
from bs4 import BeautifulSoup, Tag
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# ===================== é€šç”¨å·¥å…· =====================

try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo


def _tz():
    return ZoneInfo("Asia/Shanghai")


def now_tz():
    return datetime.now(_tz())


def norm(s):
    return re.sub(r"\s+", " ", (s or "").strip())


def zh_weekday(dt):
    return ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][dt.weekday()]


def _sign_webhook(base, secret):
    """
    é’‰é’‰ç­¾åï¼Œå…¼å®¹â€œbase ä¸å¸¦å‚æ•° / å·²å¸¦ ?access_token=â€ä¸¤ç§æƒ…å†µã€‚
    """
    if not base:
        return ""
    if not secret:
        return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(
        base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest())
    )
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"


class LegacyTLSAdapter(HTTPAdapter):
    """
    ä¸ºä¸€äº›è€ç«™ç‚¹å…¼å®¹ TLS
    """

    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)


def make_session():
    s = requests.Session()
    s.headers.update(
        {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120 Safari/537.36"
            ),
            "Accept-Language": "zh-CN,zh;q=0.9",
        }
    )
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500, 502, 503, 504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s


# ===================== ä¸€ã€ä¸‰èŒ… Â· HRLoo ä¸‰èŒ…æ—¥æŠ¥çˆ¬è™« =====================

CN_TITLE_DATE = re.compile(
    r"[ï¼ˆ(]\s*(20\d{2})\s*[å¹´\-/.]\s*(\d{1,2})\s*[æœˆ\-/.]\s*(\d{1,2})\s*[)ï¼‰]"
)


def date_from_bracket_title(text: str):
    m = CN_TITLE_DATE.search(text or "")
    if not m:
        return None
    try:
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        return date(y, mo, d)
    except:
        return None


def looks_like_numbered(text: str) -> bool:
    return bool(
        re.match(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*\S+", text or "")
    )


# â€”â€” ç»Ÿä¸€å»æ‰è‡ªå¸¦ç¼–å·ï¼ˆâ€œ1ã€â€¦/1. â€¦/(1) â€¦/â‘  â€¦/ï¼‘ï¼ â€¦â€ï¼‰
CIRCLED = "â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©"


def strip_leading_num(t: str) -> str:
    t = re.sub(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*", "", t)
    t = re.sub(r"^\s*[" + CIRCLED + r"]\s*", "", t)
    t = re.sub(r"^\s*[ï¼-ï¼™]+\s*[ã€.ï¼]\s*", "", t)
    return t.strip()


class HRLooCrawler:
    """
    ä¸‰èŒ…äººåŠ›èµ„æºç½‘ Â· ä¸‰èŒ…æ—¥æŠ¥ æŠ“å–
    """

    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = 1

        t = (os.getenv("HR_TARGET_DATE") or "").strip()
        if t:
            try:
                y, m, d = map(int, re.split(r"[-/\.]", t))
                self.target_date = date(y, m, d)
            except:
                print("âš ï¸ HR_TARGET_DATE è§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šæ—¥ã€‚")
                self.target_date = now_tz().date()
        else:
            self.target_date = now_tz().date()

        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        self.sources = [
            u.strip()
            for u in os.getenv(
                "SRC_HRLOO_URLS",
                "https://www.hrloo.com/,https://www.hrloo.com/news/hr",
            ).split(",")
            if u.strip()
        ]
        print(f"[CFG] HR target_date={self.target_date} {zh_weekday(now_tz())}  sources={self.sources}")

    def crawl(self):
        """
        å°è¯•ä»æ‰€æœ‰ sources ä¸­æ‰¾åˆ°â€œç¬¦åˆ target_date çš„ä¸‰èŒ…æ—¥æŠ¥â€ï¼Œ
        å‘½ä¸­å self.results[0] å½¢å¦‚ï¼š
        {
            "title": é¡µé¢æ ‡é¢˜,
            "url": è¯¦æƒ…é“¾æ¥,
            "date": "YYYY-MM-DD HH:MM",
            "titles": [è¦ç‚¹1, è¦ç‚¹2, ...]
        }
        """
        for base in self.sources:
            if self._crawl_source(base):
                break

    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=20)
        except Exception as e:
            print("é¦–é¡µè¯·æ±‚å¼‚å¸¸ï¼š", base, e)
            return False
        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥ï¼š", base, r.status_code)
            return False

        soup = BeautifulSoup(r.text, "html.parser")

        # 1ï¼‰æ–°å®¹å™¨ç»“æ„
        items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
        if items:
            for div in items:
                dts = (div.get("dwdata-time") or "").strip()
                if dts:
                    try:
                        pub_d = datetime.strptime(
                            dts.split()[0], "%Y-%m-%d"
                        ).date()
                        if pub_d != self.target_date:
                            continue
                    except:
                        pass
                a = div.find("a", href=True)
                if not a:
                    continue
                title_text = norm(a.get_text())
                if not self.daily_title_pat.search(title_text):
                    continue
                t2 = date_from_bracket_title(title_text)
                if t2 and t2 != self.target_date:
                    continue
                abs_url = urljoin(base, a["href"])
                if self._try_detail(abs_url):
                    return True
            print("[MISS] HR å®¹å™¨é€šé“æœªå‘½ä¸­ï¼š", base)

        # 2ï¼‰fallbackï¼šä» /news/xxx.html é‡Œç­›
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href", "")
            if not re.search(r"/news/\d+\.html$", href):
                continue
            text = norm(a.get_text())
            if not self.daily_title_pat.search(text):
                continue
            t2 = date_from_bracket_title(text)
            if t2 and t2 != self.target_date:
                continue
            links.append(urljoin(base, href))

        seen = set()
        for url in links:
            if url in seen:
                continue
            seen.add(url)
            if self._try_detail(url):
                return True

        print("[MISS] HR æœ¬æºæœªå‘½ä¸­ç›®æ ‡æ—¥æœŸï¼š", base)
        return False

    def _try_detail(self, abs_url):
        pub_dt, titles, page_title = self._fetch_detail_clean(abs_url)
        if not page_title or not self.daily_title_pat.search(page_title):
            return False

        t3 = date_from_bracket_title(page_title)
        if t3 and t3 != self.target_date:
            return False
        if pub_dt and pub_dt.date() != self.target_date and not t3:
            return False
        if not titles:
            return False

        self.results.append(
            {
                "title": page_title,
                "url": abs_url,
                "date": (
                    pub_dt.strftime("%Y-%m-%d %H:%M")
                    if pub_dt
                    else f"{self.target_date} 09:00"
                ),
                "titles": titles,
            }
        )
        print(f"[HR HIT] {abs_url} -> {len(titles)} æ¡")
        return True

    def _extract_pub_time(self, soup: BeautifulSoup):
        cand = []
        for t in soup.select("time[datetime]"):
            cand.append(t.get("datetime", ""))
        for m in soup.select(
            "meta[property='article:published_time'],meta[name='pubdate'],meta[name='publishdate']"
        ):
            cand.append(m.get("content", ""))
        for sel in [
            ".time",
            ".date",
            ".pubtime",
            ".publish-time",
            ".post-time",
            ".info",
            "meta[itemprop='datePublished']",
        ]:
            for x in soup.select(sel):
                if isinstance(x, Tag):
                    cand.append(x.get_text(" ", strip=True))

        pat = re.compile(
            r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?"
        )

        def parse_one(s):
            m = pat.search(s or "")
            if not m:
                return None
            try:
                y, mo, d = int(m[1]), int(m[2]), int(m[3])
                hh = int(m[4]) if m[4] else 9
                mm = int(m[5]) if m[5] else 0
                return datetime(y, mo, d, hh, mm, tzinfo=_tz())
            except:
                return None

        dts = [dt for dt in map(parse_one, cand) if dt]
        if dts:
            now = now_tz()
            past = [dt for dt in dts if dt <= now]
            return min(past or dts, key=lambda dt: abs((now - dt).total_seconds()))
        return None

    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=(6, 20))
            if r.status_code != 200:
                print("[HR DetailFail]", url, r.status_code)
                return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")
            title_tag = soup.find(["h1", "h2"])
            page_title = norm(title_tag.get_text()) if title_tag else ""
            pub_dt = self._extract_pub_time(soup)
            container = soup.select_one(
                ".content-con.hr-rich-text.fn-wenda-detail-infomation.fn-hr-rich-text.custom-style-w"
            ) or soup
            for sel in [
                ".other-wrap",
                ".txt",
                "a.prev.fn-dataStatistics-btn",
                "a.next.fn-dataStatistics-btn",
                ".footer",
                ".bottom",
            ]:
                for bad in container.select(sel):
                    bad.decompose()
            titles = self._extract_strong_titles(container)
            if not titles:
                titles = self._extract_numbered_titles(container)
            return pub_dt, titles, page_title
        except Exception as e:
            print("[HR DetailError]", url, e)
            return None, [], ""

    def _extract_strong_titles(self, root: Tag):
        keep = []
        for st in root.select("strong"):
            text = norm(st.get_text())
            if not text:
                continue
            if len(text) < 4:
                continue
            text = re.split(
                r"[ï¼ˆ(]?(é˜…è¯»|é˜…è¯»é‡|æµè§ˆ|æ¥æº)[:ï¼š]\s*\d+.*$", text
            )[0].strip()
            if not text:
                continue
            text = strip_leading_num(text)
            if text:
                keep.append(text)
        seen, out = set(), []
        for t in keep:
            if t in seen:
                continue
            seen.add(t)
            out.append(t)
        return out

    def _extract_numbered_titles(self, root: Tag):
        out = []
        for p in root.find_all(["p", "h2", "h3", "div", "span", "li"]):
            text = norm(p.get_text())
            if looks_like_numbered(text):
                text = strip_leading_num(text)
                text = re.split(r"[ï¼ˆ(]", text)[0].strip()
                if text and len(text) >= 4:
                    out.append(text)
        seen, final = set(), []
        for t in out:
            if t in seen:
                continue
            seen.add(t)
            final.append(t)
        return final


# ===================== äºŒã€è´¢å¯Œä¸­æ–‡ç½‘ Â· å•†ä¸šé¢‘é“çˆ¬è™« + AI æ‘˜è¦ =====================

FC_BASE = "https://www.fortunechina.com"
FC_LIST_URL_BASE = "https://www.fortunechina.com/shangye/"
FC_MAX_PAGES = 1
FC_MAX_RETRY = 3

FC_OUTPUT_CSV = "fortunechina_articles_with_ai_title.csv"
FC_OUTPUT_MD = "fortunechina_articles_with_ai_title.md"


def get_target_date() -> str:
    """
    å†³å®šè´¢å¯Œä¸­æ–‡ç½‘è¦æŠ“å–çš„ç›®æ ‡æ—¥æœŸï¼š
    1. å¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡ TARGET_DATEï¼ˆä¾‹å¦‚ "2025-12-07"ï¼‰ï¼Œä¼˜å…ˆç”¨å®ƒï¼›
    2. å¦åˆ™é»˜è®¤æŠ“ã€ŒåŒ—äº¬æ—¶é—´æ˜¨å¤©ã€ï¼Œæ ¼å¼ YYYY-MM-DDã€‚
    """
    env_date = os.getenv("TARGET_DATE", "").strip()
    if env_date:
        return env_date

    tz_cn = timezone(timedelta(hours=8))
    yesterday_cn = (datetime.now(tz_cn) - timedelta(days=1)).strftime("%Y-%m-%d")
    return yesterday_cn


FC_TARGET_DATE = get_target_date()

FC_DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": "no-cache",
}

# SiliconFlow / OpenAI å…¼å®¹é…ç½®
AI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
AI_API_BASE = os.getenv("AI_API_BASE", "https://api.siliconflow.cn/v1").rstrip("/")
AI_CHAT_URL = f"{AI_API_BASE}/chat/completions"
AI_MODEL = os.getenv("AI_MODEL", "Qwen/Qwen2.5-7B-Instruct")


def get_ai_summary(content: str, fallback_title: str = "") -> str:
    """
    ä½¿ç”¨ SiliconFlow ç”Ÿæˆä¸€å¥è¯æ‘˜è¦ï¼ˆé•¿åº¦æ§åˆ¶åœ¨ 25 ä¸ªå­—ä»¥å†…ï¼‰ã€‚
    """
    if not content or len(content) < 30:
        return fallback_title or "å†…å®¹è¿‡çŸ­ï¼Œæ— éœ€æ‘˜è¦"

    if not AI_API_KEY:
        print("  âš ï¸ æœªé…ç½® OPENAI_API_KEYï¼Œè·³è¿‡ AI æ‘˜è¦ã€‚")
        return fallback_title or "ï¼ˆæœªé…ç½® AI æ‘˜è¦ï¼‰"

    headers = {
        "Accept": "application/json",
        "Authorization": f"Bearer {AI_API_KEY}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": AI_MODEL,
        "messages": [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¸­æ–‡æ–°é—»ç¼–è¾‘ï¼Œè¯·å°†æ–°é—»æ­£æ–‡æç‚¼æˆä¸€å¥ä¸­æ–‡æ‘˜è¦ï¼Œ"
                    "è¦æ±‚ï¼šå®¢è§‚ã€åŠ¡å®ã€ä¸æ ‡é¢˜å…šï¼Œé•¿åº¦æ§åˆ¶åœ¨ 25 ä¸ªå­—ä»¥å†…ã€‚"
                ),
            },
            {
                "role": "user",
                "content": content[:2000],
            },
        ],
        "max_tokens": 120,
        "temperature": 0.3,
    }

    print(f"  ğŸ¤– æ­£åœ¨è°ƒç”¨ AIï¼ˆ{AI_CHAT_URL}ï¼Œæ¨¡å‹={AI_MODEL}ï¼‰ç”Ÿæˆæ‘˜è¦...")

    try:
        resp = requests.post(AI_CHAT_URL, headers=headers, json=payload, timeout=30)

        if resp.status_code != 200:
            print(f"  âŒ AI çŠ¶æ€ç ï¼š{resp.status_code}")
            try:
                print("  âŒ AI è¿”å›å†…å®¹ï¼š", resp.text)
            except Exception:
                pass
            resp.raise_for_status()

        data = resp.json()
        summary = data["choices"][0]["message"]["content"].strip()
        summary = summary.splitlines()[0].strip()
        print(f"  âœ¨ AI æ‘˜è¦ï¼š{summary}")
        return summary or (fallback_title or "ï¼ˆAI æ‘˜è¦ä¸ºç©ºï¼‰")

    except Exception as e:
        print(f"  âš ï¸ AI è°ƒç”¨å¤±è´¥ï¼š{e}")
        return fallback_title or f"[AI è°ƒç”¨å¤±è´¥: {e}]"


def fc_fetch_list(page: int = 1):
    """
    è´¢å¯Œä¸­æ–‡ç½‘ï¼šæŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ã€‚
    """
    if page == 1:
        current_list_url = FC_LIST_URL_BASE
    else:
        current_list_url = f"{FC_LIST_URL_BASE}?page={page}"

    print(f"\n--- è´¢å¯Œï¼šæ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ({current_list_url}) ---")

    try:
        r = requests.get(current_list_url, headers=FC_DEFAULT_HEADERS, timeout=15)
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []

    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")

        if not (h2 and a and date_div):
            continue

        href = a["href"].strip()
        pub_date = date_div.get_text(strip=True) if date_div else ""

        # åªè¦ç›®æ ‡æ—¥æœŸçš„
        if pub_date != FC_TARGET_DATE:
            continue

        # åªè¦åŒ…å« content_æ•°å­— çš„é“¾æ¥
        if not re.search(r"content_\d+\.htm", href):
            continue

        url_full = urljoin(current_list_url, href)

        items.append(
            {
                "title": h2.get_text(strip=True),
                "url": url_full,
                "date": pub_date,
                "content": "",
                "ai_summary": "",
            }
        )

    print(
        f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°ç›®æ ‡æ—¥æœŸ({FC_TARGET_DATE})æ–‡ç« æ•°ï¼š{len(items)}"
    )
    return items


def fc_fetch_article_content(item: dict):
    """
    è¯·æ±‚è´¢å¯Œæ–‡ç« æ­£æ–‡å†…å®¹
    """
    url = item["url"]
    headers = FC_DEFAULT_HEADERS.copy()
    headers["Referer"] = FC_LIST_URL_BASE

    for attempt in range(FC_MAX_RETRY):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()

            soup = BeautifulSoup(r.text, "html.parser")
            container = soup.select_one("div.article-mod div.word-text-con")
            if not container:
                container = soup.select_one("div.article-content")

            if not container:
                item["content"] = "[æ­£æ–‡å®¹å™¨æœªæ‰¾åˆ°]"
                print(f"  âš ï¸ è­¦å‘Šï¼šURL {url} è®¿é—®æˆåŠŸä½†æœªæ‰¾åˆ°æ­£æ–‡å®¹å™¨")
                return

            paras = [
                p.get_text(strip=True)
                for p in container.find_all("p")
                if p.get_text(strip=True)
            ]
            item["content"] = "\n".join(paras)
            time.sleep(0.5)
            return

        except requests.exceptions.RequestException as e:
            if attempt < FC_MAX_RETRY - 1:
                print(
                    f"  âŒ è¯·æ±‚å¤±è´¥ ({r.status_code if 'r' in locals() else 'Error'}), é‡è¯•ä¸­...: {url}"
                )
                time.sleep(1)
            else:
                print(f"  â›”ï¸ æœ€ç»ˆå¤±è´¥: {url} | é”™è¯¯: {e}")
                item["content"] = f"[è·å–å¤±è´¥: {e}]"


def fc_save_to_csv(data: list, filename: str):
    if not data:
        print("ğŸ’¡ è´¢å¯Œï¼šæ²¡æœ‰æ•°æ®å¯ä¿å­˜ CSVã€‚")
        return

    fieldnames = ["title", "ai_summary", "date", "url", "content"]
    try:
        with open(filename, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"\nğŸ‰ è´¢å¯Œï¼šæˆåŠŸä¿å­˜åˆ° CSVï¼š{filename}ï¼Œå…± {len(data)} æ¡ã€‚")
    except Exception as e:
        print(f"\nâŒ è´¢å¯Œï¼šCSV ä¿å­˜å¤±è´¥ï¼š{e}")


def fc_build_markdown(items: list) -> str:
    """
    è´¢å¯Œå•ç‹¬ Markdownï¼ˆä»…æ–‡ä»¶ä¿å­˜ç”¨ï¼Œä¸å‚ä¸é’‰é’‰åˆå¹¶é€»è¾‘ï¼‰ã€‚
    """
    if not items:
        return (
            f"### è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{FC_TARGET_DATE}ï¼‰\n\n"
            f"ä»Šæ—¥æœªæŠ“åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°é—»ã€‚"
        )

    lines = [
        f"### è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{FC_TARGET_DATE}ï¼‰",
        "",
    ]

    for idx, item in enumerate(items, start=1):
        title = item.get("ai_summary") or item.get("title") or "ï¼ˆæ— æ ‡é¢˜ï¼‰"
        url = item.get("url", "")
        lines.append(f"{idx}. [{title}]({url})")

    return "\n".join(lines)


def fc_save_markdown(content: str, filename: str):
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"\nğŸ“„ è´¢å¯Œï¼šå·²ä¿å­˜ Markdown æ–‡ä»¶ï¼š{filename}")
    except Exception as e:
        print(f"\nâŒ è´¢å¯Œï¼šMarkdown ä¿å­˜å¤±è´¥ï¼š{e}")


def run_fortune_crawler():
    """
    æ‰§è¡Œè´¢å¯Œä¸­æ–‡ç½‘æŠ“å– + AI æ‘˜è¦ + æœ¬åœ° CSV/MD å­˜å‚¨ã€‚
    è¿”å› all_articlesï¼ˆåˆ—è¡¨ï¼Œæ¯ç¯‡åŒ…å« ai_summaryï¼‰ã€‚
    """
    all_articles = []
    print(f"\n=== ğŸš€ è´¢å¯Œçˆ¬è™«å¯åŠ¨ (ç›®æ ‡æ—¥æœŸ: {FC_TARGET_DATE}) ===")
    print(
        f"=== ğŸ› ï¸ è´¢å¯Œè·¯å¾„ç­–ç•¥: åŸºäºåˆ—è¡¨é¡µ URL ({FC_LIST_URL_BASE}) è¿›è¡Œç›¸å¯¹è·¯å¾„æ‹¼æ¥ ==="
    )

    # 1. æŠ“å–åˆ—è¡¨
    for page in range(1, FC_MAX_PAGES + 1):
        list_items = fc_fetch_list(page)
        if not list_items:
            if page == 1:
                print(
                    f"âš ï¸ ç¬¬ 1 é¡µæœªæ‰¾åˆ° {FC_TARGET_DATE} çš„æ–‡ç« ï¼Œè¯·ç¡®è®¤ç½‘ç«™ä¸Šç¡®å®æœ‰è¯¥æ—¥æœŸçš„å†…å®¹ã€‚"
                )
            break
        all_articles.extend(list_items)
        time.sleep(1)

    print(
        f"\n=== ğŸ“¥ è´¢å¯Œé“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡ã€‚å¼€å§‹æŠ“å–æ­£æ–‡ + ç”Ÿæˆ AI æ‘˜è¦... ==="
    )

    # 2. æŠ“å–æ­£æ–‡ + AI æ‘˜è¦
    count = 0
    for item in all_articles:
        count += 1
        print(f"\nğŸ”¥ è´¢å¯Œ ({count}/{len(all_articles)}) å¤„ç†: {item['title']}")
        fc_fetch_article_content(item)
        item["ai_summary"] = get_ai_summary(item["content"], item["title"])

    # 3. ç»Ÿè®¡ä¸ä¿å­˜ CSV/MDï¼ˆä»…æœ¬åœ°è°ƒè¯•ç”¨ï¼‰
    success_count = sum(
        1
        for item in all_articles
        if "è·å–å¤±è´¥" not in item["content"] and item["content"]
    )
    print(
        f"\n=== è´¢å¯Œç»Ÿè®¡: æˆåŠŸ {success_count} ç¯‡ï¼Œå¤±è´¥ {len(all_articles) - success_count} ç¯‡ ==="
    )
    fc_save_to_csv(all_articles, FC_OUTPUT_CSV)
    fc_md_content = fc_build_markdown(all_articles)
    fc_save_markdown(fc_md_content, FC_OUTPUT_MD)

    return all_articles


# ===================== ä¸‰ã€ç»Ÿä¸€é’‰é’‰æ¨é€å·¥å…· =====================

def send_dingtalk_markdown(title: str, text: str):
    """
    ç»Ÿä¸€çš„é’‰é’‰ Markdown å‘é€å‡½æ•°ã€‚
    ä¼˜å…ˆä½¿ç”¨å¤šæœºå™¨äººï¼š
        - DINGTALK_BASES   : webhook åŸºç¡€ URLï¼Œå¤šä¸ªç”¨è‹±æ–‡é€—å·åˆ†éš”
        - DINGTALK_SECRETS : å¯¹åº”çš„ secretï¼Œå¤šä¸ªç”¨è‹±æ–‡é€—å·åˆ†éš”
    è‹¥æœªé…ç½®ï¼Œåˆ™ fallback åˆ°å•æœºå™¨äººï¼š
        - DINGTALK_BASE / DINGTALK_BASEA
        - DINGTALK_SECRET / DINGTALK_SECRETA
    """
    # å¤šæœºå™¨äººæ¨¡å¼
    bases_raw = os.getenv("DINGTALK_BASES", "").strip()
    secrets_raw = os.getenv("DINGTALK_SECRETS", "").strip()

    if bases_raw and secrets_raw:
        bases = [b.strip() for b in bases_raw.split(",") if b.strip()]
        secrets = [s.strip() for s in secrets_raw.split(",") if s.strip()]

        if not bases or len(bases) != len(secrets):
            print("âš ï¸ DINGTALK_BASES ä¸ DINGTALK_SECRETS æ•°é‡ä¸ä¸€è‡´ï¼Œè·³è¿‡å¤šæœºå™¨äººæ¨é€ã€‚")
        else:
            for idx, (base_url, secret) in enumerate(zip(bases, secrets), start=1):
                try:
                    full_url = _sign_webhook(base_url, secret)
                    payload = {
                        "msgtype": "markdown",
                        "markdown": {
                            "title": title,
                            "text": text,
                        },
                        "at": {
                            "isAtAll": False,
                        },
                    }
                    print(f"\nğŸ“¨ æ­£åœ¨å‘ç¬¬ {idx} ä¸ªé’‰é’‰æœºå™¨äººå‘é€æ¶ˆæ¯...")
                    resp = requests.post(full_url, json=payload, timeout=10)
                    print(f"  é’‰é’‰è¿”å›çŠ¶æ€ç ï¼š{resp.status_code}")
                    try:
                        print("  é’‰é’‰è¿”å›ï¼š", resp.text)
                    except Exception:
                        pass
                except Exception as e:
                    print(f"  âš ï¸ ç¬¬ {idx} ä¸ªé’‰é’‰æœºå™¨äººå‘é€å¤±è´¥ï¼š{e}")

    # å•æœºå™¨äºº fallback
    base_single = os.getenv("DINGTALK_BASE") or os.getenv("DINGTALK_BASEA")
    secret_single = os.getenv("DINGTALK_SECRET") or os.getenv("DINGTALK_SECRETA")

    if not base_single:
        if not (bases_raw and secrets_raw):
            print("ğŸ’¡ æœªé…ç½®ä»»ä½•é’‰é’‰ webhookï¼ˆDINGTALK_BASE(S)ï¼‰ï¼Œè·³è¿‡æ¨é€ã€‚")
        return

    try:
        full_url = _sign_webhook(base_single, secret_single)
        payload = {
            "msgtype": "markdown",
            "markdown": {
                "title": title,
                "text": text,
            },
            "at": {
                "isAtAll": False,
            },
        }
        print("\nğŸ“¨ æ­£åœ¨å‘å•ä¸€é’‰é’‰æœºå™¨äººå‘é€æ¶ˆæ¯...")
        resp = requests.post(full_url, json=payload, timeout=10)
        print(f"  é’‰é’‰è¿”å›çŠ¶æ€ç ï¼š{resp.status_code}")
        try:
            print("  é’‰é’‰è¿”å›ï¼š", resp.text)
        except Exception:
            pass
    except Exception as e:
        print(f"  âš ï¸ å•æœºå™¨äººé’‰é’‰å‘é€å¤±è´¥ï¼š{e}")


# ===================== å››ã€åˆå¹¶ Markdownï¼šç»Ÿä¸€ç¼–å· + æ ‡é¢˜å¯ç‚¹å‡» =====================

def _strip_trailing_punc(title: str) -> str:
    """
    å»æ‰æ ‡é¢˜æœ«å°¾å¤šä½™çš„å¥å·/åˆ†å·/æ„Ÿå¹å·/é€—å·ç­‰ï¼Œç„¶åå†ç»Ÿä¸€åŠ â€œï¼›â€ã€‚
    é¿å…å‡ºç°â€œã€‚ï¼›â€â€œï¼›ï¼›â€è¿™ç§é‡å¤æ ‡ç‚¹ã€‚
    """
    if not title:
        return ""
    return re.sub(r"[ï¼›;ã€‚.!ï¼?ï¼Ÿã€ï¼Œ,]+$", "", title.strip())


def build_clean_markdown(hr_items: list, fc_items: list) -> str:
    """
    æŠŠä¸‰èŒ…æ—¥æŠ¥ + è´¢å¯Œå•†ä¸šé¢‘é“ åˆå¹¶ä¸ºä¸€æ¡ Markdown æ–‡æœ¬ï¼š
    - ä¸å†åŒºåˆ†æ¥æºï¼›
    - æ‰€æœ‰æ¡ç›®ç»Ÿä¸€ 1ã€2ã€3... ç¼–å·ï¼›
    - æ¯æ¡å†™æˆï¼š[æ ‡é¢˜](URL)ï¼›  ï¼ˆç‚¹å‡»æ ‡é¢˜å³å¯è·³è½¬ï¼‰ï¼›
    """
    now_cn = now_tz()
    today_str = now_cn.strftime("%Y-%m-%d")
    weekday_str = zh_weekday(now_cn)

    merged_items = []

    # å…ˆæ”¾ä¸‰èŒ… titlesï¼ˆåŒä¸€ç¯‡æ–‡ç« ï¼Œé“¾æ¥ç›¸åŒæ²¡é—®é¢˜ï¼‰
    if hr_items and hr_items[0].get("titles"):
        it = hr_items[0]
        detail_url = it.get("url", "")
        for t in it["titles"]:
            title = _strip_trailing_punc(t)
            if not title:
                continue
            merged_items.append({
                "title": title,
                "url": detail_url or "#"
            })

    # å†æ”¾è´¢å¯Œæ–°é—»ï¼ˆä½¿ç”¨ AI æ‘˜è¦ä½œä¸ºæ ‡é¢˜ä¼˜å…ˆï¼‰
    for art in fc_items or []:
        raw_title = (art.get("ai_summary") or art.get("title") or "")
        title = _strip_trailing_punc(raw_title)
        if not title:
            continue
        merged_items.append({
            "title": title,
            "url": art.get("url", "#")
        })

    # å¦‚æœä»Šå¤©å•¥ä¹Ÿæ²¡æŠ“åˆ°
    if not merged_items:
        return f"æ—¥æœŸï¼š{today_str}ï¼ˆ{weekday_str}ï¼‰\nä»Šæ—¥æœªæŠ“å–åˆ°æœ‰æ•ˆèµ„è®¯ã€‚"

    lines = [
        f"æ—¥æœŸï¼š{today_str}ï¼ˆ{weekday_str}ï¼‰",
        "æ¯æ—¥èµ„è®¯è¦ç‚¹",
        ""
    ]

    for idx, item in enumerate(merged_items, start=1):
        title = item["title"]
        url = item["url"]
        # æ¯æ¡ï¼šç¼–å· + å¯ç‚¹å‡»æ ‡é¢˜ + ä¸­æ–‡åˆ†å·
        lines.append(f"{idx}. [{title}]({url})ï¼›")

    return "\n".join(lines)


# ===================== äº”ã€ä¸»å…¥å£ =====================

def main():
    print("=== æ‰§è¡Œåˆå¹¶çˆ¬è™«ï¼šä¸‰èŒ… + è´¢å¯Œä¸­æ–‡ç½‘ ===")

    # 1. ä¸‰èŒ…æ—¥æŠ¥
    print("\n>>> [æ­¥éª¤1] æŠ“å–ä¸‰èŒ…äººåŠ›èµ„æºç½‘ Â· ä¸‰èŒ…æ—¥æŠ¥")
    hr_crawler = HRLooCrawler()
    hr_crawler.crawl()
    hr_results = hr_crawler.results

    # 2. è´¢å¯Œä¸­æ–‡ç½‘
    print("\n>>> [æ­¥éª¤2] æŠ“å–è´¢å¯Œä¸­æ–‡ç½‘ Â· å•†ä¸šé¢‘é“ + AI æ‘˜è¦")
    fc_articles = run_fortune_crawler()

    # 3. åˆå¹¶ Markdownï¼ˆæ–°çš„ç»Ÿä¸€ç¼–å· + å¯ç‚¹å‡»æ ‡é¢˜æ ·å¼ï¼‰
    print("\n>>> [æ­¥éª¤3] ç”Ÿæˆåˆå¹¶ Markdown æ¶ˆæ¯ï¼ˆç»Ÿä¸€ç¼–å· + æ ‡é¢˜å¯ç‚¹å‡»ï¼‰")
    combined_md = build_clean_markdown(hr_results, fc_articles)
    print("\n===== åˆå¹¶ Markdown é¢„è§ˆ =====\n")
    print(combined_md)

    # 4. å‘é€é’‰é’‰
    print("\n>>> [æ­¥éª¤4] æ¨é€åˆ°é’‰é’‰æœºå™¨äºº")
    md_title = f"äººèµ„ & å•†ä¸šèµ„è®¯æ—¥æŠ¥ï¼ˆ{now_tz().strftime('%Y-%m-%d')}ï¼‰"
    send_dingtalk_markdown(md_title, combined_md)


if __name__ == "__main__":
    main()
