# -*- coding: utf-8 -*-
"""
äººç¤¾éƒ¨ - æ–°é—»ä¸­å¿ƒ - åœ°æ–¹åŠ¨æ€
æŒ‰å·¥ä½œæ—¥è§„åˆ™æŠ“å– + é’‰é’‰å®éªŒç¾¤æ¨é€ï¼ˆå®Œæ•´ä»£ç ï¼‰

è§„åˆ™ï¼š
- å‘¨ä¸€ï¼šæŠ“ä¸Šå‘¨äº”
- å‘¨äºŒ~å‘¨äº”ï¼šæŠ“å‰ä¸€å¤©
- å‘¨å…­/å‘¨æ—¥ï¼šä¸æŠ“

é’‰é’‰ï¼ˆå®éªŒç¾¤ï¼‰ç¯å¢ƒå˜é‡ï¼š
- SHIYANQUNWEBHOOK  é’‰é’‰æœºå™¨äºº webhookï¼ˆå« access_tokenï¼‰
- SHIYANQUNSECRET   é’‰é’‰æœºå™¨äººåŠ ç­¾ secret

å¯é€‰ï¼š
- HR_TZ   é»˜è®¤ Asia/Shanghai
- LIST_URL è¦†ç›–åˆ—è¡¨é¡µåœ°å€
"""

import os
import re
import json
import time
import hmac
import base64
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urljoin, quote_plus

import requests
from bs4 import BeautifulSoup

try:
    from zoneinfo import ZoneInfo
except Exception:
    from backports.zoneinfo import ZoneInfo


UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123 Safari/537.36"

# âœ… æ”¹åŠ¨ç‚¹ï¼šé»˜è®¤æ”¹æˆâ€œæ–°é—»ä¸­å¿ƒ-åœ°æ–¹åŠ¨æ€â€æ ç›®ç›®å½•ï¼ˆä¸ä½ æˆªå›¾ä¸€è‡´ï¼‰
DEFAULT_LIST_URL = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/dfdt/"

RE_DATE = re.compile(r"\b(20\d{2}-\d{2}-\d{2})\b")


def _tz():
    return ZoneInfo(os.getenv("HR_TZ", "Asia/Shanghai"))


def now_tz():
    return datetime.now(_tz())


def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def zh_weekday(dt: datetime) -> str:
    return ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][dt.weekday()]


def compute_target_date(now: datetime) -> str | None:
    wd = now.weekday()
    if wd == 0:  # å‘¨ä¸€ -> ä¸Šå‘¨äº”
        return (now - timedelta(days=3)).strftime("%Y-%m-%d")
    if 1 <= wd <= 4:  # å‘¨äºŒ~å‘¨äº” -> æ˜¨å¤©
        return (now - timedelta(days=1)).strftime("%Y-%m-%d")
    return None


def fetch_html(url: str) -> str:
    s = requests.Session()
    s.headers.update({"User-Agent": UA})
    r = s.get(url, timeout=25)
    r.raise_for_status()
    return r.text


def parse_list(html: str, page_url: str) -> list[dict]:
    """
    é²æ£’è§£æï¼šä¸ä¾èµ–å›ºå®š class
    æ€è·¯ï¼š
    - åœ¨é¡µé¢é‡Œæ‰¾æ‰€æœ‰å‡ºç° YYYY-MM-DD çš„èŠ‚ç‚¹
    - å¾€ä¸Šæ‰¾çˆ¶å®¹å™¨ï¼ˆæœ€å¤š 8 å±‚ï¼‰ï¼Œåœ¨å®¹å™¨å†…æ‰¾ <a href> å½“æ ‡é¢˜é“¾æ¥
    """
    soup = BeautifulSoup(html, "html.parser")
    items = []

    # 1) æ‰¾åˆ°æ‰€æœ‰â€œå«æ—¥æœŸæ–‡æœ¬â€çš„èŠ‚ç‚¹
    date_nodes = soup.find_all(string=lambda s: bool(s and RE_DATE.search(str(s))))
    for node in date_nodes:
        date_text = RE_DATE.search(str(node)).group(1)

        container = node.parent
        for _ in range(8):
            if not container:
                break
            a = container.find("a", href=True)
            if a and norm(a.get_text()):
                href = a["href"].strip()
                # åªè¦åƒæ–‡ç« é¡µï¼ˆt2024...html / .htmlï¼‰ï¼Œå°±æ”¶
                if ".html" in href:
                    items.append({
                        "date": date_text,
                        "title": norm(a.get_text()),
                        "url": urljoin(page_url, href)
                    })
                    break
            container = container.parent

    # 2) å…œåº•ï¼šå¦‚æœä¸Šé¢ä»ç„¶æŠ“ä¸åˆ°ï¼Œç›´æ¥æ‰«æ‰€æœ‰ aï¼Œåœ¨çˆ¶å®¹å™¨æ–‡æœ¬é‡Œæ‰¾æ—¥æœŸ
    if not items:
        for a in soup.find_all("a", href=True):
            title = norm(a.get_text())
            if not title:
                continue
            href = a["href"].strip()
            if ".html" not in href:
                continue

            parent = a
            found_date = None
            for _ in range(8):
                if not parent:
                    break
                txt = norm(parent.get_text(" "))
                m = RE_DATE.search(txt)
                if m:
                    found_date = m.group(1)
                    break
                parent = parent.parent

            if found_date:
                items.append({
                    "date": found_date,
                    "title": title,
                    "url": urljoin(page_url, href)
                })

    # å»é‡
    seen = set()
    uniq = []
    for it in items:
        key = (it["date"], it["title"], it["url"])
        if key in seen:
            continue
        seen.add(key)
        uniq.append(it)

    uniq.sort(key=lambda x: (x["date"], x["title"]), reverse=True)
    return uniq


def signed_dingtalk_url(webhook: str, secret: str) -> str:
    timestamp = str(int(time.time() * 1000))
    string_to_sign = f"{timestamp}\n{secret}"
    h = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), digestmod=hashlib.sha256).digest()
    sign = quote_plus(base64.b64encode(h))
    joiner = "&" if "?" in webhook else "?"
    return f"{webhook}{joiner}timestamp={timestamp}&sign={sign}"


def send_to_shiyanqun(title: str, markdown: str):
    webhook = os.getenv("SHIYANQUNWEBHOOK", "").strip()
    secret = os.getenv("SHIYANQUNSECRET", "").strip()

    if not webhook or not secret:
        print("[WARN] æœªé…ç½® SHIYANQUNWEBHOOK / SHIYANQUNSECRETï¼Œè·³è¿‡é’‰é’‰æ¨é€ã€‚")
        return {"skipped": True}

    url = signed_dingtalk_url(webhook, secret)
    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": markdown}}

    r = requests.post(url, json=payload, timeout=25)
    r.raise_for_status()
    data = r.json()
    if data.get("errcode") not in (0, None):
        raise RuntimeError(f"é’‰é’‰å‘é€å¤±è´¥ï¼š{data}")
    return data


def build_markdown(list_url: str, target_date: str, items: list[dict], now: datetime):
    title = f"ğŸ“° äººç¤¾éƒ¨Â·åœ°æ–¹åŠ¨æ€ï¼ˆ{target_date}ï¼‰"
    head = [
        f"### ğŸ“° äººç¤¾éƒ¨Â·åœ°æ–¹åŠ¨æ€ï¼ˆç›®æ ‡æ—¥ï¼š**{target_date}**ï¼‰",
        f"- æŠ“å–æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆ{zh_weekday(now)}ï¼‰",
        f"- åˆ—è¡¨é¡µï¼š{list_url}",
        ""
    ]
    if not items:
        return title, "\n".join(head + ["æœ¬æ¬¡æœªåŒ¹é…åˆ°ç›®æ ‡æ—¥æœŸçš„å†…å®¹ã€‚"])

    body = [f"{i}. [{it['title']}]({it['url']})  `({it['date']})`" for i, it in enumerate(items, 1)]
    return title, "\n".join(head + body + ["", f"â€”â€” å…± **{len(items)}** æ¡"])


def main():
    list_url = os.getenv("LIST_URL", DEFAULT_LIST_URL).strip()
    now = now_tz()
    target = compute_target_date(now)

    if not target:
        print("å‘¨æœ«ï¼Œä¸æ‰§è¡Œã€‚")
        return

    print(f"[INFO] ç›®æ ‡æ—¥æœŸï¼š{target}")
    html = fetch_html(list_url)
    items = parse_list(html, list_url)
    hit = [x for x in items if x["date"] == target]

    print(f"[INFO] è§£æ {len(items)} æ¡ï¼Œå‘½ä¸­ {len(hit)} æ¡ã€‚")

    out_path = f"mohrss_local_news_{target}.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump({"target_date": target, "list_url": list_url, "items": hit}, f, ensure_ascii=False, indent=2)
    print(f"[INFO] å·²å†™å‡ºï¼š{out_path}")

    title, md = build_markdown(list_url, target, hit, now)
    resp = send_to_shiyanqun(title, md)
    print(f"[INFO] é’‰é’‰è¿”å›ï¼š{resp}")


if __name__ == "__main__":
    main()
