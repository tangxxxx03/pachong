# -*- coding: utf-8 -*-
"""
äººç¤¾éƒ¨ - åœ°æ–¹åŠ¨æ€ï¼ˆPlaywright æ¸²æŸ“ + é²æ£’è§£æï¼‰
é’‰é’‰è¾“å‡ºï¼šç±»ä¼¼â€œå›¾2â€é‚£ç§ç¼–å·åˆ—è¡¨ + æŸ¥çœ‹è¯¦ç»†ï¼ˆmarkdown å¡ç‰‡é£æ ¼ï¼‰

æ¨é€æ•ˆæœï¼š
äººåŠ›èµ„è®¯ï¼ˆæˆ– åœ°æ–¹æ”¿ç­–ï¼‰
1. æ ‡é¢˜
2. æ ‡é¢˜
...
ğŸ‘‰ æŸ¥çœ‹è¯¦ç»†

è§„åˆ™ï¼š
- å‘¨ä¸€ï¼šæŠ“ä¸Šå‘¨äº”
- å‘¨äºŒ~å‘¨äº”ï¼šæŠ“å‰ä¸€å¤©
- å‘¨å…­/å‘¨æ—¥ï¼šä¸æŠ“

é’‰é’‰ç¯å¢ƒå˜é‡ï¼š
- SHIYANQUNWEBHOOK
- SHIYANQUNSECRET
"""

import os
import re
import time
import hmac
import base64
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urljoin, quote_plus

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

try:
    from zoneinfo import ZoneInfo
except Exception:
    from backports.zoneinfo import ZoneInfo


DEFAULT_LIST_BASE = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/dfdt/"
DEFAULT_LIST_URL = urljoin(DEFAULT_LIST_BASE, "index.html")

RE_DATE_DASH = re.compile(r"\b(20\d{2}-\d{2}-\d{2})\b")
RE_DATE_CN = re.compile(r"\b(20\d{2})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥\b")


def _tz():
    return ZoneInfo(os.getenv("HR_TZ", "Asia/Shanghai"))


def now_tz():
    return datetime.now(_tz())


def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def compute_target_date(now: datetime) -> str | None:
    wd = now.weekday()
    if wd == 0:
        return (now - timedelta(days=3)).strftime("%Y-%m-%d")
    if 1 <= wd <= 4:
        return (now - timedelta(days=1)).strftime("%Y-%m-%d")
    return None


def normalize_date_text(text: str) -> str | None:
    if not text:
        return None
    s = norm(text)

    m1 = RE_DATE_DASH.search(s)
    if m1:
        return m1.group(1)

    m2 = RE_DATE_CN.search(s)
    if m2:
        y = m2.group(1)
        mo = int(m2.group(2))
        d = int(m2.group(3))
        return f"{y}-{mo:02d}-{d:02d}"

    return None


def fetch_rendered_html(url: str, retries: int = 2) -> str:
    last_html = ""
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=["--disable-blink-features=AutomationControlled", "--no-sandbox", "--disable-dev-shm-usage"],
        )

        for attempt in range(retries + 1):
            page = browser.new_page(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/123.0.0.0 Safari/537.36"
                ),
                locale="zh-CN",
                timezone_id="Asia/Shanghai",
            )
            page.set_extra_http_headers({"Accept-Language": "zh-CN,zh;q=0.9"})

            try:
                page.goto(url, wait_until="domcontentloaded", timeout=60000)
                try:
                    page.wait_for_function(
                        "document.body && /20\\d{2}-\\d{2}-\\d{2}/.test(document.body.innerText)",
                        timeout=12000
                    )
                except Exception:
                    page.wait_for_timeout(1500)

                html = page.content()
                last_html = html

                # å¤ªçŸ­å°±å½“ä½œç©ºå£³ï¼Œé‡è¯•
                if len(html or "") < 5000:
                    page.close()
                    time.sleep(1.2)
                    continue

                page.close()
                browser.close()
                return html

            except Exception:
                try:
                    page.close()
                except Exception:
                    pass
                time.sleep(1.2)

        browser.close()
        return last_html


def parse_list_robust(html: str, page_url: str) -> list[dict]:
    soup = BeautifulSoup(html, "html.parser")
    items = []

    # æ—¥æœŸèŠ‚ç‚¹ -> å›æº¯æ‰¾åŒæ¡ç›®çš„æ–‡ç« é“¾æ¥
    for node in soup.find_all(string=True):
        dt = normalize_date_text(str(node))
        if not dt:
            continue

        container = node.parent
        for _ in range(12):
            if not container:
                break
            a = container.find("a", href=True)
            if a and norm(a.get_text()):
                href = a["href"].strip()
                if ".html" in href:
                    items.append({
                        "date": dt,
                        "title": norm(a.get_text()),
                        "url": urljoin(page_url, href)
                    })
                    break
            container = container.parent

    # å»é‡ + æ’åº
    seen, uniq = set(), []
    for it in items:
        key = (it["date"], it["title"], it["url"])
        if key in seen:
            continue
        seen.add(key)
        uniq.append(it)

    uniq.sort(key=lambda x: (x["date"], x["title"]), reverse=True)
    return uniq


def signed_dingtalk_url(webhook: str, secret: str) -> str:
    timestamp = str(int(time.time() * 1000))
    string_to_sign = f"{timestamp}\n{secret}"
    h = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), hashlib.sha256).digest()
    sign = quote_plus(base64.b64encode(h))
    joiner = "&" if "?" in webhook else "?"
    return f"{webhook}{joiner}timestamp={timestamp}&sign={sign}"


def dingtalk_send_markdown(title: str, md: str):
    webhook = os.getenv("SHIYANQUNWEBHOOK", "").strip()
    secret = os.getenv("SHIYANQUNSECRET", "").strip()

    if not webhook or not secret:
        print("[WARN] æœªé…ç½® SHIYANQUNWEBHOOK / SHIYANQUNSECRETï¼Œè·³è¿‡æ¨é€")
        return

    url = signed_dingtalk_url(webhook, secret)
    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md}}

    r = requests.post(url, json=payload, timeout=25)
    r.raise_for_status()
    resp = r.json()
    if resp.get("errcode") not in (0, None):
        raise RuntimeError(f"é’‰é’‰å‘é€å¤±è´¥ï¼š{resp}")


def build_card_style_markdown(card_title: str, hit: list[dict], detail_url: str) -> tuple[str, str]:
    """
    è¾“å‡ºç±»ä¼¼å›¾2ï¼š
    äººåŠ›èµ„è®¯
    1. xxx
    2. xxx
    ğŸ‘‰ æŸ¥çœ‹è¯¦ç»†
    """
    lines = [f"## {card_title}"]
    for i, it in enumerate(hit, 1):
        # è¿™é‡Œç”¨çº¯æ–‡æœ¬æ ‡é¢˜ï¼›å¦‚æœä½ æƒ³æ¯æ¡å¯ç‚¹å¼€ï¼ŒæŠŠä¸‹é¢ä¸€è¡Œæ¢æˆ markdown é“¾æ¥å³å¯
        # lines.append(f"{i}. [{it['title']}]({it['url']})")
        lines.append(f"{i}. {it['title']}")
    lines.append("")
    lines.append(f"ğŸ‘‰ [æŸ¥çœ‹è¯¦ç»†]({detail_url})")

    # é’‰é’‰ markdown çš„ title å»ºè®®çŸ­ä¸€ç‚¹
    return card_title, "\n".join(lines)


def main():
    now = now_tz()
    target = compute_target_date(now)
    if not target:
        print("[INFO] å‘¨æœ«ä¸è¿è¡Œ")
        return

    list_base = os.getenv("LIST_BASE", DEFAULT_LIST_BASE).strip()
    list_url = os.getenv("LIST_URL", DEFAULT_LIST_URL).strip()

    html = fetch_rendered_html(list_url, retries=2)
    items = parse_list_robust(html, list_url)
    hit = [x for x in items if x["date"] == target]

    print(f"[INFO] ç›®æ ‡æ—¥æœŸï¼š{target}")
    print(f"[INFO] è§£ææ€»æ¡æ•°ï¼š{len(items)}ï¼Œå‘½ä¸­ï¼š{len(hit)}")

    # å‘½ä¸­ 0ï¼šä¸æ¨é€
    if not hit:
        print("[INFO] å‘½ä¸­ 0ï¼Œä¸æ¨é€")
        return

    # âœ… ä½ æƒ³è¦çš„â€œå›¾2æ ·å¼â€
    card_title = "äººåŠ›èµ„è®¯"  # ä½ è¦æ”¹æˆâ€œåœ°æ–¹æ”¿ç­–â€ä¹Ÿè¡Œ
    # å¦‚æœä½ æƒ³æ ‡é¢˜é‡Œå¸¦æ•°é‡ï¼Œå¯æ”¹æˆï¼šcard_title = f"åœ°æ–¹æ”¿ç­–ï¼ˆ{len(hit)}ï¼‰"
    title, md = build_card_style_markdown(card_title, hit, detail_url=list_url)
    dingtalk_send_markdown(title, md)
    print(md)


if __name__ == "__main__":
    main()
