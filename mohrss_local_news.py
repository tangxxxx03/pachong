# -*- coding: utf-8 -*-
"""
äººç¤¾éƒ¨-æ–°é—»ä¸­å¿ƒ-åœ°æ–¹åŠ¨æ€ï¼šæŒ‰æ—¥æœŸæŠ“å–å¹¶ç”¨é’‰é’‰æœºå™¨äººæ¨é€ï¼ˆå®Œæ•´ä»£ç ï¼‰

è§„åˆ™ï¼š
- å‘¨ä¸€ï¼šæŠ“ä¸Šå‘¨äº”
- å‘¨äºŒ~å‘¨äº”ï¼šæŠ“å‰ä¸€å¤©
- å‘¨å…­/å‘¨æ—¥ï¼šä¸æŠ“ï¼ˆå¯è‡ªè¡Œæ”¹ï¼‰

é’‰é’‰ï¼ˆå¯é€‰ï¼‰ï¼š
- è‡ªå®šä¹‰æœºå™¨äºº + åŠ ç­¾
- ç¯å¢ƒå˜é‡ï¼ˆå»ºè®® GitHub Secretsï¼‰ï¼š
  - DINGTALK_BASE   ä¾‹ï¼šhttps://oapi.dingtalk.com/robot/send?access_token=xxxxx
  - DINGTALK_SECRET æœºå™¨äººåŠ ç­¾ secret

å…¶ä»–å¯é€‰ç¯å¢ƒå˜é‡ï¼š
  - HR_TZ           é»˜è®¤ Asia/Shanghai
  - LIST_URL        è¦†ç›–åˆ—è¡¨é¡µåœ°å€
"""

import os
import re
import json
import time
import hmac
import base64
import hashlib
from datetime import datetime, timedelta
from urllib.parse import urljoin, quote_plus

import requests
from bs4 import BeautifulSoup

try:
    from zoneinfo import ZoneInfo
except Exception:
    from backports.zoneinfo import ZoneInfo


UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123 Safari/537.36"
DEFAULT_LIST_URL = "https://www.mohrss.gov.cn/SYrlzyhshbzb/rdzt/gzdt/"


def _tz():
    return ZoneInfo(os.getenv("HR_TZ", "Asia/Shanghai"))


def now_tz():
    return datetime.now(_tz())


def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def zh_weekday(dt: datetime) -> str:
    return ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"][dt.weekday()]


def compute_target_date(now: datetime) -> str | None:
    """
    - å‘¨ä¸€ï¼šæŠ“ä¸Šå‘¨äº”ï¼ˆ-3å¤©ï¼‰
    - å‘¨äºŒ~å‘¨äº”ï¼šæŠ“æ˜¨å¤©ï¼ˆ-1å¤©ï¼‰
    - å‘¨å…­/å‘¨æ—¥ï¼šNoneï¼ˆä¸æŠ“ï¼‰
    """
    wd = now.weekday()
    if wd == 0:
        return (now - timedelta(days=3)).strftime("%Y-%m-%d")
    if 1 <= wd <= 4:
        return (now - timedelta(days=1)).strftime("%Y-%m-%d")
    return None


def fetch_html(url: str) -> str:
    s = requests.Session()
    s.headers.update({"User-Agent": UA})
    r = s.get(url, timeout=25)
    r.raise_for_status()
    return r.text


def parse_list(html: str, page_url: str) -> list[dict]:
    """
    è§£æåˆ—è¡¨é¡µï¼štitle + url + date(YYYY-MM-DD)
    ä½ çš„æˆªå›¾é‡Œæ—¥æœŸæ˜¯ span.organMenuTxtLinkï¼Œæ ‡é¢˜æ˜¯ a æ ‡ç­¾
    """
    soup = BeautifulSoup(html, "html.parser")
    items = []

    # æ–¹æ¡ˆ1ï¼šæŒ‰æ—¥æœŸ span å®šä½
    date_spans = soup.select("span.organMenuTxtLink")
    for sp in date_spans:
        date_text = norm(sp.get_text())
        if not re.fullmatch(r"\d{4}-\d{2}-\d{2}", date_text):
            continue

        container = sp
        for _ in range(6):
            if container is None:
                break
            a = container.find("a", href=True)
            if a and norm(a.get_text()):
                title = norm(a.get_text())
                href = a["href"].strip()
                full_url = urljoin(page_url, href)
                items.append({"date": date_text, "title": title, "url": full_url})
                break
            container = container.parent

    # å…œåº•ï¼šæŠ“æ‰€æœ‰ a å¹¶åœ¨çˆ¶å®¹å™¨æ‰¾æ—¥æœŸ
    if not items:
        for a in soup.find_all("a", href=True):
            title = norm(a.get_text())
            if not title:
                continue
            href = a["href"].strip()
            if ".html" not in href:
                continue

            parent = a
            found_date = None
            for _ in range(6):
                if parent is None:
                    break
                txt = norm(parent.get_text(" "))
                m = re.search(r"\b(20\d{2}-\d{2}-\d{2})\b", txt)
                if m:
                    found_date = m.group(1)
                    break
                parent = parent.parent

            if found_date:
                items.append({"date": found_date, "title": title, "url": urljoin(page_url, href)})

    # å»é‡
    seen = set()
    uniq = []
    for it in items:
        key = (it["date"], it["title"], it["url"])
        if key in seen:
            continue
        seen.add(key)
        uniq.append(it)

    uniq.sort(key=lambda x: (x["date"], x["title"]), reverse=True)
    return uniq


def dingtalk_signed_url(base_url: str, secret: str) -> str:
    timestamp = str(int(time.time() * 1000))
    string_to_sign = f"{timestamp}\n{secret}"
    h = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), digestmod=hashlib.sha256).digest()
    sign = quote_plus(base64.b64encode(h))
    joiner = "&" if "?" in base_url else "?"
    return f"{base_url}{joiner}timestamp={timestamp}&sign={sign}"


def dingtalk_send_markdown(title: str, markdown: str):
    base = os.getenv("DINGTALK_BASE", "").strip()
    secret = os.getenv("DINGTALK_SECRET", "").strip()

    # âœ… æ”¹åŠ¨ç‚¹ï¼šæ²¡é…ç½®é’‰é’‰å°±è·³è¿‡ï¼Œä¸è®© workflow å¤±è´¥
    if not base or not secret:
        print("[WARN] æœªé…ç½® DINGTALK_BASE / DINGTALK_SECRETï¼Œè·³è¿‡é’‰é’‰æ¨é€ã€‚")
        return {"skipped": True}

    url = dingtalk_signed_url(base, secret)

    payload = {
        "msgtype": "markdown",
        "markdown": {
            "title": title,
            "text": markdown
        }
    }

    r = requests.post(url, json=payload, timeout=25)
    r.raise_for_status()
    data = r.json()
    if data.get("errcode") != 0:
        raise RuntimeError(f"é’‰é’‰å‘é€å¤±è´¥ï¼š{data}")
    return data


def build_markdown(list_url: str, target_date: str, items: list[dict], now: datetime) -> tuple[str, str]:
    title = f"ğŸ“° äººç¤¾éƒ¨Â·åœ°æ–¹åŠ¨æ€ï¼ˆ{target_date}ï¼‰"

    head = [
        f"### ğŸ“° äººç¤¾éƒ¨Â·åœ°æ–¹åŠ¨æ€ï¼ˆç›®æ ‡æ—¥ï¼š**{target_date}**ï¼‰",
        f"- æŠ“å–æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆ{zh_weekday(now)}ï¼‰",
        f"- åˆ—è¡¨é¡µï¼š{list_url}",
        ""
    ]

    if not items:
        body = [
            "æœ¬æ¬¡æœªåŒ¹é…åˆ°ç›®æ ‡æ—¥æœŸçš„æ¡ç›®ã€‚",
            "",
            "> å¯èƒ½åŸå› ï¼šå½“å¤©æœªå‘å¸ƒ / é¡µé¢å»¶è¿Ÿæ›´æ–° / åˆ—è¡¨é¡µç»“æ„å˜åŠ¨ã€‚",
        ]
        return title, "\n".join(head + body)

    lines = []
    for i, it in enumerate(items, 1):
        lines.append(f"{i}. [{it['title']}]({it['url']})  `({it['date']})`")

    tail = ["", f"â€”â€” å…± **{len(items)}** æ¡"]
    return title, "\n".join(head + lines + tail)


def main():
    list_url = os.getenv("LIST_URL", DEFAULT_LIST_URL).strip()
    now = now_tz()
    target = compute_target_date(now)

    if not target:
        print("ä»Šå¤©æ˜¯å‘¨æœ«ï¼ˆæˆ–æœªå®‰æ’æŠ“å–æ—¥ï¼‰ï¼ŒæŒ‰è§„åˆ™ä¸æŠ“å–ï¼Œä¹Ÿä¸æ¨é€ã€‚")
        return

    print(f"[INFO] ç›®æ ‡æ—¥æœŸï¼š{target}")

    html = fetch_html(list_url)
    items = parse_list(html, list_url)
    hit = [x for x in items if x.get("date") == target]

    print(f"[INFO] è§£æ {len(items)} æ¡ï¼Œå‘½ä¸­ {len(hit)} æ¡ã€‚")

    out = {
        "source": "mohrss_local_news",
        "list_url": list_url,
        "target_date": target,
        "count": len(hit),
        "items": hit,
        "generated_at": now.strftime("%Y-%m-%d %H:%M:%S"),
    }
    out_path = f"mohrss_local_news_{target}.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
    print(f"[INFO] å·²å†™å‡ºï¼š{out_path}")

    title, md = build_markdown(list_url, target, hit, now)
    resp = dingtalk_send_markdown(title, md)
    print(f"[INFO] é’‰é’‰è¿”å›ï¼š{resp}")


if __name__ == "__main__":
    main()
