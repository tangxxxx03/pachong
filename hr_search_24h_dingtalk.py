# -*- coding: utf-8 -*-
"""
hr_news_auto_range.py
æŠ“å–ä¸¤ç«™å›ºå®šæ ç›®ï¼ˆä¸éœ€è¦å…³é”®è¯ï¼‰ï¼š
  1) äººç¤¾éƒ¨ï¼šrsxw äººç¤¾æ–°é—»æ ç›®
     https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/buneiyaowen/rsxw/
  2) ä¸­å›½å…¬å…±æ‹›è˜ç½‘ï¼šèµ„è®¯åˆ—è¡¨é¦–é¡µï¼ˆä¸å¸¦å…³é”®è¯ï¼‰
     http://job.mohrss.gov.cn/zxss/index.jhtml

æ—¶é—´ç­–ç•¥ï¼ˆé»˜è®¤ï¼‰ï¼š
  - éå‘¨ä¸€ï¼šæŠ“â€œæ˜¨å¤©â€
  - å‘¨ä¸€ï¼šæŠ“â€œè¿‘3å¤©åˆè¾‘â€ï¼ˆå¯é€šè¿‡ env/å‚æ•°ä¿®æ”¹ä¸º N å¤©ï¼‰
ä¹Ÿæ”¯æŒï¼š
  --date yesterday / YYYY-MM-DD
  --window-hours N  ï¼ˆåœ¨ --auto-range ç¦ç”¨æ—¶ç”Ÿæ•ˆï¼‰

ä¾èµ–ï¼š
  pip install requests beautifulsoup4 urllib3
"""

import os, re, time, hmac, base64, hashlib, argparse
from dataclasses import dataclass
from typing import List, Tuple, Optional
from urllib.parse import urljoin, urlparse, urlencode, quote
from datetime import datetime, timedelta

try:
    from zoneinfo import ZoneInfo
except Exception:  # pragma: no cover
    from backports.zoneinfo import ZoneInfo

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

# ------------ å…¨å±€é…ç½® ------------
TZ = ZoneInfo("Asia/Shanghai")
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
      "AppleWebKit/537.36 (KHTML, like Gecko) "
      "Chrome/123.0.0.0 Safari/537.36")

# æ ç›®åœ°å€ï¼ˆå›ºå®šï¼‰
MOHRSS_RSXW = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/buneiyaowen/rsxw/"
JOB_ZXSS    = "http://job.mohrss.gov.cn/zxss/index.jhtml"

# é’‰é’‰ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
DEFAULT_WEBHOOK = (
    "https://oapi.dingtalk.com/robot/send?"
    "access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
)
DEFAULT_SECRET = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"

def _first_env(*keys: str, default: str = "") -> str:
    for k in keys:
        v = os.getenv(k, "")
        if v and v.strip():
            return v.strip()
    return default

DINGTALK_WEBHOOK = _first_env("DINGTALK_WEBHOOK", "DINGTALK_BASE", "WEBHOOK", default=DEFAULT_WEBHOOK)
DINGTALK_SECRET  = _first_env("DINGTALK_SECRET",  "SECRET",        default=DEFAULT_SECRET)

# ------------ HTTP åŸºç¡€ ------------
def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": UA, "Accept-Language": "zh-CN,zh;q=0.9"})
    retries = Retry(
        total=3,
        backoff_factor=0.6,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "POST"]),
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    s.mount("https://", adapter)
    s.mount("http://", adapter)
    s.trust_env = False
    return s

def _sign_webhook(base_webhook: str, secret: str) -> str:
    if not base_webhook:
        return ""
    if not secret:
        return base_webhook
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = quote(base64.b64encode(hmac_code))
    sep = "&" if "?" in base_webhook else "?"
    return f"{base_webhook}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
    if not webhook:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    try:
        r = requests.post(webhook, json=payload, timeout=20)
        ok = (r.status_code == 200 and isinstance(r.json(), dict) and r.json().get("errcode") == 0)
        print("DingTalk resp:", r.status_code, r.text[:200])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

# ------------ æ—¶é—´/è§£æå·¥å…· ------------
DATE_PATS = [
    r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",
    r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})",
    r"(\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",
]

def parse_dt(text: str) -> Optional[datetime]:
    if not text:
        return None
    t = re.sub(r"\s+", " ", text.strip())
    for pat in DATE_PATS:
        m = re.search(pat, t)
        if not m:
            continue
        if len(m.groups()) == 5:
            y, mo, d, hh, mm = map(int, m.groups())
            return datetime(y, mo, d, hh, mm, tzinfo=TZ)
        if len(m.groups()) == 3:
            y, mo, d = map(int, m.groups())
            return datetime(y, mo, d, 12, 0, tzinfo=TZ)  # åªæœ‰æ—¥æœŸ â†’ å–ä¸­åˆ
        if len(m.groups()) == 4:
            mo, d, hh, mm = map(int, m.groups())
            y = datetime.now(TZ).year
            return datetime(y, mo, d, hh, mm, tzinfo=TZ)
    if re.search(r"(åˆšåˆš|åˆ†é’Ÿ|å°æ—¶å‰|ä»Šå¤©|ä»Šæ—¥)", t):
        return datetime.now(TZ)
    return None

def day_range(date_str: str) -> Tuple[datetime, datetime]:
    if date_str.lower() == "yesterday":
        base = datetime.now(TZ).date() - timedelta(days=1)
    else:
        base = datetime.strptime(date_str, "%Y-%m-%d").date()
    start = datetime(base.year, base.month, base.day, 0, 0, 0, tzinfo=TZ)
    end   = datetime(base.year, base.month, base.day, 23, 59, 59, tzinfo=TZ)
    return start, end

def auto_range(days_for_monday: int = 3) -> Tuple[datetime, datetime, str]:
    now = datetime.now(TZ)
    if now.weekday() == 0:  # å‘¨ä¸€
        end = datetime(now.year, now.month, now.day, 23, 59, 59, tzinfo=TZ) - timedelta(days=1)
        start = end - timedelta(days=days_for_monday - 1)
        title = f"è¿‘{days_for_monday}å¤©åˆè¾‘"
    else:
        start, end = day_range("yesterday")
        title = "æ˜¨æ—¥ä¸“è¾‘"
    return start, end, title

# ------------ æ•°æ®ç»“æ„ ------------
@dataclass
class Item:
    title: str
    url: str
    dt: Optional[datetime]
    content: str
    source: str

# ------------ ç«™ç‚¹ 1ï¼šäººç¤¾éƒ¨ äººç¤¾æ–°é—» ------------
class RsxwMohrss:
    BASE = "https://www.mohrss.gov.cn"
    LIST = MOHRSS_RSXW

    def __init__(self, session: requests.Session, delay: float = 1.0):
        self.session = session
        self.delay = delay

    def _fetch(self, url: str) -> str:
        r = self.session.get(url, timeout=20)
        r.encoding = r.apparent_encoding or "utf-8"
        time.sleep(self.delay)
        return r.text

    def _page_url(self, page: int) -> str:
        # äººç¤¾éƒ¨æ ç›®å¸¸è§åˆ†é¡µï¼šindex.htmlã€index_2.htmlã€index_3.html...
        if page <= 1:
            return self.LIST
        tail = "" if self.LIST.endswith("/") else "/"
        return urljoin(self.LIST, f"{tail}index_{page}.html")

    def parse_list(self, html: str) -> List[Item]:
        soup = BeautifulSoup(html, "html.parser")
        items: List[Item] = []

        # å¸¸è§ä¸¤ç±»ç»“æ„ï¼š
        # 1) <ul class="..."> <li><a ...>æ ‡é¢˜</a><span>æ—¥æœŸ</span></li>
        # 2) <table> åˆ—è¡¨ï¼šç¬¬ä¸€åˆ—æ ‡é¢˜ã€ç¬¬ä¸‰åˆ—æ—¥æœŸ
        # 3) å…œåº•ï¼šé¡µé¢æ‰€æœ‰ aï¼Œæ—è¾¹/åŒè¡Œæœæ—¥æœŸ
        # ä¼˜å…ˆ ul/li
        lis = soup.select("ul li")
        for li in lis:
            a = li.find("a")
            if not a or not a.get("href"): continue
            title = a.get_text(" ", strip=True)
            url = urljoin(self.BASE, a.get("href").strip())
            # æ—¥æœŸ
            dt_txt = ""
            for sel in ["span", "em", ".date", ".time"]:
                node = li.select_one(sel)
                if node:
                    dt_txt = node.get_text(" ", strip=True); break
            if not dt_txt:
                dt_txt = li.get_text(" ", strip=True)
            dt = parse_dt(dt_txt)
            # æ‘˜è¦
            content = ""
            p = li.find("p")
            if p: content = p.get_text(" ", strip=True)
            items.append(Item(title=title, url=url, dt=dt, content=content, source="äººç¤¾éƒ¨Â·äººç¤¾æ–°é—»"))
        if items:
            return items

        # å†è¯•è¡¨æ ¼
        rows = soup.select("table tr")
        for tr in rows:
            a = tr.find("a"); tds = tr.find_all("td")
            if not a or not a.get("href"): continue
            title = a.get_text(" ", strip=True)
            url = urljoin(self.BASE, a.get("href").strip())
            dt_txt = ""
            if len(tds) >= 3:
                dt_txt = tds[-1].get_text(" ", strip=True)
            elif len(tds) >= 1:
                dt_txt = tds[0].get_text(" ", strip=True)
            else:
                dt_txt = tr.get_text(" ", strip=True)
            dt = parse_dt(dt_txt)
            items.append(Item(title=title, url=url, dt=dt, content="", source="äººç¤¾éƒ¨Â·äººç¤¾æ–°é—»"))
        if items:
            return items

        # å…œåº•ï¼šå…¨é¡µ a
        for a in soup.select("a"):
            if not a.get("href"): continue
            title = a.get_text(" ", strip=True)
            if not title: continue
            url = urljoin(self.BASE, a.get("href").strip())
            txt = a.parent.get_text(" ", strip=True) if a.parent else title
            dt = parse_dt(txt)
            if dt:
                items.append(Item(title=title, url=url, dt=dt, content="", source="äººç¤¾éƒ¨Â·äººç¤¾æ–°é—»"))
        return items

    def run(self, pages: int) -> List[Item]:
        all_items: List[Item] = []
        for p in range(1, pages + 1):
            url = self._page_url(p)
            html = self._fetch(url)
            items = self.parse_list(html)
            if not items and p == 1:
                break
            all_items.extend(items)
        return all_items

# ------------ ç«™ç‚¹ 2ï¼šå…¬å…±æ‹›è˜ç½‘ èµ„è®¯åˆ—è¡¨ï¼ˆæ— å…³é”®è¯ï¼‰ ------------
class JobZxss:
    BASE = "http://job.mohrss.gov.cn"
    LIST = JOB_ZXSS

    def __init__(self, session: requests.Session, delay: float = 1.0):
        self.session = session
        self.delay = delay

    def _fetch(self, url: str) -> str:
        r = self.session.get(url, timeout=20)
        r.encoding = r.apparent_encoding or "utf-8"
        time.sleep(self.delay)
        return r.text

    def _page_url(self, page: int) -> str:
        # å¸¸è§åˆ†é¡µï¼š?pageNo=2 æˆ– ?page=2ï¼›é¦–é¡µæ— å‚æ•°
        if page <= 1:
            return self.LIST
        return self.LIST + ("&" if "?" in self.LIST else "?") + urlencode({"pageNo": page})

    def parse_list(self, html: str) -> List[Item]:
        soup = BeautifulSoup(html, "html.parser")
        items: List[Item] = []

        # ç»“æ„ä¸€ï¼šè¡¨æ ¼/åˆ—è¡¨ï¼Œåˆ—ï¼šæ ‡é¢˜ / åˆ†ç±» / æ—¶é—´
        rows = soup.select("table tr, .list li, ul.list li, ul li")
        for node in rows:
            a = node if getattr(node, "name", None) == "a" else node.find("a")
            if not a or not a.get("href"): continue
            title = a.get_text(" ", strip=True)
            url = urljoin(self.BASE, a.get("href").strip())
            # è¿‡æ»¤éæœ¬ç«™
            host = urlparse(url).netloc.lower()
            if not host.endswith("mohrss.gov.cn"): continue
            # æ—¶é—´
            dt_txt = ""
            for sel in ["td:last-child", "em", "span", ".date", ".time"]:
                sub = node.select_one(sel)
                if sub:
                    dt_txt = sub.get_text(" ", strip=True); break
            if not dt_txt:
                dt_txt = node.get_text(" ", strip=True)
            dt = parse_dt(dt_txt)
            items.append(Item(title=title, url=url, dt=dt, content="", source="å…¬å…±æ‹›è˜ç½‘Â·èµ„è®¯"))
        if items:
            return items

        # å…œåº•ï¼šå…¨é¡µ a + é‚»è¿‘æ—¥æœŸ
        for a in soup.select("a"):
            if not a.get("href"): continue
            title = a.get_text(" ", strip=True)
            if not title: continue
            url = urljoin(self.BASE, a.get("href").strip())
            host = urlparse(url).netloc.lower()
            if not host.endswith("mohrss.gov.cn"): continue
            txt = (a.parent.get_text(" ", strip=True) if a.parent else title)
            dt = parse_dt(txt)
            if dt:
                items.append(Item(title=title, url=url, dt=dt, content="", source="å…¬å…±æ‹›è˜ç½‘Â·èµ„è®¯"))
        return items

    def run(self, pages: int) -> List[Item]:
        all_items: List[Item] = []
        for p in range(1, pages + 1):
            url = self._page_url(p)
            html = self._fetch(url)
            items = self.parse_list(html)
            if not items and p == 1:
                break
            all_items.extend(items)
        return all_items

# ------------ æ±‡æ€»/è¾“å‡º ------------
def dedup_by_url(items: List[Item]) -> List[Item]:
    seen = set(); out: List[Item] = []
    for it in items:
        if it.url and it.url not in seen:
            seen.add(it.url)
            out.append(it)
    return out

def filter_by_range(items: List[Item], start: datetime, end: datetime) -> List[Item]:
    return [it for it in items if it.dt and start <= it.dt <= end]

def build_markdown(items: List[Item], title_prefix: str) -> str:
    now_dt = datetime.now(TZ)
    wd = ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][now_dt.weekday()]
    lines = [
        f"**æ—¥æœŸï¼š{now_dt.strftime('%Y-%m-%d')}ï¼ˆ{wd}ï¼‰**",
        "",
        f"**æ ‡é¢˜ï¼š{title_prefix}ï½œäººç¤¾éƒ¨ & å…¬å…±æ‹›è˜ç½‘ï¼ˆå›ºå®šæ ç›®ï¼‰**",
        "",
        "**ä¸»è¦å†…å®¹**",
    ]
    if not items:
        lines.append("> æš‚æ— æ›´æ–°ã€‚")
        return "\n".join(lines)
    for i, it in enumerate(items, 1):
        dt_str = it.dt.strftime("%Y-%m-%d %H:%M") if it.dt else ""
        line = f"{i}. [{it.title}]({it.url})ã€€â€”ã€€*{it.source}*"
        if dt_str:
            line += f"ã€€`{dt_str}`"
        lines.append(line)
        if it.content:
            snippet = re.sub(r"\s+", " ", it.content).strip()[:120]
            lines.append(f"> {snippet}")
        lines.append("")
    return "\n".join(lines)

# ------------ ä¸»æµç¨‹ ------------
def main():
    ap = argparse.ArgumentParser(description="äººç¤¾éƒ¨äººç¤¾æ–°é—» + å…¬å…±æ‹›è˜ç½‘èµ„è®¯ï¼ˆå›ºå®šæ ç›®ï¼Œæ— å…³é”®è¯ï¼‰â†’ é’‰é’‰æ¨é€")
    ap.add_argument("--pages", type=int, default=int(os.getenv("PAGES", "2")), help="æ¯ç«™ç¿»é¡µæ•°ï¼ˆé»˜è®¤2ï¼‰")
    ap.add_argument("--delay", type=float, default=float(os.getenv("DELAY", "0.8")), help="è¯·æ±‚é—´éš”ç§’ï¼ˆé»˜è®¤0.8ï¼‰")
    ap.add_argument("--limit", type=int, default=int(os.getenv("LIMIT", "50")), help="å±•ç¤ºä¸Šé™ï¼ˆé»˜è®¤50ï¼‰")
    ap.add_argument("--date", default=os.getenv("DATE", ""), help="æŒ‡å®šæ—¥æœŸï¼ˆyesterday/2025-09-17ï¼‰ï¼Œä¸ºç©ºå¯ç”¨ --auto-range")
    ap.add_argument("--auto-range", default=os.getenv("AUTO_RANGE", "true").lower()=="true",
                    action="store_true", help="å¯ç”¨è‡ªåŠ¨èŒƒå›´ï¼ˆé»˜è®¤å¼€ï¼‰")
    ap.add_argument("--days-for-monday", type=int, default=int(os.getenv("DAYS_FOR_MONDAY", "3")),
                    help="å‘¨ä¸€åˆå¹¶å¤©æ•°ï¼ˆé»˜è®¤3ï¼‰")
    ap.add_argument("--window-hours", type=int, default=int(os.getenv("WINDOW_HOURS", "48")),
                    help="å½“ä¸å¯ç”¨è‡ªåŠ¨èŒƒå›´æ—¶ä½¿ç”¨çš„æ»šåŠ¨çª—å£å°æ—¶æ•°ï¼ˆé»˜è®¤48ï¼‰")
    ap.add_argument("--no-push", action="store_true", help="åªæ‰“å°ä¸æ¨é€é’‰é’‰")
    args = ap.parse_args()

    session = make_session()

    a = RsxwMohrss(session, delay=args.delay).run(args.pages)
    b = JobZxss(session, delay=args.delay).run(args.pages)

    all_items = dedup_by_url(a + b)

    # æ—¶é—´èŒƒå›´
    title_prefix = "æ—©å®‰èµ„è®¯"
    if args.date:
        start, end = day_range(args.date)
        title_prefix = f"{args.date} ä¸“é¢˜"
        all_items = filter_by_range(all_items, start, end)
    elif args.auto_range:
        start, end, tp = auto_range(args.days_for_monday)
        title_prefix = tp
        all_items = filter_by_range(all_items, start, end)
    else:
        # å…œåº•ï¼šæ»šåŠ¨çª—å£
        now = datetime.now(TZ)
        start = now - timedelta(hours=args.window_hours)
        end = now
        all_items = filter_by_range(all_items, start, end)

    # æ’åº + æˆªæ–­
    all_items.sort(key=lambda x: x.dt or datetime(1970,1,1,tzinfo=TZ), reverse=True)
    show = all_items[:args.limit] if args.limit and args.limit > 0 else all_items

    print(f"âœ… åŸå§‹æŠ“å– {len(a)+len(b)} æ¡ï¼›å»é‡å {len(all_items)} æ¡ï¼›å±•ç¤º {len(show)} æ¡ã€‚")
    md = build_markdown(show, title_prefix)
    print("\n--- Markdown Preview ---\n")
    print(md)

    if not args.no_push:
        ok = send_dingtalk_markdown(f"{title_prefix}ï½œäººç¤¾éƒ¨&å…¬å…±æ‹›è˜ç½‘ï¼ˆå›ºå®šæ ç›®ï¼‰", md)
        print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥/æœªæ¨é€ âŒ")

if __name__ == "__main__":
    main()
