# -*- coding: utf-8 -*-
"""
æ–°æµªè´¢ç» - ä¸Šå¸‚å…¬å¸ç ”ç©¶é™¢
æŠ“å–ã€å‰ä¸€å¤©ã€‘æ–°é—»æ ‡é¢˜ + é“¾æ¥ï¼Œå¹¶é€šè¿‡ã€é’‰é’‰æœºå™¨äººã€‘è‡ªåŠ¨æ¨é€åˆ°ç¾¤é‡Œï¼ˆMarkdownï¼‰

é¡µé¢ï¼šhttps://finance.sina.com.cn/roll/c/221431.shtml

GitHub Secretsï¼ˆä½ ç°åœ¨å·²æœ‰çš„ï¼‰ï¼š
- SHIYANQUNWEBHOOK : å¯ä»¥æ˜¯ã€æ•´æ¡ webhook URLã€‘æˆ–ã€ä»… access_tokenã€‘
- SHIYANQUNSECRET  : åŠ ç­¾ secret

ç¯å¢ƒå˜é‡ï¼ˆç”± yml æ³¨å…¥ï¼‰ï¼š
- DINGTALK_TOKEN   : webhook æˆ– tokenï¼ˆäºŒè€…éƒ½æ”¯æŒï¼‰
- DINGTALK_SECRET  : åŠ ç­¾å¯†é’¥
"""

import os
import re
import time
import hmac
import base64
import hashlib
import urllib.parse
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta

try:
    from zoneinfo import ZoneInfo
except Exception:
    from backports.zoneinfo import ZoneInfo


START_URL = "https://finance.sina.com.cn/roll/c/221431.shtml"
MAX_PAGES = int(os.getenv("MAX_PAGES", "5"))
SLEEP_SEC = float(os.getenv("SLEEP_SEC", "0.8"))
OUT_FILE = os.getenv("OUT_FILE", "sina_yesterday.md")

TZ = ZoneInfo("Asia/Shanghai")
DATE_RE = re.compile(r"\((\d{2})æœˆ(\d{2})æ—¥\s*(\d{2}):(\d{2})\)")


def now_cn():
    return datetime.now(TZ)


def get_html(url: str) -> str:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (X11; Linux x86_64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "zh-CN,zh;q=0.9",
    }
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    if not r.encoding or r.encoding.lower() == "iso-8859-1":
        r.encoding = r.apparent_encoding
    return r.text


def parse_datetime(text: str):
    m = DATE_RE.search(text)
    if not m:
        return None

    month, day, hh, mm = map(int, m.groups())
    now = now_cn()
    year = now.year
    if now.month == 1 and month == 12:
        year -= 1

    try:
        return datetime(year, month, day, hh, mm, tzinfo=TZ)
    except Exception:
        return None


def find_next_page(soup: BeautifulSoup):
    a = soup.find("a", string=lambda s: s and "ä¸‹ä¸€é¡µ" in s)
    if a and a.get("href"):
        return urljoin(START_URL, a["href"])
    return None


def extract_access_token(token_or_webhook: str) -> str:
    s = (token_or_webhook or "").strip()
    if not s:
        return ""
    if "access_token=" in s:
        try:
            if s.startswith("http"):
                u = urllib.parse.urlparse(s)
                q = urllib.parse.parse_qs(u.query)
                return (q.get("access_token") or [""])[0].strip()
            else:
                part = s.split("access_token=", 1)[1]
                return part.split("&", 1)[0].strip()
        except Exception:
            return ""
    return s


def dingtalk_signed_url(access_token: str, secret: str) -> str:
    timestamp = str(int(time.time() * 1000))
    string_to_sign = f"{timestamp}\n{secret}"
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    return f"https://oapi.dingtalk.com/robot/send?access_token={access_token}&timestamp={timestamp}&sign={sign}"


def dingtalk_send_markdown(title: str, markdown_text: str) -> dict:
    raw = (os.getenv("DINGTALK_TOKEN") or "").strip()
    secret = (os.getenv("DINGTALK_SECRET") or "").strip()
    access_token = extract_access_token(raw)

    if not access_token:
        raise RuntimeError("ç¼ºå°‘ DINGTALK_TOKENï¼ˆå¯å¡«æ•´æ¡ webhook æˆ– access_tokenï¼‰")
    if not secret:
        raise RuntimeError("ç¼ºå°‘ DINGTALK_SECRETï¼ˆè¯·ç¡®è®¤æœºå™¨äººå·²å¼€å¯â€œåŠ ç­¾â€å¹¶å¡«å…¥ secretï¼‰")
    if len(access_token) < 10:
        raise RuntimeError(f"DINGTALK_TOKEN è§£æåå¤ªçŸ­ï¼Œç–‘ä¼¼é…ç½®é”™è¯¯ï¼ˆlen={len(access_token)}ï¼‰")

    url = dingtalk_signed_url(access_token, secret)
    payload = {
        "msgtype": "markdown",
        "markdown": {"title": title, "text": markdown_text}
    }

    r = requests.post(url, json=payload, timeout=15)
    r.raise_for_status()
    data = r.json()

    if str(data.get("errcode")) != "0":
        if str(data.get("errcode")) == "300005":
            raise RuntimeError(
                f"é’‰é’‰å‘é€å¤±è´¥ï¼š{data}ã€‚é€šå¸¸æ˜¯ access_token ä¸å¯¹ï¼š"
                f"è¯·ç¡®è®¤ SHIYANQUNWEBHOOK å­˜çš„æ˜¯ã€åŒä¸€ä¸ªæœºå™¨äººã€‘çš„ webhook/tokenï¼Œä¸”æ²¡æœ‰å¤šä½™ç©ºæ ¼ã€‚"
            )
        raise RuntimeError(f"é’‰é’‰å‘é€å¤±è´¥ï¼š{data}")
    return data


def build_markdown(yesterday_date, results):
    lines = [f"### ğŸ“° æ–°æµªè´¢ç» Â· æ˜¨æ—¥æ›´æ–°ï¼ˆ{yesterday_date}ï¼‰\n"]
    if not results:
        lines.append("ï¼ˆæ˜¨æ—¥æ— æ›´æ–°æˆ–é¡µé¢ç»“æ„å˜åŒ–ï¼‰")
    else:
        for dt, title, link in results:
            lines.append(f"- [{title}]({link})  `{dt.strftime('%H:%M')}`")
    lines.append(f"\n> ç”Ÿæˆæ—¶é—´ï¼š{now_cn().strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆAsia/Shanghaiï¼‰")
    return "\n".join(lines)


def main():
    yesterday = (now_cn() - timedelta(days=1)).date()
    results = []

    url = START_URL
    hit_yesterday = False

    for _ in range(1, MAX_PAGES + 1):
        html = get_html(url)
        soup = BeautifulSoup(html, "html.parser")

        container = soup.select_one("div.listBlk")
        if not container:
            print("âŒ æœªæ‰¾åˆ° listBlk å®¹å™¨ï¼Œé¡µé¢ç»“æ„å¯èƒ½å˜åŒ–")
            break

        lis = container.find_all("li")
        if not lis:
            print("âŒ listBlk ä¸‹æœªæ‰¾åˆ° liï¼Œé¡µé¢ç»“æ„å¯èƒ½å˜åŒ–")
            break

        for li in lis:
            a = li.find("a", href=True)
            if not a:
                continue
            title = a.get_text(strip=True)
            link = urljoin(START_URL, a["href"])
            text = li.get_text(" ", strip=True)

            dt = parse_datetime(text)
            if not dt:
                continue

            if dt.date() == yesterday:
                results.append((dt, title, link))
                hit_yesterday = True

        if hit_yesterday:
            dts = [parse_datetime(li.get_text(" ", strip=True)) for li in lis]
            dts = [d for d in dts if d]
            if dts and all(d.date() < yesterday for d in dts):
                break

        next_url = find_next_page(soup)
        if not next_url:
            break

        url = next_url
        time.sleep(SLEEP_SEC)

    results.sort(key=lambda x: x[0], reverse=True)

    md = build_markdown(yesterday, results)

    with open(OUT_FILE, "w", encoding="utf-8") as f:
        f.write(md + "\n")

    print(f"âœ… æŠ“å–å®Œæˆï¼Œå…± {len(results)} æ¡ï¼Œå·²å†™å…¥ {OUT_FILE}")

    title = f"æ–°æµªè´¢ç»æ˜¨æ—¥æ›´æ–° {yesterday}"
    resp = dingtalk_send_markdown(title=title, markdown_text=md)
    print(f"âœ… é’‰é’‰æ¨é€æˆåŠŸï¼š{resp}")


if __name__ == "__main__":
    main()
