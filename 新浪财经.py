# -*- coding: utf-8 -*-
"""
æ–°æµªè´¢ç» - ä¸Šå¸‚å…¬å¸ç ”ç©¶é™¢
æŠ“å–ã€å‰ä¸€å¤©ã€‘æ–°é—»æ ‡é¢˜ + é“¾æ¥ï¼Œå¹¶é€šè¿‡ã€é’‰é’‰æœºå™¨äººã€‘è‡ªåŠ¨æ¨é€åˆ°ç¾¤é‡Œï¼ˆMarkdownï¼‰

é¡µé¢ï¼šhttps://finance.sina.com.cn/roll/c/221431.shtml

ä¿®å¤ç‚¹ï¼š
- å¼ºå»é‡ï¼šæŒ‰ link å»é‡ + (title, time) å…œåº•
- é€‰â€œçœŸå®æ­£æ–‡é“¾æ¥â€ï¼šä¼˜å…ˆ .shtml æˆ– /doc- è¿™ç±»è¯¦æƒ…é¡µ
- é˜²æ­¢åŒé¡µé‡å¤ a å¯¼è‡´åˆ·å±
"""

import os
import re
import time
import hmac
import base64
import hashlib
import urllib.parse
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta

try:
    from zoneinfo import ZoneInfo
except Exception:
    from backports.zoneinfo import ZoneInfo


START_URL = "https://finance.sina.com.cn/roll/c/221431.shtml"
MAX_PAGES = int(os.getenv("MAX_PAGES", "5"))
SLEEP_SEC = float(os.getenv("SLEEP_SEC", "0.8"))
OUT_FILE = os.getenv("OUT_FILE", "sina_yesterday.md")

TZ = ZoneInfo("Asia/Shanghai")
DATE_RE = re.compile(r"\((\d{2})æœˆ(\d{2})æ—¥\s*(\d{2}):(\d{2})\)")


def now_cn():
    return datetime.now(TZ)


def get_html(url: str) -> str:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (X11; Linux x86_64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "zh-CN,zh;q=0.9",
    }
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    if not r.encoding or r.encoding.lower() == "iso-8859-1":
        r.encoding = r.apparent_encoding
    return r.text


def parse_datetime(text: str):
    m = DATE_RE.search(text)
    if not m:
        return None
    month, day, hh, mm = map(int, m.groups())
    now = now_cn()
    year = now.year
    if now.month == 1 and month == 12:
        year -= 1
    try:
        return datetime(year, month, day, hh, mm, tzinfo=TZ)
    except Exception:
        return None


def find_next_page(soup: BeautifulSoup):
    a = soup.find("a", string=lambda s: s and "ä¸‹ä¸€é¡µ" in s)
    if a and a.get("href"):
        return urljoin(START_URL, a["href"])
    return None


# ===== é’‰é’‰ =====
def extract_access_token(token_or_webhook: str) -> str:
    s = (token_or_webhook or "").strip()
    if not s:
        return ""
    if "access_token=" in s:
        try:
            if s.startswith("http"):
                u = urllib.parse.urlparse(s)
                q = urllib.parse.parse_qs(u.query)
                return (q.get("access_token") or [""])[0].strip()
            part = s.split("access_token=", 1)[1]
            return part.split("&", 1)[0].strip()
        except Exception:
            return ""
    return s


def dingtalk_signed_url(access_token: str, secret: str) -> str:
    timestamp = str(int(time.time() * 1000))
    string_to_sign = f"{timestamp}\n{secret}"
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    return f"https://oapi.dingtalk.com/robot/send?access_token={access_token}&timestamp={timestamp}&sign={sign}"


def dingtalk_send_markdown(title: str, markdown_text: str) -> dict:
    raw = (os.getenv("DINGTALK_TOKEN") or "").strip()
    secret = (os.getenv("DINGTALK_SECRET") or "").strip()
    access_token = extract_access_token(raw)

    if not access_token:
        raise RuntimeError("ç¼ºå°‘ DINGTALK_TOKENï¼ˆå¯å¡«æ•´æ¡ webhook æˆ– access_tokenï¼‰")
    if not secret:
        raise RuntimeError("ç¼ºå°‘ DINGTALK_SECRETï¼ˆè¯·ç¡®è®¤æœºå™¨äººå·²å¼€å¯â€œåŠ ç­¾â€å¹¶å¡«å…¥ secretï¼‰")
    if len(access_token) < 10:
        raise RuntimeError(f"DINGTALK_TOKEN è§£æåå¤ªçŸ­ï¼Œç–‘ä¼¼é…ç½®é”™è¯¯ï¼ˆlen={len(access_token)}ï¼‰")

    url = dingtalk_signed_url(access_token, secret)
    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": markdown_text}}

    r = requests.post(url, json=payload, timeout=15)
    r.raise_for_status()
    data = r.json()
    if str(data.get("errcode")) != "0":
        raise RuntimeError(f"é’‰é’‰å‘é€å¤±è´¥ï¼š{data}")
    return data


# ===== å…³é”®ï¼šä»ä¸€ä¸ª li é‡Œé€‰â€œçœŸå®æ­£æ–‡é“¾æ¥â€ =====
def pick_best_link(li: BeautifulSoup):
    """
    li é‡Œå¯èƒ½æœ‰å¤šä¸ª <a>ï¼Œæˆ‘ä»¬æŒ‘æœ€åƒæ­£æ–‡é¡µçš„ï¼š
    1) ä¼˜å…ˆ href åŒ…å« '.shtml' æˆ– '/doc-' æˆ– '/article/'
    2) å†é€€å›ç¬¬ä¸€ä¸ª href
    """
    links = []
    for a in li.find_all("a", href=True):
        href = a["href"].strip()
        text = a.get_text(strip=True)
        if not href:
            continue
        abs_url = urljoin(START_URL, href)
        links.append((abs_url, text))

    if not links:
        return None, None

    def score(u: str):
        s = 0
        if ".shtml" in u:
            s += 10
        if "/doc-" in u:
            s += 8
        if "/article/" in u:
            s += 6
        if "finance.sina.com.cn" in u:
            s += 2
        return s

    links.sort(key=lambda x: score(x[0]), reverse=True)
    return links[0][0], links[0][1]


def build_markdown(yesterday_date, results):
    lines = [f"### ğŸ“° æ–°æµªè´¢ç» Â· æ˜¨æ—¥æ›´æ–°ï¼ˆ{yesterday_date}ï¼‰\n"]
    if not results:
        lines.append("ï¼ˆæ˜¨æ—¥æ— æ›´æ–°æˆ–é¡µé¢ç»“æ„å˜åŒ–ï¼‰")
    else:
        for dt, title, link in results:
            lines.append(f"- [{title}]({link})  `{dt.strftime('%H:%M')}`")
    lines.append(f"\n> ç”Ÿæˆæ—¶é—´ï¼š{now_cn().strftime('%Y-%m-%d %H:%M:%S')}ï¼ˆAsia/Shanghaiï¼‰")
    return "\n".join(lines)


def main():
    yesterday = (now_cn() - timedelta(days=1)).date()

    # å¼ºå»é‡å®¹å™¨
    seen_link = set()
    seen_title_time = set()

    results = []
    url = START_URL
    hit_yesterday = False

    for _ in range(1, MAX_PAGES + 1):
        html = get_html(url)
        soup = BeautifulSoup(html, "html.parser")

        container = soup.select_one("div.listBlk")
        if not container:
            print("âŒ æœªæ‰¾åˆ° listBlk å®¹å™¨ï¼Œé¡µé¢ç»“æ„å¯èƒ½å˜åŒ–")
            break

        lis = container.find_all("li")
        if not lis:
            print("âŒ listBlk ä¸‹æœªæ‰¾åˆ° li")
            break

        page_links = []

        for li in lis:
            text_all = li.get_text(" ", strip=True)
            dt = parse_datetime(text_all)
            if not dt:
                continue

            # åªè¦æ˜¨å¤©
            if dt.date() != yesterday:
                continue

            link, _anchor_text = pick_best_link(li)
            if not link:
                continue

            # æ ‡é¢˜ï¼šä¼˜å…ˆç”¨ li é‡Œç¬¬ä¸€ä¸ª a çš„æ–‡å­—ï¼›å¦‚æœä¸ºç©ºï¼Œç”¨ anchor_text
            a0 = li.find("a")
            title = (a0.get_text(strip=True) if a0 else "") or (_anchor_text or "")
            title = title.strip()
            if not title:
                continue

            # å»é‡ key
            k_link = link
            k_tt = (title, dt.strftime("%Y-%m-%d %H:%M"))

            # å¼ºå»é‡ï¼šå…ˆæŒ‰ link
            if k_link in seen_link:
                continue
            # å…œåº•å»é‡ï¼šåŒæ ‡é¢˜åŒæ—¶é—´
            if k_tt in seen_title_time:
                continue

            seen_link.add(k_link)
            seen_title_time.add(k_tt)

            results.append((dt, title, link))
            page_links.append(link)
            hit_yesterday = True

        # å¦‚æœè¿™ä¸€é¡µæŠ“åˆ°çš„ link å…¨ä¸€æ ·ï¼Œç›´æ¥æç¤ºï¼ˆé¿å…åˆ·å±ï¼‰
        if page_links and len(set(page_links)) == 1 and len(page_links) >= 2:
            print("âš ï¸ è­¦å‘Šï¼šæœ¬é¡µæŠ“åˆ°çš„é“¾æ¥å…¨éƒ¨ç›¸åŒï¼Œå·²é€šè¿‡å»é‡è¿‡æ»¤ï¼›å»ºè®®åç»­ç»§ç»­è§‚å¯Ÿé¡µé¢ç»“æ„ã€‚")

        # æ—©åœï¼šå·²ç»å‘½ä¸­æ˜¨å¤©ï¼Œå¹¶ä¸”æœ¬é¡µæ‰€æœ‰æ—¶é—´éƒ½ < yesterday -> åœ
        if hit_yesterday:
            dts = [parse_datetime(li.get_text(" ", strip=True)) for li in lis]
            dts = [d for d in dts if d]
            if dts and all(d.date() < yesterday for d in dts):
                break

        next_url = find_next_page(soup)
        if not next_url:
            break
        url = next_url
        time.sleep(SLEEP_SEC)

    # å€’åºæ’åˆ—
    results.sort(key=lambda x: x[0], reverse=True)

    md = build_markdown(yesterday, results)

    with open(OUT_FILE, "w", encoding="utf-8") as f:
        f.write(md + "\n")

    print(f"âœ… æŠ“å–å®Œæˆï¼ˆå»é‡åï¼‰ï¼Œå…± {len(results)} æ¡ï¼Œå·²å†™å…¥ {OUT_FILE}")

    title = f"æ–°æµªè´¢ç»æ˜¨æ—¥æ›´æ–° {yesterday}"
    resp = dingtalk_send_markdown(title=title, markdown_text=md)
    print(f"âœ… é’‰é’‰æ¨é€æˆåŠŸï¼š{resp}")


if __name__ == "__main__":
    main()
