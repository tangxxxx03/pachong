# -*- coding: utf-8 -*-
"""
æ¯æ—¥æ—©æŠ¥ï¼ˆé’‰é’‰å‹å¥½ç‰ˆï¼‰
- ğŸ‘¥ äººåŠ›èµ„è®¯ï¼šHRLoo ä¸‰èŒ…æ—¥æŠ¥è¦ç‚¹ï¼ˆæŠ“å½“å¤©ï¼›ä¿ç•™â€œæŸ¥çœ‹è¯¦ç»†â€å¯ç‚¹å‡»ï¼‰
- ğŸ¢ ä¼ä¸šæ–°é—»ï¼šæ–°æµªè´¢ç» ä¸Šå¸‚å…¬å¸ç ”ç©¶é™¢ï¼ˆå‘¨ä¸€æŠ“ä¸Šå‘¨äº”ï¼›å…¶ä»–å·¥ä½œæ—¥æŠ“æ˜¨å¤©ï¼›æ ‡é¢˜+å¯ç‚¹å‡»é“¾æ¥ï¼‰

å±•ç¤ºè¦æ±‚ï¼ˆä½ å¼ºè°ƒçš„ï¼‰ï¼š
1) ä¸æ˜¾ç¤ºâ€œæŠ“å–æ—¥æœŸâ€
2) ä¸æ˜¾ç¤ºâ€œAIæœ€å‰æ²¿â€ç­‰æ ç›®æ ‡é¢˜ï¼ˆåªè¦ numbered è¦ç‚¹ï¼‰
3) é“¾æ¥å¿…é¡»å¯ç‚¹å‡»ï¼šæ¯æ¡æ–°é—»ç”¨â€œä¸¤è¡Œå†™æ³•â€ï¼ˆé’‰é’‰æœ€ç¨³ï¼‰

ç¯å¢ƒå˜é‡ï¼ˆGitHub Actions / Secretsï¼‰ï¼š
- DINGTALK_TOKEN   ï¼šå¯å¡«æ•´æ¡ webhook æˆ– access_token
- DINGTALK_SECRET  ï¼šæœºå™¨äººåŠ ç­¾ secretï¼ˆå¿…é¡»å¼€å¯åŠ ç­¾ï¼‰

å¯é€‰ç¯å¢ƒå˜é‡ï¼š
- RUN_HRLOO=1/0
- RUN_SINA=1/0
- OUT_FILE=daily_report.md

- HR_TARGET_DATE=YYYY-MM-DDï¼ˆé»˜è®¤å½“å¤©ï¼›ä½ è¯´ä¸‰èŒ…æŠ“å½“å¤©ï¼‰
- SRC_HRLOO_URLS=...ï¼ˆé»˜è®¤ hrloo é¦–é¡µ+é¢‘é“ï¼‰

- SINA_TARGET_DATE=YYYY-MM-DDï¼ˆå¯è¦†ç›–ä¼ä¸šæ–°é—»æŠ“å–æ—¥ï¼‰
- SINA_MAX_PAGES=5
- SINA_SLEEP_SEC=0.8
- SINA_MAX_ITEMS=15
"""

import os
import re
import time
import ssl
import hmac
import base64
import hashlib
import urllib.parse
import requests
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin
from datetime import datetime, timedelta, date
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

try:
    from zoneinfo import ZoneInfo
except Exception:
    from backports.zoneinfo import ZoneInfo

TZ = ZoneInfo("Asia/Shanghai")


# ===================== é€šç”¨ =====================
def now_cn() -> datetime:
    return datetime.now(TZ)

def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

def truncate_text(s: str, max_len: int = 60) -> str:
    """é’‰é’‰ä¸€è¡Œå¤ªé•¿å®¹æ˜“è¢«æˆªæ–­/ç‚¹ä¸å¼€ï¼Œä¸»åŠ¨æˆªçŸ­æ›´ç¨³ã€‚"""
    s = norm(s)
    if len(s) <= max_len:
        return s
    return s[: max_len - 1] + "â€¦"

def parse_ymd(s: str):
    s = (s or "").strip()
    if not s:
        return None
    try:
        y, m, d = map(int, re.split(r"[-/\.]", s))
        return date(y, m, d)
    except Exception:
        return None

def target_date_sina(today: date) -> date:
    """
    ä½ çš„è§„åˆ™ï¼šæ–°æµªè´¢ç»
    - å‘¨ä¸€ï¼šæŠ“ä¸Šå‘¨äº”ï¼ˆtoday - 3ï¼‰
    - å…¶ä»–å·¥ä½œæ—¥ï¼šæŠ“æ˜¨å¤©ï¼ˆtoday - 1ï¼‰
    è¯´æ˜ï¼šå·¥ä½œæµåªåœ¨å‘¨ä¸€åˆ°å‘¨äº”è¿è¡Œï¼Œæ‰€ä»¥ä¸éœ€è¦è€ƒè™‘å‘¨æœ«è¿è¡Œçš„æƒ…å†µã€‚
    """
    if today.weekday() == 0:  # å‘¨ä¸€
        return today - timedelta(days=3)
    return today - timedelta(days=1)


# ===================== é’‰é’‰ï¼ˆåŠ ç­¾ï¼‰ =====================
def extract_access_token(token_or_webhook: str) -> str:
    s = (token_or_webhook or "").strip()
    if not s:
        return ""
    if "access_token=" in s:
        u = urllib.parse.urlparse(s)
        q = urllib.parse.parse_qs(u.query)
        return (q.get("access_token") or [""])[0].strip()
    return s

def dingtalk_signed_url(access_token: str, secret: str) -> str:
    ts = str(int(time.time() * 1000))
    to_sign = f"{ts}\n{secret}"
    sign = urllib.parse.quote_plus(
        base64.b64encode(
            hmac.new(secret.encode("utf-8"), to_sign.encode("utf-8"), hashlib.sha256).digest()
        )
    )
    return f"https://oapi.dingtalk.com/robot/send?access_token={access_token}&timestamp={ts}&sign={sign}"

def dingtalk_send_markdown(title: str, markdown_text: str) -> dict:
    raw = (os.getenv("DINGTALK_TOKEN") or "").strip()
    secret = (os.getenv("DINGTALK_SECRET") or "").strip()
    token = extract_access_token(raw)

    if not token:
        raise RuntimeError("ç¼ºå°‘ DINGTALK_TOKENï¼ˆå¯å¡«æ•´æ¡ webhook æˆ– access_tokenï¼‰")
    if not secret:
        raise RuntimeError("ç¼ºå°‘ DINGTALK_SECRETï¼ˆè¯·ç¡®è®¤æœºå™¨äººå·²å¼€å¯â€œåŠ ç­¾â€ï¼‰")

    url = dingtalk_signed_url(token, secret)
    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": markdown_text}}
    r = requests.post(url, json=payload, timeout=20)
    r.raise_for_status()
    data = r.json()
    if str(data.get("errcode")) != "0":
        raise RuntimeError(f"é’‰é’‰å‘é€å¤±è´¥ï¼š{data}")
    return data


# ===================== ä¼ä¸šæ–°é—»ï¼šæ–°æµªè´¢ç» =====================
SINA_START_URL = "https://finance.sina.com.cn/roll/c/221431.shtml"
SINA_MAX_PAGES = int(os.getenv("SINA_MAX_PAGES", "5"))
SINA_SLEEP_SEC = float(os.getenv("SINA_SLEEP_SEC", "0.8"))
SINA_MAX_ITEMS = int(os.getenv("SINA_MAX_ITEMS", "15"))
SINA_DATE_RE = re.compile(r"\((\d{2})æœˆ(\d{2})æ—¥\s*(\d{2}):(\d{2})\)")

def sina_get_html(url: str) -> str:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (X11; Linux x86_64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "zh-CN,zh;q=0.9",
    }
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    if not r.encoding or r.encoding.lower() == "iso-8859-1":
        r.encoding = r.apparent_encoding
    return r.text

def sina_parse_datetime(text: str):
    m = SINA_DATE_RE.search(text or "")
    if not m:
        return None
    month, day, hh, mm = map(int, m.groups())
    now = now_cn()
    year = now.year
    if now.month == 1 and month == 12:
        year -= 1
    try:
        return datetime(year, month, day, hh, mm, tzinfo=TZ)
    except Exception:
        return None

def sina_find_next_page(soup: BeautifulSoup):
    a = soup.find("a", string=lambda s: s and "ä¸‹ä¸€é¡µ" in s)
    if a and a.get("href"):
        return urljoin(SINA_START_URL, a["href"])
    return None

def sina_pick_best_link(li: Tag):
    """
    li é‡Œå¯èƒ½å¤šä¸ª <a>ï¼Œä¼˜å…ˆé€‰æœ€åƒæ­£æ–‡é¡µçš„é“¾æ¥ï¼š
    - .shtml æˆ– /doc- æˆ– /article/
    """
    links = []
    for a in li.find_all("a", href=True):
        href = (a.get("href") or "").strip()
        if not href:
            continue
        abs_url = urljoin(SINA_START_URL, href)
        text = a.get_text(strip=True)
        links.append((abs_url, text))
    if not links:
        return None, None

    def score(u: str):
        s = 0
        if ".shtml" in u: s += 10
        if "/doc-" in u: s += 8
        if "/article/" in u: s += 6
        if "finance.sina.com.cn" in u: s += 2
        return s

    links.sort(key=lambda x: score(x[0]), reverse=True)
    return links[0][0], links[0][1]

def crawl_sina_target_day():
    # å…è®¸ç¯å¢ƒå˜é‡è¦†ç›–
    override = parse_ymd(os.getenv("SINA_TARGET_DATE"))
    today = now_cn().date()
    target = override or target_date_sina(today)

    seen_link = set()
    seen_tt = set()
    results = []

    url = SINA_START_URL
    hit = False

    for _ in range(1, SINA_MAX_PAGES + 1):
        html = sina_get_html(url)
        soup = BeautifulSoup(html, "html.parser")

        container = soup.select_one("div.listBlk")
        if not container:
            break
        lis = container.find_all("li")
        if not lis:
            break

        for li in lis:
            text_all = li.get_text(" ", strip=True)
            dt = sina_parse_datetime(text_all)
            if not dt or dt.date() != target:
                continue

            link, anchor_text = sina_pick_best_link(li)
            if not link:
                continue

            a0 = li.find("a")
            title = (a0.get_text(strip=True) if a0 else "") or (anchor_text or "")
            title = norm(title)
            if not title:
                continue

            k1 = link
            k2 = (title, dt.strftime("%Y-%m-%d %H:%M"))
            if k1 in seen_link or k2 in seen_tt:
                continue

            seen_link.add(k1)
            seen_tt.add(k2)
            results.append((dt, title, link))
            hit = True

        # æ—©åœï¼šå·²ç»å‘½ä¸­ç›®æ ‡æ—¥ï¼Œä¸”æœ¬é¡µæ—¶é—´éƒ½æ—©äºç›®æ ‡æ—¥
        if hit:
            dts = [sina_parse_datetime(li.get_text(" ", strip=True)) for li in lis]
            dts = [d for d in dts if d]
            if dts and all(d.date() < target for d in dts):
                break

        next_url = sina_find_next_page(soup)
        if not next_url:
            break
        url = next_url
        time.sleep(SINA_SLEEP_SEC)

    results.sort(key=lambda x: x[0], reverse=True)
    return target, results[:SINA_MAX_ITEMS]

def md_enterprise_news(target_day: date, results):
    lines = []
    lines.append("## ğŸ¢ ä¼ä¸šæ–°é—»")

    if not results:
        lines.append("ï¼ˆæ— æ›´æ–°æˆ–é¡µé¢ç»“æ„å˜åŒ–ï¼‰")
        return "\n".join(lines)

    # é’‰é’‰ç¨³å®šå†™æ³•ï¼šæ¯æ¡ä¸¤è¡Œï¼ˆæ ‡é¢˜ä¸€è¡Œ + é“¾æ¥ä¸€è¡Œï¼‰
    for dt, title, link in results:
        short = truncate_text(title, 50)
        lines.append(f"- {short}")
        lines.append(f"  ğŸ‘‰ [æ‰“å¼€è¯¦æƒ…]({link})")

    return "\n".join(lines)


# ===================== äººåŠ›èµ„è®¯ï¼šHRLoo =====================
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9"
    })
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500, 502, 503, 504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

CN_TITLE_DATE = re.compile(r"[ï¼ˆ(]\s*(20\d{2})\s*[å¹´\-/.]\s*(\d{1,2})\s*[æœˆ\-/.]\s*(\d{1,2})\s*[)ï¼‰]")

def date_from_bracket_title(text: str):
    m = CN_TITLE_DATE.search(text or "")
    if not m:
        return None
    try:
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        return date(y, mo, d)
    except Exception:
        return None

def looks_like_numbered(text: str) -> bool:
    return bool(re.match(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*\S+", text or ""))

CIRCLED = "â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©"

def strip_leading_num(t: str) -> str:
    t = re.sub(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*", "", t)
    t = re.sub(r"^\s*[" + CIRCLED + r"]\s*", "", t)
    t = re.sub(r"^\s*[ï¼-ï¼™]+\s*[ã€.ï¼]\s*", "", t)
    return t.strip()

# è¿‡æ»¤æ‰æ ç›®æ ‡é¢˜ï¼ˆä½ æŒ‡å‡ºçš„â€œAIæœ€å‰æ²¿â€ç­‰ï¼‰
SECTION_BLACKLIST = {"AIæœ€å‰æ²¿", "çƒ­ç‚¹é€Ÿé€’", "è¡Œä¸šè§‚å¯Ÿ", "æœ€æ–°åŠ¨æ€"}

class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []

        # âœ… ä½ è¯´ä¸‰èŒ…æ—¥æŠ¥æŠ“å½“å¤©ï¼šé»˜è®¤ä»Šå¤©ï¼ˆä¹Ÿå…è®¸ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
        override = parse_ymd(os.getenv("HR_TARGET_DATE"))
        self.target_date = override or now_cn().date()

        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        self.sources = [u.strip() for u in os.getenv(
            "SRC_HRLOO_URLS",
            "https://www.hrloo.com/,https://www.hrloo.com/news/hr"
        ).split(",") if u.strip()]

    def crawl(self):
        for base in self.sources:
            if self._crawl_source(base):
                break

    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=20)
        except Exception:
            return False
        if r.status_code != 200:
            return False

        soup = BeautifulSoup(r.text, "html.parser")

        # é€šé“1ï¼šåˆ—è¡¨å®¹å™¨
        items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
        if items:
            for div in items:
                a = div.find("a", href=True)
                if not a:
                    continue
                title_text = norm(a.get_text())
                if not self.daily_title_pat.search(title_text):
                    continue

                t2 = date_from_bracket_title(title_text)
                if t2 and t2 != self.target_date:
                    continue

                abs_url = urljoin(base, a["href"])
                if self._try_detail(abs_url):
                    return True

        # é€šé“2ï¼šå…œåº•æ‰« links
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href", "")
            if not re.search(r"/news/\d+\.html$", href):
                continue
            text = norm(a.get_text())
            if not self.daily_title_pat.search(text):
                continue
            t2 = date_from_bracket_title(text)
            if t2 and t2 != self.target_date:
                continue
            links.append(urljoin(base, href))

        seen = set()
        for u in links:
            if u in seen:
                continue
            seen.add(u)
            if self._try_detail(u):
                return True
        return False

    def _try_detail(self, abs_url):
        pub_dt, titles, page_title = self._fetch_detail_clean(abs_url)
        if not page_title or not self.daily_title_pat.search(page_title):
            return False

        t3 = date_from_bracket_title(page_title)
        if t3 and t3 != self.target_date:
            return False
        if not titles:
            return False

        self.results.append({
            "title": page_title,
            "url": abs_url,
            "date": (pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else f"{self.target_date} 09:00"),
            "titles": titles
        })
        return True

    def _extract_pub_time(self, soup: BeautifulSoup):
        cand = []
        for t in soup.select("time[datetime]"):
            cand.append(t.get("datetime", ""))
        for m in soup.select("meta[property='article:published_time'],meta[name='pubdate'],meta[name='publishdate']"):
            cand.append(m.get("content", ""))
        for sel in [".time", ".date", ".pubtime", ".publish-time", ".post-time", ".info", "meta[itemprop='datePublished']"]:
            for x in soup.select(sel):
                if isinstance(x, Tag):
                    cand.append(x.get_text(" ", strip=True))

        pat = re.compile(r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?")
        def parse_one(s):
            m = pat.search(s or "")
            if not m:
                return None
            try:
                y, mo, d = int(m[1]), int(m[2]), int(m[3])
                hh = int(m[4]) if m[4] else 9
                mm = int(m[5]) if m[5] else 0
                return datetime(y, mo, d, hh, mm, tzinfo=TZ)
            except Exception:
                return None

        dts = [dt for dt in map(parse_one, cand) if dt]
        if dts:
            now = now_cn()
            past = [dt for dt in dts if dt <= now]
            return min(past or dts, key=lambda dt: abs((now - dt).total_seconds()))
        return None

    def _extract_h2_titles(self, root: Tag):
        """
        âœ… åªæå– numbered è¦ç‚¹ï¼Œå¹¶è¿‡æ»¤â€œAIæœ€å‰æ²¿â€ç­‰æ ç›®æ ‡é¢˜
        """
        out = []
        for h2 in root.select("h2.style-h2, h2[class*='style-h2']"):
            text = norm(h2.get_text())
            if not text:
                continue

            # å»ç¼–å·/å»æ‹¬å·
            text = strip_leading_num(text)
            text = re.split(r"[ï¼ˆ(]", text)[0].strip()
            if not text:
                continue

            # âŒ è¿‡æ»¤æ ç›®æ ‡é¢˜
            if text in SECTION_BLACKLIST:
                continue

            # âœ… åªä¿ç•™æ›´åƒâ€œè¦ç‚¹â€çš„å†…å®¹ï¼šè‡³å°‘4å­—
            if len(text) >= 4:
                out.append(text)

        seen, final = set(), []
        for t in out:
            if t not in seen:
                seen.add(t)
                final.append(t)
        return final

    def _extract_strong_titles(self, root: Tag):
        keep = []
        for st in root.select("strong"):
            text = norm(st.get_text())
            if not text or len(text) < 4:
                continue
            text = strip_leading_num(text)
            text = re.split(r"[ï¼ˆ(]", text)[0].strip()
            if text and text not in SECTION_BLACKLIST:
                keep.append(text)
        seen, out = set(), []
        for t in keep:
            if t not in seen:
                seen.add(t)
                out.append(t)
        return out

    def _extract_numbered_titles(self, root: Tag):
        out = []
        for p in root.find_all(["p", "h2", "h3", "div", "span", "li"]):
            text = norm(p.get_text())
            if looks_like_numbered(text):
                text = strip_leading_num(text)
                text = re.split(r"[ï¼ˆ(]", text)[0].strip()
                if text and len(text) >= 4 and text not in SECTION_BLACKLIST:
                    out.append(text)
        seen, final = set(), []
        for t in out:
            if t not in seen:
                seen.add(t)
                final.append(t)
        return final

    def _pick_container(self, soup: BeautifulSoup):
        selectors = [
            ".content-con.fn-wenda-detail-infomation",
            ".fn-wenda-detail-infomation",
            ".content-con.hr-rich-text.fn-wenda-detail-infomation",
            ".hr-rich-text.fn-wenda-detail-infomation",
            ".fn-hr-rich-text.custom-style-warp",
            ".custom-style-warp",
            ".content-wrap-con",
        ]
        for sel in selectors:
            node = soup.select_one(sel)
            if node:
                return node
        return soup

    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=(6, 20))
            if r.status_code != 200:
                return None, [], ""
            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            h1 = soup.find("h1")
            page_title = norm(h1.get_text()) if h1 else ""
            if not page_title:
                title_tag = soup.find(["h1", "h2"])
                page_title = norm(title_tag.get_text()) if title_tag else ""

            pub_dt = self._extract_pub_time(soup)
            container = self._pick_container(soup)

            for sel in [".other-wrap", ".txt", ".footer", ".bottom"]:
                for bad in container.select(sel):
                    bad.decompose()

            # âœ… ä¼˜å…ˆ h2ï¼ˆå¹¶è¿‡æ»¤æ ç›®ï¼‰
            titles = self._extract_h2_titles(container)

            # é€€å›ï¼šstrong
            if not titles:
                titles = self._extract_strong_titles(container)

            # å†é€€å›ï¼šç¼–å·æ®µè½
            if not titles:
                titles = self._extract_numbered_titles(container)

            return pub_dt, titles, page_title
        except Exception:
            return None, [], ""

def crawl_hrloo():
    c = HRLooCrawler()
    c.crawl()
    if not c.results:
        return None, []
    it = c.results[0]
    return it, it.get("titles", [])

def md_hr_info(item, titles):
    lines = []
    lines.append("## ğŸ‘¥ äººåŠ›èµ„è®¯")

    if not item or not titles:
        lines.append("ï¼ˆæœªå‘ç°å½“å¤©çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ï¼‰")
        return "\n".join(lines)

    for idx, t in enumerate(titles, 1):
        lines.append(f"{idx}. {truncate_text(t, 55)}")

    # âœ… â€œæŸ¥çœ‹è¯¦ç»†â€å•ç‹¬ä¸€è¡Œï¼Œé’‰é’‰ç¨³å®šå¯ç‚¹
    lines.append(f"\nğŸ‘‰ [æŸ¥çœ‹è¯¦ç»†]({item['url']})")
    return "\n".join(lines)


# ===================== æ±‡æ€» Markdownï¼ˆä½ è¦çš„æ ‡é¢˜é£æ ¼ï¼‰ =====================
def build_markdown(hr_block: str, enterprise_block: str):
    today_mmdd = now_cn().strftime("%m-%d")
    md = [f"## ğŸ“Œ {today_mmdd} æ¯æ—¥æ—©æŠ¥", ""]
    md.append(hr_block or "## ğŸ‘¥ äººåŠ›èµ„è®¯\nï¼ˆæœ¬æ¬¡æœªç”Ÿæˆï¼‰")
    md.append("\n---\n")
    md.append(enterprise_block or "## ğŸ¢ ä¼ä¸šæ–°é—»\nï¼ˆæœ¬æ¬¡æœªç”Ÿæˆï¼‰")
    return "\n".join(md).strip() + "\n"


def main():
    run_hrloo = (os.getenv("RUN_HRLOO", "1").strip() != "0")
    run_sina = (os.getenv("RUN_SINA", "1").strip() != "0")

    hr_block = ""
    enterprise_block = ""

    # ğŸ‘¥ ä¸‰èŒ…æ—¥æŠ¥ï¼šæŠ“å½“å¤©
    if run_hrloo:
        hr_item, hr_titles = crawl_hrloo()
        hr_block = md_hr_info(hr_item, hr_titles)

    # ğŸ¢ æ–°æµªè´¢ç»ï¼šå‘¨ä¸€æŠ“ä¸Šå‘¨äº”ï¼Œå…¶ä»–å·¥ä½œæ—¥æŠ“æ˜¨å¤©
    if run_sina:
        target_day, sina_items = crawl_sina_target_day()
        enterprise_block = md_enterprise_news(target_day, sina_items)

    md = build_markdown(hr_block, enterprise_block)

    out_file = os.getenv("OUT_FILE", "daily_report.md")
    with open(out_file, "w", encoding="utf-8") as f:
        f.write(md)

    title = f"{now_cn().strftime('%m-%d')} æ¯æ—¥æ—©æŠ¥"
    resp = dingtalk_send_markdown(title, md)
    print("âœ… DingTalk OK:", resp)
    print("âœ… wrote:", out_file)


if __name__ == "__main__":
    main()
