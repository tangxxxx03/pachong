# -*- coding: utf-8 -*-
"""
æ–°æµªè´¢ç» - ä¸Šå¸‚å…¬å¸ç ”ç©¶é™¢
æŠ“å–ã€å‰ä¸€å¤©ã€‘æ–°é—»æ ‡é¢˜ + é“¾æ¥ï¼ˆGitHub Actions ç¨³å®šç‰ˆï¼‰
é¡µé¢ï¼šhttps://finance.sina.com.cn/roll/c/221431.shtml
"""

import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta

try:
    from zoneinfo import ZoneInfo
except Exception:
    from backports.zoneinfo import ZoneInfo


# ================= é…ç½® =================
START_URL = "https://finance.sina.com.cn/roll/c/221431.shtml"
MAX_PAGES = 5
SLEEP_SEC = 0.8
OUT_FILE = "sina_yesterday.md"

TZ = ZoneInfo("Asia/Shanghai")
DATE_RE = re.compile(r"\((\d{2})æœˆ(\d{2})æ—¥\s*(\d{2}):(\d{2})\)")


# ================= å·¥å…·å‡½æ•° =================
def now_cn():
    return datetime.now(TZ)


def get_html(url):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (X11; Linux x86_64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "zh-CN,zh;q=0.9",
    }
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    if not r.encoding or r.encoding.lower() == "iso-8859-1":
        r.encoding = r.apparent_encoding
    return r.text


def parse_datetime(text):
    m = DATE_RE.search(text)
    if not m:
        return None

    month, day, hh, mm = map(int, m.groups())
    now = now_cn()
    year = now.year

    if now.month == 1 and month == 12:
        year -= 1

    try:
        return datetime(year, month, day, hh, mm, tzinfo=TZ)
    except Exception:
        return None


def find_next_page(soup):
    a = soup.find("a", string=lambda s: s and "ä¸‹ä¸€é¡µ" in s)
    if a and a.get("href"):
        return urljoin(START_URL, a["href"])
    return None


# ================= ä¸»é€»è¾‘ =================
def main():
    yesterday = (now_cn() - timedelta(days=1)).date()
    results = []

    url = START_URL
    hit_yesterday = False

    for page in range(1, MAX_PAGES + 1):
        html = get_html(url)
        soup = BeautifulSoup(html, "html.parser")

        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä¸å†æ­»ç›¯ ul.listcontent
        container = soup.select_one("div.listBlk")
        if not container:
            print("âŒ æœªæ‰¾åˆ° listBlk å®¹å™¨")
            break

        lis = container.find_all("li")
        if not lis:
            print("âŒ listBlk ä¸‹æœªæ‰¾åˆ° li")
            break

        for li in lis:
            a = li.find("a", href=True)
            if not a:
                continue

            title = a.get_text(strip=True)
            link = urljoin(START_URL, a["href"])
            text = li.get_text(" ", strip=True)

            dt = parse_datetime(text)
            if not dt:
                continue

            if dt.date() == yesterday:
                results.append((dt, title, link))
                hit_yesterday = True

        # æ—©åœé€»è¾‘
        if hit_yesterday:
            dts = [
                parse_datetime(li.get_text(" ", strip=True))
                for li in lis
            ]
            dts = [d for d in dts if d]
            if dts and all(d.date() < yesterday for d in dts):
                break

        next_url = find_next_page(soup)
        if not next_url:
            break

        url = next_url
        time.sleep(SLEEP_SEC)

    # æ’åº
    results.sort(key=lambda x: x[0], reverse=True)

    # è¾“å‡º Markdown
    lines = [f"# æ–°æµªè´¢ç» Â· æ˜¨æ—¥æ›´æ–°ï¼ˆ{yesterday}ï¼‰\n"]

    if not results:
        lines.append("ï¼ˆæ˜¨æ—¥æ— æ›´æ–°æˆ–é¡µé¢ç»“æ„å˜åŒ–ï¼‰")
    else:
        for dt, title, link in results:
            lines.append(f"- [{title}]({link})  {dt.strftime('%H:%M')}")

    lines.append(f"\n_ç”Ÿæˆæ—¶é—´ï¼š{now_cn().strftime('%Y-%m-%d %H:%M:%S')}_")

    with open(OUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"âœ… æŠ“å–å®Œæˆï¼Œå…± {len(results)} æ¡ï¼Œå·²å†™å…¥ {OUT_FILE}")


if __name__ == "__main__":
    main()
