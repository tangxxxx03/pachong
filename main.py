# -*- coding: utf-8 -*-
"""
å¤–åŒ…/æ´¾é£ï¼šæ‹›æ ‡ & ä¸­æ ‡é‡‡é›†ï¼ˆåŒ—äº¬å…¬å…±èµ„æºäº¤æ˜“å¹³å° + zsxtzb.cn æœç´¢ï¼‰
â€”â€” æ¸…çˆ½è¾“å‡º + å­—æ®µå¢å¼ºç‰ˆï¼ˆå®Œæ•´ä»£ç ï¼‰
"""

import os, re, time, math, hmac, base64, hashlib
from datetime import datetime, timedelta
from io import BytesIO
from urllib.parse import urlparse, urljoin, quote_plus

import pandas as pd
import pdfplumber
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service


# ================== å›ºå®šé…ç½®ï¼ˆä¸è¯»ç¯å¢ƒå˜é‡ï¼‰ ==================
DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=6e945607bb71c2fd9bb3399c6424fa7dece4b9798d2a8ff74b0b71ab47c9d182"
DINGTALK_SECRET  = ""  # è‹¥å¼€å¯â€œåŠ ç­¾â€ï¼Œå¡«å…¥å¯†é’¥ï¼›æœªå¼€å¯åˆ™ç•™ç©ºå­—ç¬¦ä¸²

KEYWORDS        = ["å¤–åŒ…", "æ´¾é£"]
CRAWL_BEIJING   = True
CRAWL_ZSXTZB    = True

# åªä¿ç•™æœªæ¥ N å¤©å†…æˆªæ­¢çš„æ‹›æ ‡ï¼›<=0 è¡¨ç¤ºä¸è¿‡æ»¤
DUE_FILTER_DAYS = 30
# ä¸¢å¼ƒå·²è¿‡æœŸçš„æ‹›æ ‡ï¼ˆä»…å½“èƒ½è§£æå‡ºæˆªæ­¢æ—¶é—´ï¼‰
SKIP_EXPIRED    = True

HEADLESS        = True

# è¾“å‡ºæ§åˆ¶ï¼šæ‘˜è¦æˆªæ–­é•¿åº¦ã€å•æ¡å¡ç‰‡æœ€å¤šæ˜¾ç¤ºå‡ è¡Œâ€œæ‰©å±•å­—æ®µâ€
BRIEF_MAX_LEN   = 120
EXTRA_MAX_LINES = 3

# DingTalk å•æ¡ markdown å®‰å…¨é•¿åº¦ï¼ˆç»éªŒå€¼ï¼‰
DINGTALK_CHUNK  = 4200


# ========== HTTP ä¼šè¯ï¼ˆç¦ç”¨ç¯å¢ƒä»£ç† + é‡è¯•ï¼‰ ==========
for _k in ('http_proxy','https_proxy','HTTP_PROXY','HTTPS_PROXY','ALL_PROXY','all_proxy','NO_PROXY'):
    os.environ.pop(_k, None)

_SESSION = requests.Session()
_SESSION.trust_env = False
_retry = Retry(
    total=4,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=frozenset(["GET", "POST"])
)
_SESSION.mount("http://", HTTPAdapter(max_retries=_retry))
_SESSION.mount("https://", HTTPAdapter(max_retries=_retry))
_SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
})


# ================== DingTalk åŠ ç­¾ä¸å‘é€ ==================
def _build_signed_webhook(base_url: str, secret: str) -> str:
    base_url = (base_url or "").strip()
    if not base_url or not secret:
        return base_url
    ts = str(int(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}"
    h = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), hashlib.sha256).digest()
    sign = quote_plus(base64.b64encode(h))
    sep = "&" if ("?" in base_url) else "?"
    return f"{base_url}{sep}timestamp={ts}&sign={sign}"

def send_to_dingtalk_markdown(title: str, md_text: str):
    base_webhook = (DINGTALK_WEBHOOK or "").strip()
    if not base_webhook.startswith("http"):
        print("? Webhook æœªé…ç½®æˆ–æ— æ•ˆ"); return
    final_url = _build_signed_webhook(base_webhook, (DINGTALK_SECRET or "").strip())
    headers = {"Content-Type": "application/json"}
    data = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    try:
        resp = _SESSION.post(final_url, json=data, headers=headers, timeout=15)
        print("é’‰é’‰æ¨é€ï¼š", resp.status_code, resp.text[:180])
    except Exception as e:
        print("? å‘é€é’‰é’‰å¤±è´¥ï¼š", e)


# ================== æ—¥æœŸèŒƒå›´ï¼šé»˜è®¤â€œæ˜¨æ—¥â€ï¼Œå‘¨ä¸€æŠ“å‘¨äº” ==================
def get_date_range():
    today = datetime.now()
    if today.weekday() == 0:
        start = today - timedelta(days=3)
        end   = today - timedelta(days=1)
    else:
        start = today - timedelta(days=1)
        end   = today - timedelta(days=1)
    return start.strftime("%Y-%m-%d"), end.strftime("%Y-%m-%d")


# ================== åˆ†ç±»ï¼ˆä¿æŒä½ åŸé€»è¾‘ï¼Œç•¥å¢å¼ºï¼‰ ==================
def classify(title: str) -> str:
    t = title or ""
    if any(k in t for k in ["ä¸­æ ‡", "æˆäº¤", "ç»“æœ", "å®šæ ‡", "å€™é€‰äººå…¬ç¤º", "æˆäº¤å…¬å‘Š", "ä¸­æ ‡å…¬å‘Š"]): return "ä¸­æ ‡å…¬å‘Š"
    if any(k in t for k in ["æ›´æ­£", "å˜æ›´", "æ¾„æ¸…", "è¡¥é—"]): return "æ›´æ­£å…¬å‘Š"
    if any(k in t for k in ["ç»ˆæ­¢", "åºŸæ ‡", "æµæ ‡"]): return "ç»ˆæ­¢å…¬å‘Š"
    if any(k in t for k in ["æ‹›æ ‡", "é‡‡è´­", "ç£‹å•†", "é‚€è¯·", "æ¯”é€‰", "è°ˆåˆ¤", "ç«äº‰æ€§", "å…¬å¼€æ‹›æ ‡"]): return "æ‹›æ ‡å…¬å‘Š"
    return "å…¶ä»–"


# ================== æ–‡æœ¬å·¥å…· ==================
def _safe_text(s: str) -> str:
    return (s or "").replace("\u3000", " ").replace("\xa0", " ").strip()

def _date_in_text(s: str):
    if not s: return ""
    m = re.search(r"(20\d{2}[-/.]\d{1,2}[-/.]\d{1,2})", s)
    return m.group(1).replace(".", "-").replace("/", "-") if m else ""

def _normalize_amount_text(s: str) -> str:
    """æŠŠé‡‘é¢é‡Œå¸¸è§çš„ç©ºæ ¼/é€—å·å»æ‰ï¼Œä¿ç•™åŸå•ä½"""
    if not s: return ""
    s = str(s).replace("ï¼Œ", ",").replace(",", "")
    s = re.sub(r"\s+", "", s)
    return s

def _normalize_date_string(s: str) -> str:
    """æŠŠ '2026å¹´1æœˆ9æ—¥ 09:30' / '2026-01-09 9:30' è§„æ•´æˆ 'YYYY-MM-DD HH:MM' or 'YYYY-MM-DD' """
    if not s: return ""
    s = s.strip()
    s = s.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", " ")
    s = s.replace("/", "-").replace("ï¼š", ":").replace("ï¼", ".")
    s = re.sub(r"\s+", " ", s)

    m = re.search(r"(20\d{2})[-\.](\d{1,2})[-\.](\d{1,2})(?:\s+(\d{1,2}):(\d{2}))?", s)
    if not m:
        return ""
    y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
    hh = int(m.group(4)) if m.group(4) else None
    mm = int(m.group(5)) if m.group(5) else None
    try:
        if hh is not None and mm is not None:
            return datetime(y, mo, d, hh, mm).strftime("%Y-%m-%d %H:%M")
        return datetime(y, mo, d).strftime("%Y-%m-%d")
    except Exception:
        return ""

def _to_datetime(s: str):
    if not s: return None
    for fmt in ("%Y-%m-%d %H:%M", "%Y-%m-%d"):
        try:
            return datetime.strptime(s, fmt)
        except Exception:
            pass
    return None

def _pick_first(text: str, patterns):
    """å¤šå¥—æ­£åˆ™ï¼šè¿”å›ç¬¬ä¸€ä¸ªå‘½ä¸­çš„ group(1)"""
    for pat in patterns:
        m = re.search(pat, text, re.S | re.I)
        if m:
            val = m.group(1).strip()
            if val:
                return val
    return ""


# ================== æˆªæ­¢æ—¶é—´æŠ½å–ï¼ˆå¢å¼ºï¼šæ›´å¤šè§¦å‘è¯ï¼‰ ==================
def extract_deadline(detail_text: str) -> str:
    txt = _safe_text(detail_text)

    pats = [
        # æŠ•æ ‡/å“åº”/é€’äº¤ æˆªæ­¢
        r"(?:æŠ•æ ‡(?:æ–‡ä»¶)?|é€’äº¤(?:å“åº”)?æ–‡ä»¶|å“åº”æ–‡ä»¶æäº¤|æŠ¥ä»·|æŠ¥å|è·å–æ‹›æ ‡æ–‡ä»¶)\s*æˆªæ­¢(?:æ—¶é—´|æ—¥æœŸ)\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})",
        r"(?:æˆªæ­¢(?:æ—¶é—´|æ—¥æœŸ))\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})(?=.*?(?:æŠ•æ ‡|é€’äº¤|å“åº”|æŠ¥ä»·|æŠ¥å))",
        # â€œæäº¤æˆªæ­¢â€â€œæˆªæ­¢è‡³â€
        r"(?:æäº¤|é€’äº¤)\s*æˆªæ­¢(?:æ—¶é—´|æ—¥æœŸ)\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})",
        r"(?:æˆªæ­¢è‡³)\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})",
    ]
    s = _pick_first(txt, pats)
    norm = _normalize_date_string(s)
    if norm:
        return norm

    # å…œåº•ï¼šå¼€æ ‡æ—¶é—´
    s2 = _pick_first(txt, [r"(?:å¼€æ ‡(?:æ—¶é—´|æ—¥æœŸ))\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})"])
    norm2 = _normalize_date_string(s2)
    return norm2 or ""


# ================== æ‘˜è¦æŠ½å–ï¼ˆå¢å¼ºï¼šå¤šæ®µåå…œåº•ï¼‰ ==================
def extract_project_brief(detail_text: str, max_len: int = 120) -> str:
    txt = _safe_text(detail_text)
    blocks = []

    # 1) é¡¹ç›®æ¦‚å†µæ®µ
    m = re.search(r"é¡¹ç›®æ¦‚å†µ\s*([\s\S]{0,900}?)(?=\n\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€|\n\s*ä¸€ã€|$)", txt)
    if m:
        blocks.append(m.group(1))

    # 2) é¡¹ç›®åŸºæœ¬æƒ…å†µ
    m2 = re.search(r"(?:é¡¹ç›®åŸºæœ¬æƒ…å†µ|ä¸€ã€é¡¹ç›®åŸºæœ¬æƒ…å†µ)\s*([\s\S]{0,900}?)(?=\n\s*[äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€|\n\s*äºŒã€|$)", txt)
    if m2:
        blocks.append(m2.group(1))

    # 3) é‡‡è´­éœ€æ±‚/æœåŠ¡èŒƒå›´
    m3 = re.search(r"(?:é‡‡è´­éœ€æ±‚|æœåŠ¡èŒƒå›´|é¡¹ç›®å†…å®¹|æœåŠ¡å†…å®¹)\s*[:ï¼š]?\s*([\s\S]{0,300}?)\n", txt)
    if m3:
        blocks.append(m3.group(1))

    block = ""
    for b in blocks:
        b = re.sub(r"\s+", " ", (b or "")).strip()
        b = re.sub(r"^[ï¼š:ã€\-ï¼Œã€‚.\s]*", "", b).strip()
        if len(b) >= 20:
            block = b
            break

    if not block:
        plain = re.sub(r"\s+", " ", txt)
        block = plain[:max_len]

    block = block[:max_len] + ("..." if len(block) > max_len else "")
    return block.strip()


# ================== PDF æ–‡æœ¬è¯»å– ==================
def fetch_pdf_text(url: str, referer: str = None, timeout=20) -> str:
    try:
        headers = {"User-Agent":"Mozilla/5.0"}
        if referer:
            headers["Referer"] = referer
        r = _SESSION.get(url, headers=headers, timeout=timeout)
        ct = (r.headers.get("Content-Type") or "").lower()
        if "pdf" not in ct and not url.lower().endswith(".pdf"):
            return ""
        with pdfplumber.open(BytesIO(r.content)) as pdf:
            pages = []
            for p in pdf.pages:
                try:
                    pages.append(p.extract_text() or "")
                except Exception:
                    continue
        return "\n".join(pages)
    except Exception as e:
        print("PDF è¯»å–å¤±è´¥ï¼š", e)
        return ""


# ================== è¯¦æƒ…æ–‡æœ¬æŠ½å–ï¼ˆå¢å¼ºï¼šæ›´å¤šå®¹å™¨ + é™„ä»¶ PDF å…œåº•ï¼‰ ==================
def extract_detail_text_with_pdf_fallback(driver, page_html: str, page_url: str):
    # å…ˆå°è¯•æ›´å¸¸è§å†…å®¹å®¹å™¨
    xps = [
        "//*[@id='vsb_content']",
        "//*[@id='zoom']",
        "//*[@class='content']",
        "//*[@class='article']",
        "//*[@class='detail']",
        "//*[@class='cont']",
        "//*[@id='xxnr']",
        "//*[@id='info']",
        "//article",
        "//main",
    ]
    for xp in xps:
        try:
            t = driver.find_element(By.XPATH, xp).text
            if t and len(t.strip()) > 80:
                return t
        except Exception:
            pass

    # å†æ‰¾é™„ä»¶ pdf
    try:
        links = re.findall(r'href=["\'](.*?)["\']', page_html, flags=re.I)
        pdfs = []
        for h in links:
            absu = urljoin(page_url, (h or "").strip())
            if absu.lower().endswith(".pdf"):
                pdfs.append(absu)

        if not pdfs:
            for a in driver.find_elements(By.XPATH, "//a"):
                try:
                    txt = (a.text or "").strip()
                    href = a.get_attribute("href") or ""
                    if href and (("PDF" in (txt.upper())) or ("é™„ä»¶" in txt) or ("ä¸‹è½½" in txt)):
                        absu = urljoin(page_url, href)
                        if absu.lower().endswith(".pdf"):
                            pdfs.append(absu)
                except Exception:
                    continue

        for pdf_url in pdfs[:3]:
            pdf_text = fetch_pdf_text(pdf_url, referer=page_url)
            if pdf_text and len(pdf_text.strip()) > 80:
                return pdf_text
    except Exception:
        pass

    # å…œåº•ï¼šæ•´é¡µ body
    try:
        return driver.find_element(By.TAG_NAME, "body").text
    except Exception:
        return ""


# ================== æ‹›æ ‡å­—æ®µè§£æï¼ˆå¢å¼ºï¼šé¢„ç®—/é‡‡è´­äºº/ä»£ç†/åœ°å€/è”ç³»äºº/ç”µè¯/æˆªæ­¢/æ‘˜è¦ï¼‰ ==================
def parse_bidding_fields(detail_text: str):
    txt = _safe_text(detail_text)

    # é¢„ç®—/æœ€é«˜é™ä»·/æ§åˆ¶ä»·
    amount = _pick_first(txt, [
        r"(?:é¢„ç®—é‡‘é¢|é‡‡è´­é¢„ç®—)\s*[:ï¼š]?\s*([0-9\.,ï¼Œ]+\s*(?:ä¸‡å…ƒ|å…ƒ))",
        r"(?:æœ€é«˜é™ä»·|æ§åˆ¶ä»·)\s*[:ï¼š]?\s*([0-9\.,ï¼Œ]+\s*(?:ä¸‡å…ƒ|å…ƒ))",
        r"(?:é¡¹ç›®é¢„ç®—)\s*[:ï¼š]?\s*([0-9\.,ï¼Œ]+\s*(?:ä¸‡å…ƒ|å…ƒ))",
    ])
    amount = _normalize_amount_text(amount) if amount else "æš‚æ— "

    # é‡‡è´­äºº
    purchaser = _pick_first(txt, [
        r"(?:é‡‡è´­äºº|é‡‡è´­å•ä½|æ‹›æ ‡äºº)\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{2,60})",
    ])
    purchaser = purchaser or "æš‚æ— "

    # ä»£ç†æœºæ„
    agent = _pick_first(txt, [
        r"(?:é‡‡è´­ä»£ç†æœºæ„|ä»£ç†æœºæ„|æ‹›æ ‡ä»£ç†)\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{2,60})",
    ])
    agent = agent or "æš‚æ— "

    # åœ°å€ï¼ˆé‡‡è´­äººåœ°å€/é¡¹ç›®åœ°ç‚¹ï¼‰
    address = _pick_first(txt, [
        r"(?:åœ°å€|é¡¹ç›®åœ°ç‚¹|æœåŠ¡åœ°ç‚¹|å®æ–½åœ°ç‚¹)\s*[:ï¼š]?\s*([^\n\rã€‚ï¼›;]{5,80})",
    ])
    address = address or "æš‚æ— "

    # è”ç³»äºº+ç”µè¯ï¼ˆä¼˜å…ˆâ€œé¡¹ç›®è”ç³»äººâ€å—ï¼‰
    contact = "æš‚æ— "
    phone   = "æš‚æ— "
    m_cp = re.search(
        r"é¡¹ç›®è”ç³»äºº[ï¼š:\s]*([^\sã€ï¼Œã€‚;ï¼›]+)[\s\S]{0,120}?"
        r"(?:ç”µ\s*è¯|è”ç³»ç”µè¯|è”ç³»æ–¹å¼)[ï¼š:\s]*([0-9\-ï¼â€”\s]{6,})",
        txt, re.S
    )
    if m_cp:
        contact = m_cp.group(1).strip()
        phone = re.sub(r"\s+", "", m_cp.group(2)).replace("ï¼", "-").replace("â€”", "-")
    else:
        c2 = _pick_first(txt, [
            r"(?:è”ç³»äºº|é¡¹ç›®è”ç³»äºº|é‡‡è´­äººè”ç³»äºº)\s*[:ï¼š]?\s*([^\sã€ï¼Œã€‚;ï¼›]{2,20})"
        ])
        p2 = _pick_first(txt, [
            r"(?:è”ç³»ç”µè¯|è”ç³»æ–¹å¼|ç”µ\s*è¯)\s*[:ï¼š]?\s*([0-9\-ï¼â€”\s]{6,})"
        ])
        if c2: contact = c2
        if p2: phone = re.sub(r"\s+", "", p2).replace("ï¼", "-").replace("â€”", "-")

    # æˆªæ­¢
    deadline = extract_deadline(txt) or "æš‚æ— "

    # æ‘˜è¦
    brief = extract_project_brief(txt, max_len=BRIEF_MAX_LEN) or "æš‚æ— "

    # æ‰©å±•å­—æ®µï¼ˆå°‘é‡å…³é”®å­—å‘½ä¸­æ—¶æ‰åŠ ï¼‰
    extra = []
    # è·å–æ–‡ä»¶æ–¹å¼/å¹³å°æç¤ºï¼ˆç»å¸¸å¯¹ä½ ä»¬å¾ˆæœ‰ç”¨ï¼‰
    m_get = re.search(r"(æ½œåœ¨æŠ•æ ‡äºº.*?è·å–æ‹›æ ‡æ–‡ä»¶.*?)(?=ã€‚\s|\n)", txt)
    if m_get:
        extra.append(re.sub(r"\s+", " ", m_get.group(1)).strip())

    # æœåŠ¡æœŸé™/åˆåŒå±¥è¡ŒæœŸé™
    m_term = re.search(r"(?:æœåŠ¡æœŸé™|åˆåŒå±¥è¡ŒæœŸé™|å±¥çº¦æœŸé™)\s*[:ï¼š]?\s*([^\n\rã€‚ï¼›;]{3,60})", txt)
    if m_term:
        extra.append(f"æœŸé™ï¼š{m_term.group(1).strip()}")

    return {
        "é‡‘é¢": amount,
        "é‡‡è´­äºº": purchaser,
        "ä»£ç†æœºæ„": agent,
        "åœ°å€": address,
        "è”ç³»äºº": contact,
        "è”ç³»ç”µè¯": phone,
        "ç®€è¦æ‘˜è¦": brief,
        "æŠ•æ ‡æˆªæ­¢": deadline,
        "æ‰©å±•ä¿¡æ¯": extra[:EXTRA_MAX_LINES],
    }


# ================== ä¸­æ ‡è§£æï¼šè¡¨æ ¼ä¼˜å…ˆ + æ–‡æœ¬å…œåº•ï¼ˆå¢å¼ºï¼‰ ==================
def _num_from_any(v):
    if v in (None, "", "æš‚æ— "): return None
    s = str(v).replace(",", "").replace("ï¼Œ", "")
    m = re.search(r"(-?\d+(?:\.\d+)?)", s)
    return float(m.group(1)) if m else None

def parse_award_from_tables(html: str):
    supplier = amount = score = content = "æš‚æ— "
    unit = ""

    try:
        tables = pd.read_html(html)
    except Exception:
        tables = []

    rows = []
    for tb in tables:
        t = tb.fillna("").astype(str)
        cols = [str(c) for c in t.columns]
        joined_cols = "".join(cols)

        if not any(k in joined_cols for k in ["ä¾›åº”å•†", "å•ä½åç§°", "ä¸­æ ‡äºº", "æˆäº¤äºº"]):
            continue

        def find_col(keys):
            for k in keys:
                for c in cols:
                    if k in c:
                        return c
            return None

        c_sup = find_col(["ä¾›åº”å•†åç§°", "ä¾›åº”å•†", "å•ä½åç§°", "ä¸­æ ‡äºº", "æˆäº¤äºº"])
        c_sco = find_col(["è¯„å®¡å¾—åˆ†", "ç»¼åˆå¾—åˆ†", "æœ€ç»ˆå¾—åˆ†", "å¾—åˆ†"])
        c_rnk = find_col(["åæ¬¡", "æ’åº", "æ’å"])
        c_pri = find_col(["è¯„å®¡æŠ¥ä»·", "ä¸­æ ‡é‡‘é¢", "æˆäº¤é‡‘é¢", "æŠ¥ä»·", "æŠ•æ ‡æŠ¥ä»·", "é‡‘é¢"])

        for _, r in t.iterrows():
            name = (r.get(c_sup, "") if c_sup else "").strip()
            if not name:
                continue

            price_val = r.get(c_pri) if c_pri else None
            score_val = r.get(c_sco) if c_sco else None
            rank_val  = r.get(c_rnk) if c_rnk else None

            row = {
                "supplier": name,
                "score": _num_from_any(score_val),
                "rank":  _num_from_any(rank_val),
                "price": _num_from_any(price_val),
            }

            # å•ä½ä»åˆ—åæˆ–å•å…ƒæ ¼çŒœ
            if c_pri and ("ä¸‡å…ƒ" in c_pri):
                unit = "ä¸‡å…ƒ"
            if isinstance(price_val, str) and ("ä¸‡å…ƒ" in price_val):
                unit = "ä¸‡å…ƒ"
            if isinstance(price_val, str) and (price_val.strip().endswith("å…ƒ")):
                unit = "å…ƒ"

            rows.append(row)

    chosen = None
    if rows:
        with_score = [r for r in rows if r["score"] is not None]
        if with_score:
            chosen = max(with_score, key=lambda x: x["score"])
        else:
            with_rank = [r for r in rows if r["rank"] is not None]
            if with_rank:
                chosen = min(with_rank, key=lambda x: x["rank"])
            else:
                with_price = [r for r in rows if r["price"] is not None]
                chosen = min(with_price, key=lambda x: x["price"]) if with_price else rows[0]

    if chosen:
        supplier = chosen["supplier"] or "æš‚æ— "
        if chosen["price"] is not None:
            amount = str(chosen["price"])
            if unit:
                amount = f"{amount}{unit}"
        score = str(chosen["score"]) if chosen["score"] is not None else "æš‚æ— "

    return {
        "ä¸­æ ‡å…¬å¸": supplier,
        "ä¸­æ ‡é‡‘é¢": amount,
        "è¯„å®¡å¾—åˆ†": (score or "æš‚æ— ").rstrip("åˆ†"),
        "ä¸­æ ‡å†…å®¹": content or "æš‚æ— ",
    }

def parse_award_from_text(detail_text: str):
    txt = _safe_text(detail_text)

    supplier = _pick_first(txt, [
        r"(?:ä¸­æ ‡(?:ä¾›åº”å•†|äºº|å•ä½)|æˆäº¤(?:ä¾›åº”å•†|äºº|å•ä½)|ä¾›åº”å•†åç§°)\s*[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›;]{2,80})",
        r"(?:æˆäº¤å•ä½)\s*[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›;]{2,80})",
    ])

    amount = _pick_first(txt, [
        r"(?:ä¸­æ ‡(?:ä»·|é‡‘é¢)|æˆäº¤(?:ä»·|é‡‘é¢)|è¯„å®¡æŠ¥ä»·|æˆäº¤ä»·)\s*[ï¼š:]\s*([0-9\.,ï¼Œ]+\s*(?:ä¸‡å…ƒ|å…ƒ)?)",
        r"(?:åˆåŒé‡‘é¢)\s*[ï¼š:]\s*([0-9\.,ï¼Œ]+\s*(?:ä¸‡å…ƒ|å…ƒ)?)",
    ])
    amount = _normalize_amount_text(amount) if amount else "æš‚æ— "

    score = _pick_first(txt, [
        r"(?:è¯„å®¡(?:å¾—åˆ†|åˆ†å€¼)|ç»¼åˆå¾—åˆ†|æœ€ç»ˆå¾—åˆ†|å¾—åˆ†)\s*[ï¼š:]\s*([0-9\.]+)",
    ])
    score = (score or "æš‚æ— ").rstrip("åˆ†")

    content = _pick_first(txt, [
        r"(?:é‡‡è´­å†…å®¹|é‡‡è´­éœ€æ±‚|é¡¹ç›®æ¦‚å†µ|æœåŠ¡å†…å®¹|ä¸­æ ‡å†…å®¹)\s*[ï¼š:]\s*([^\n\r]{6,120})",
    ])
    content = content or "æš‚æ— "

    return {
        "ä¸­æ ‡å…¬å¸": supplier or "æš‚æ— ",
        "ä¸­æ ‡é‡‘é¢": amount,
        "è¯„å®¡å¾—åˆ†": score,
        "ä¸­æ ‡å†…å®¹": content,
    }


# ================== åŸæ‹›æ ‡ç½‘å€ï¼ˆæ›´ç¨³ï¼šå‰”é™¤â€œåˆ—è¡¨/é¢‘é“/è¿”å›â€ç­‰ï¼‰ ==================
def choose_origin_notice_url(detail_html: str, current_url: str) -> str:
    if not detail_html:
        return "æš‚æ— "

    hrefs = re.findall(r'<a[^>]+href=["\'](.*?)["\']', detail_html, flags=re.I)
    if not hrefs:
        return "æš‚æ— "

    cur = urlparse(current_url or "")
    cur_dom = f"{cur.scheme}://{cur.netloc}" if cur.scheme and cur.netloc else ""

    clean = []
    for h in hrefs:
        h = (h or "").strip()
        if not h or h.startswith("#") or h.lower().startswith("javascript"):
            continue
        absu = urljoin(current_url or "", h)
        if absu == current_url:
            continue
        clean.append(absu)

    if not clean:
        return "æš‚æ— "

    kw_good = ["æ‹›æ ‡", "é‡‡è´­", "å…¬å‘Š", "å…¬å¼€", "zb", "zhaobiao", "notice"]
    bad_words = ["é¦–é¡µ", "è¿”å›", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ", "æ›´å¤š", "ä¸‹è½½ä¸­å¿ƒ", "æ ç›®", "é¢‘é“", "åˆ—è¡¨", "index", "list", "channel", "column"]
    good_exts = [".html", ".shtml", ".htm", ".pdf"]

    def score(u: str) -> tuple:
        p = urlparse(u)
        low = u.lower()
        s = 0
        if any(k in u for k in kw_good) or any(k in low for k in ["zbgg", "zhaobiao", "cgxx", "notice"]):
            s += 6
        if any(low.endswith(ext) for ext in good_exts):
            s += 3
        if cur_dom and (f"{p.scheme}://{p.netloc}" == cur_dom):
            s += 2
        depth = len([seg for seg in p.path.split("/") if seg])
        s += min(depth, 6)
        if re.search(r"(20\d{2}[-/_.]?\d{2}([-/_.]?\d{2})?)", low):
            s += 2
        if any(b in u for b in bad_words):
            s -= 6
        return (s, -len(u))

    best = sorted(set(clean), key=score, reverse=True)[0]

    # æœ€åå†åšä¸€æ¬¡â€œåƒä¸åƒå…¬å‘Šé¡µâ€çš„å…œåº•
    if not any(best.lower().endswith(ext) for ext in good_exts):
        if not any(k in best for k in kw_good) and not any(k in best.lower() for k in ["zbgg", "zhaobiao", "cgxx", "notice"]):
            return "æš‚æ— "

    return best


def parse_award_fields(detail_html: str, detail_text: str, current_url: str = ""):
    # 1) è¡¨æ ¼ä¼˜å…ˆ
    data = parse_award_from_tables(detail_html)

    # 2) è¡¨æ ¼å¤±è´¥å†ç”¨æ–‡æœ¬
    if data.get("ä¸­æ ‡å…¬å¸") == "æš‚æ— " and data.get("ä¸­æ ‡é‡‘é¢") == "æš‚æ— ":
        data = parse_award_from_text(detail_text)

    # 3) åŸæ‹›æ ‡ç½‘å€
    data["åŸæ‹›æ ‡ç½‘å€"] = choose_origin_notice_url(detail_html, current_url) or "æš‚æ— "

    # 4) ä¸­æ ‡æ—¥æœŸï¼šä¼˜å…ˆå­—æ®µï¼Œå†å…œåº•é¡µé¢å†…æ—¥æœŸ
    txt = _safe_text(detail_text or "")
    award_date = _pick_first(txt, [
        r"(?:å…¬å‘Šæ—¥æœŸ|å…¬ç¤ºæ—¶é—´|å‘å¸ƒæ—¶é—´|æˆäº¤æ—¥æœŸ|ä¸­æ ‡æ—¥æœŸ)\s*[ï¼š:]\s*([0-9]{4}[-/.][0-9]{1,2}[-/.][0-9]{1,2})",
    ])
    if not award_date:
        award_date = _date_in_text(txt)

    award_date = _normalize_date_string(award_date) or award_date or "æš‚æ— "
    data["ä¸­æ ‡æ—¥æœŸ"] = award_date

    return data


# -------- Seleniumï¼ˆå®¹å™¨å‹å¥½ï¼‰ --------
def _build_driver():
    opts = Options()
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    if HEADLESS:
        opts.add_argument("--headless=new")
    try:
        driver = webdriver.Chrome(options=opts)  # Selenium Manager
    except Exception:
        from webdriver_manager.chrome import ChromeDriverManager
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)
    driver.implicitly_wait(5)
    driver.set_page_load_timeout(45)
    driver.set_script_timeout(45)
    return driver


# ================== ç«™ç‚¹ä¸€ï¼šåŒ—äº¬å…¬å…±èµ„æºäº¤æ˜“å¹³å° ==================
def crawl_beijing(keywords, max_pages=10, date_start=None, date_end=None):
    driver = _build_driver()
    all_bidding, all_award, seen_links = [], [], set()

    try:
        for kw in keywords:
            url = f"https://ggzyfw.beijing.gov.cn/elasticsearch/index.jsp?qt={kw}"
            driver.get(url)
            time.sleep(3.0)

            # æ—¶é—´è¿‡æ»¤ï¼šä¸€å‘¨ï¼ˆå°½é‡ç‚¹ï¼Œç‚¹ä¸åˆ°å°±ç®—ï¼‰
            try:
                driver.find_element(By.XPATH, "//span[contains(text(),'æ—¶é—´ä¸é™')]").click()
                time.sleep(0.6)
                driver.find_element(By.ID, "week").click()
                time.sleep(1.0)
            except Exception:
                pass

            for page in range(1, max_pages + 1):
                cards = driver.find_elements(By.CLASS_NAME, "cs_search_content_box")
                if not cards:
                    break

                for c in cards:
                    try:
                        title_el = c.find_element(By.CLASS_NAME, "cs_search_title")
                        title = title_el.text.strip()
                        ann_type = classify(title)
                        if ann_type not in ("æ‹›æ ‡å…¬å‘Š", "ä¸­æ ‡å…¬å‘Š"):
                            continue

                        # åˆ—è¡¨æ‘˜è¦
                        snippet = ""
                        try:
                            snippet = c.find_element(By.CLASS_NAME, "cs_search_content_p").text
                        except Exception:
                            pass

                        # ä¿¡æ¯æ¥æº + å‘å¸ƒæ—¶é—´ï¼ˆåˆ—è¡¨è¡Œï¼‰
                        info_source, pub_time = "æš‚æ— ", "æš‚æ— "
                        try:
                            source_line = c.find_element(By.CLASS_NAME, "cs_search_content_time").text
                            if "å‘å¸ƒæ—¶é—´ï¼š" in source_line:
                                parts = source_line.split("å‘å¸ƒæ—¶é—´ï¼š")
                                info_source = parts[0].replace("ä¿¡æ¯æ¥æºï¼š", "").strip() or "æš‚æ— "
                                pub_time = parts[1].strip() or "æš‚æ— "
                        except Exception:
                            pass

                        pub_date = pub_time[:10] if pub_time and pub_time != "æš‚æ— " else ""

                        # æ—¥æœŸè¿‡æ»¤ï¼ˆæŒ‰å‘å¸ƒæ—¥æœŸï¼‰
                        if date_start and date_end and pub_date:
                            if pub_date < date_start or pub_date > date_end:
                                continue

                        # è¯¦æƒ…é“¾æ¥
                        url_link = ""
                        try:
                            url_link = title_el.find_element(By.TAG_NAME, "a").get_attribute("href")
                        except Exception:
                            url_link = ""

                        if url_link and url_link in seen_links:
                            continue
                        if url_link:
                            seen_links.add(url_link)

                        detail_text, detail_html = "", ""
                        if url_link:
                            win = driver.current_window_handle
                            driver.execute_script('window.open(arguments[0])', url_link)
                            driver.switch_to.window(driver.window_handles[-1])
                            time.sleep(1.2)

                            detail_html = driver.page_source
                            detail_text = extract_detail_text_with_pdf_fallback(driver, detail_html, url_link) or snippet

                            # è¯¦æƒ…é¡µâ€œå‘å¸ƒæ¥æºâ€å…œåº•è¦†ç›–
                            if detail_text:
                                m_src = re.search(r"å‘å¸ƒæ¥æº[ï¼š:\s]*([^\n\r]+)", detail_text)
                                if m_src:
                                    info_source = (m_src.group(1).strip() or info_source)

                            driver.close()
                            driver.switch_to.window(win)

                        if ann_type == "æ‹›æ ‡å…¬å‘Š":
                            fields = parse_bidding_fields(detail_text)

                            due_str = fields.get("æŠ•æ ‡æˆªæ­¢", "æš‚æ— ")
                            due_dt  = _to_datetime(due_str if due_str != "æš‚æ— " else "")

                            keep = True
                            now = datetime.now()

                            if SKIP_EXPIRED and due_dt and due_dt < now:
                                keep = False
                            if keep and DUE_FILTER_DAYS > 0 and due_dt and due_dt > now + timedelta(days=DUE_FILTER_DAYS):
                                keep = False

                            if keep:
                                all_bidding.append({
                                    "ç«™ç‚¹": "åŒ—äº¬å…¬å…±èµ„æº",
                                    "å…³é”®è¯": kw,
                                    "å…¬å‘Šæ ‡é¢˜": title,
                                    "å…¬å‘Šå‘å¸ƒæ—¶é—´": pub_time,
                                    "ä¿¡æ¯æ¥æº": info_source,
                                    "æŠ•æ ‡æˆªæ­¢": due_str,
                                    "é‡‘é¢": fields["é‡‘é¢"],
                                    "é‡‡è´­äºº": fields["é‡‡è´­äºº"],
                                    "ä»£ç†æœºæ„": fields["ä»£ç†æœºæ„"],
                                    "è”ç³»äºº": fields["è”ç³»äºº"],
                                    "è”ç³»ç”µè¯": fields["è”ç³»ç”µè¯"],
                                    "åœ°å€": fields["åœ°å€"],
                                    "ç®€è¦æ‘˜è¦": fields["ç®€è¦æ‘˜è¦"],
                                    "æ‰©å±•ä¿¡æ¯": fields.get("æ‰©å±•ä¿¡æ¯", []),
                                    "å…¬å‘Šç½‘å€": url_link or "æš‚æ— ",
                                })
                        else:
                            fields = parse_award_fields(detail_html, detail_text, current_url=url_link)
                            all_award.append({
                                "ç«™ç‚¹": "åŒ—äº¬å…¬å…±èµ„æº",
                                "å…³é”®è¯": kw,
                                "æ ‡é¢˜": title,
                                "å‘å¸ƒæ—¶é—´": pub_time,
                                "ä¿¡æ¯æ¥æº": info_source,
                                "ä¸­æ ‡æ—¥æœŸ": fields.get("ä¸­æ ‡æ—¥æœŸ", pub_date or "æš‚æ— "),
                                "ä¸­æ ‡å…¬å¸": fields.get("ä¸­æ ‡å…¬å¸", "æš‚æ— "),
                                "ä¸­æ ‡é‡‘é¢": fields.get("ä¸­æ ‡é‡‘é¢", "æš‚æ— "),
                                "è¯„å®¡å¾—åˆ†": fields.get("è¯„å®¡å¾—åˆ†", "æš‚æ— "),
                                "ä¸­æ ‡å†…å®¹": fields.get("ä¸­æ ‡å†…å®¹", "æš‚æ— "),
                                "åŸæ‹›æ ‡ç½‘å€": fields.get("åŸæ‹›æ ‡ç½‘å€", "æš‚æ— "),
                                "ä¸­æ ‡ç½‘å€": url_link or "æš‚æ— ",
                            })

                    except Exception as ex:
                        print("è§£æä¸€æ¡å‡ºé”™ï¼š", ex)

                # ç¿»é¡µ
                try:
                    next_btn = driver.find_element(By.LINK_TEXT, "ä¸‹ä¸€é¡µ")
                    if "disable" in (next_btn.get_attribute("class") or "") or next_btn.get_attribute("aria-disabled") == 'true':
                        break
                    if page < max_pages:
                        driver.execute_script("arguments[0].click();", next_btn)
                        time.sleep(1.0)
                except Exception:
                    break

    finally:
        driver.quit()

    return all_bidding, all_award


# ================== ç«™ç‚¹äºŒï¼šzsxtzb.cn èšåˆæœç´¢ ==================
def _zs_search_url(keyword, page=1):
    base = f"https://www.zsxtzb.cn/search?keyword={keyword}"
    if page > 1:
        base += f"&page={page}"
    return base

def _zs_pick_list_items(driver):
    items = []

    # å¸¸è§ li åˆ—è¡¨
    lis = driver.find_elements(By.XPATH, "//div[contains(@class,'search') or contains(@class,'result') or contains(@class,'list')]//li[a]")
    for li in lis:
        try:
            a = li.find_element(By.TAG_NAME, "a")
            title = a.text.strip()
            href = a.get_attribute("href")
            raw = li.text
            dt = _date_in_text(raw)
            if title and href:
                items.append((title, href, dt))
        except Exception:
            pass

    # h3 åˆ—è¡¨
    if not items:
        blocks = driver.find_elements(By.XPATH, "//div[contains(@class,'search') or contains(@class,'result') or contains(@class,'list')]//h3[a]")
        for b in blocks:
            try:
                a = b.find_element(By.TAG_NAME, "a")
                title = a.text.strip()
                href = a.get_attribute("href")
                raw = b.text
                dt = _date_in_text(raw)
                if not dt:
                    try:
                        sib = b.find_element(By.XPATH, "./following-sibling::*[1]")
                        dt = _date_in_text(sib.text)
                    except Exception:
                        pass
                if title and href:
                    items.append((title, href, dt))
            except Exception:
                pass

    # å…œåº•ï¼šæŠ“æ‰€æœ‰é“¾æ¥
    if not items:
        anchors = driver.find_elements(By.XPATH, "//a")
        bad = ["é¦–é¡µ", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ", "æœ«é¡µ", "æ›´å¤š", "ä¸‹è½½", "è¿”å›"]
        for a in anchors:
            try:
                title = (a.text or "").strip()
                href = a.get_attribute("href") or ""
                if not title or not href:
                    continue
                if any(b in title for b in bad):
                    continue
                parent_text = a.find_element(By.XPATH, "./ancestor::*[self::li or self::div][1]").text
                dt = _date_in_text(parent_text)
                items.append((title, href, dt))
            except Exception:
                pass

    # å»é‡ï¼ˆåŒ hrefï¼‰
    uniq = []
    seen = set()
    for t, h, d in items:
        if h in seen:
            continue
        seen.add(h)
        uniq.append((t, h, d))
    return uniq

def _zs_next_page(driver, cur_page):
    for xp in [
        "//a[contains(.,'ä¸‹ä¸€é¡µ') or contains(.,'ä¸‹é¡µ')]",
        "//a[contains(@class,'next')]",
        f"//a[normalize-space(text())='{cur_page+1}']",
        f"//button[normalize-space(text())='{cur_page+1}']"
    ]:
        try:
            el = driver.find_element(By.XPATH, xp)
            driver.execute_script("arguments[0].click();", el)
            time.sleep(1.0)
            return True
        except Exception:
            pass
    return False

def crawl_zsxtzb_search(keywords, max_pages=8, date_start=None, date_end=None):
    driver = _build_driver()
    all_bidding, all_award, seen = [], [], set()

    try:
        for kw in keywords:
            page = 1
            while page <= max_pages:
                url = _zs_search_url(kw, page)
                print(f"[zsxtzb] {kw} ç¬¬{page}é¡µ -> {url}")
                driver.get(url)
                time.sleep(1.4)

                items = _zs_pick_list_items(driver)
                if not items:
                    break

                for title, href, dt in items:
                    ann_type = classify(title)
                    if ann_type not in ("æ‹›æ ‡å…¬å‘Š", "ä¸­æ ‡å…¬å‘Š"):
                        continue
                    if href in seen:
                        continue
                    seen.add(href)

                    pub_date = dt[:10] if dt else ""
                    if date_start and date_end and pub_date:
                        if pub_date < date_start or pub_date > date_end:
                            continue

                    win = driver.current_window_handle
                    driver.execute_script('window.open(arguments[0])', href)
                    driver.switch_to.window(driver.window_handles[-1])
                    time.sleep(1.2)

                    detail_html = driver.page_source
                    detail_text = extract_detail_text_with_pdf_fallback(driver, detail_html, href)

                    driver.close()
                    driver.switch_to.window(win)

                    if ann_type == "æ‹›æ ‡å…¬å‘Š":
                        fields = parse_bidding_fields(detail_text)

                        due_str = fields.get("æŠ•æ ‡æˆªæ­¢", "æš‚æ— ")
                        due_dt  = _to_datetime(due_str if due_str != "æš‚æ— " else "")

                        keep = True
                        now = datetime.now()
                        if SKIP_EXPIRED and due_dt and due_dt < now:
                            keep = False
                        if keep and DUE_FILTER_DAYS > 0 and due_dt and due_dt > now + timedelta(days=DUE_FILTER_DAYS):
                            keep = False

                        if keep:
                            all_bidding.append({
                                "ç«™ç‚¹": "zsxtzbèšåˆ",
                                "å…³é”®è¯": kw,
                                "å…¬å‘Šæ ‡é¢˜": title,
                                "å…¬å‘Šå‘å¸ƒæ—¶é—´": pub_date or "æš‚æ— ",
                                "ä¿¡æ¯æ¥æº": "zsxtzbèšåˆæœç´¢",
                                "æŠ•æ ‡æˆªæ­¢": due_str,
                                "é‡‘é¢": fields["é‡‘é¢"],
                                "é‡‡è´­äºº": fields["é‡‡è´­äºº"],
                                "ä»£ç†æœºæ„": fields["ä»£ç†æœºæ„"],
                                "è”ç³»äºº": fields["è”ç³»äºº"],
                                "è”ç³»ç”µè¯": fields["è”ç³»ç”µè¯"],
                                "åœ°å€": fields["åœ°å€"],
                                "ç®€è¦æ‘˜è¦": fields["ç®€è¦æ‘˜è¦"],
                                "æ‰©å±•ä¿¡æ¯": fields.get("æ‰©å±•ä¿¡æ¯", []),
                                "å…¬å‘Šç½‘å€": href or "æš‚æ— ",
                            })
                    else:
                        fields = parse_award_fields(detail_html, detail_text, current_url=href)
                        all_award.append({
                            "ç«™ç‚¹": "zsxtzbèšåˆ",
                            "å…³é”®è¯": kw,
                            "æ ‡é¢˜": title,
                            "å‘å¸ƒæ—¶é—´": pub_date or "æš‚æ— ",
                            "ä¿¡æ¯æ¥æº": "zsxtzbèšåˆæœç´¢",
                            "ä¸­æ ‡æ—¥æœŸ": fields.get("ä¸­æ ‡æ—¥æœŸ", pub_date or "æš‚æ— "),
                            "ä¸­æ ‡å…¬å¸": fields.get("ä¸­æ ‡å…¬å¸", "æš‚æ— "),
                            "ä¸­æ ‡é‡‘é¢": fields.get("ä¸­æ ‡é‡‘é¢", "æš‚æ— "),
                            "è¯„å®¡å¾—åˆ†": fields.get("è¯„å®¡å¾—åˆ†", "æš‚æ— "),
                            "ä¸­æ ‡å†…å®¹": fields.get("ä¸­æ ‡å†…å®¹", "æš‚æ— "),
                            "åŸæ‹›æ ‡ç½‘å€": fields.get("åŸæ‹›æ ‡ç½‘å€", "æš‚æ— "),
                            "ä¸­æ ‡ç½‘å€": href or "æš‚æ— ",
                        })

                if not _zs_next_page(driver, page):
                    break
                page += 1

    finally:
        driver.quit()

    return all_bidding, all_award


# ================== Markdown è¾“å‡ºï¼ˆæ¸…çˆ½å¡ç‰‡å¼ï¼‰ ==================
def md_escape(s: str) -> str:
    if not isinstance(s, str):
        s = str(s)
    return s.replace("|", "\\|")

def _mk_link(text: str, url: str):
    t = md_escape(text or "")
    u = (url or "").strip()
    return f"[{t}]({u})" if u.startswith("http") else t

def _sort_key_time(s: str):
    """ç”¨äºæ’åºï¼šä¼˜å…ˆæŒ‰ 'YYYY-MM-DD HH:MM' å†æŒ‰ 'YYYY-MM-DD' """
    if not s or s == "æš‚æ— ":
        return datetime(1970, 1, 1)
    ns = _normalize_date_string(s)
    dt = _to_datetime(ns)
    return dt or datetime(1970, 1, 1)

def _dedup_items(items, key_fields):
    """
    å»é‡ï¼šæŒ‰ (ç«™ç‚¹, æ ‡é¢˜, é“¾æ¥) æˆ–ç”¨æˆ·æŒ‡å®šå­—æ®µç»„åˆ
    """
    seen = set()
    out = []
    for it in items:
        key = tuple((it.get(k) or "").strip() for k in key_fields)
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out

def format_bidding_markdown(items, date_start, date_end):
    items = _dedup_items(items, ["ç«™ç‚¹", "å…¬å‘Šæ ‡é¢˜", "å…¬å‘Šç½‘å€"])
    items = sorted(items, key=lambda x: _sort_key_time(x.get("å…¬å‘Šå‘å¸ƒæ—¶é—´")), reverse=True)

    # é¡¶éƒ¨ç»Ÿè®¡
    by_site = {}
    for it in items:
        by_site[it.get("ç«™ç‚¹","æœªçŸ¥")] = by_site.get(it.get("ç«™ç‚¹","æœªçŸ¥"), 0) + 1

    head = f"### ğŸ§¾ã€æ‹›æ ‡å…¬å‘Šã€‘{date_start} ~ {date_end}  å…± {len(items)} æ¡"
    stat = "ï½œ".join([f"{k}:{v}" for k, v in by_site.items()]) if by_site else "æš‚æ— "
    lines = [head, f"> ç«™ç‚¹ç»Ÿè®¡ï¼š{stat}", ""]

    for idx, it in enumerate(items, 1):
        title = it.get("å…¬å‘Šæ ‡é¢˜","")
        url   = it.get("å…¬å‘Šç½‘å€","")
        show  = _mk_link(title, url)

        pub   = it.get("å…¬å‘Šå‘å¸ƒæ—¶é—´","æš‚æ— ")
        due   = it.get("æŠ•æ ‡æˆªæ­¢","æš‚æ— ")
        amt   = it.get("é‡‘é¢","æš‚æ— ")
        pur   = it.get("é‡‡è´­äºº","æš‚æ— ")
        agt   = it.get("ä»£ç†æœºæ„","æš‚æ— ")
        ctc   = it.get("è”ç³»äºº","æš‚æ— ")
        tel   = it.get("è”ç³»ç”µè¯","æš‚æ— ")
        src   = it.get("ä¿¡æ¯æ¥æº","æš‚æ— ")
        site  = it.get("ç«™ç‚¹","æš‚æ— ")
        kw    = it.get("å…³é”®è¯","")

        brief = it.get("ç®€è¦æ‘˜è¦","æš‚æ— ")
        extras = it.get("æ‰©å±•ä¿¡æ¯", []) or []

        lines.append(f"**{idx}. {show}**")
        lines.append(f"- â±ï¸ æˆªæ­¢ï¼š{md_escape(due)}")
        lines.append(f"- ğŸ’° é¢„ç®—/é™ä»·ï¼š{md_escape(amt)}")
        lines.append(f"- ğŸ§© é‡‡è´­äººï¼š{md_escape(pur)}")
        if agt and agt != "æš‚æ— ":
            lines.append(f"- ğŸ¢ ä»£ç†ï¼š{md_escape(agt)}")
        lines.append(f"- ğŸ‘¤ è”ç³»ï¼š{md_escape(ctc)}ï¼ˆ{md_escape(tel)}ï¼‰")
        lines.append(f"- ğŸ—‚ï¸ æ¥æºï¼š{md_escape(site)}ï½œ{md_escape(src)}ï½œå‘å¸ƒï¼š{md_escape(pub)}ï½œå…³é”®è¯ï¼š{md_escape(kw)}")
        lines.append(f"- ğŸ“ æ‘˜è¦ï¼š{md_escape(brief)}")

        for ex in extras[:EXTRA_MAX_LINES]:
            ex = re.sub(r"\s+", " ", ex).strip()
            if ex:
                lines.append(f"- ğŸ” {md_escape(ex)}")
        lines.append("")  # ç©ºè¡Œåˆ†éš”

    return "\n".join(lines).strip()

def format_award_markdown(items, date_start, date_end):
    items = _dedup_items(items, ["ç«™ç‚¹", "æ ‡é¢˜", "ä¸­æ ‡ç½‘å€"])
    items = sorted(items, key=lambda x: _sort_key_time(x.get("å‘å¸ƒæ—¶é—´")), reverse=True)

    by_site = {}
    for it in items:
        by_site[it.get("ç«™ç‚¹","æœªçŸ¥")] = by_site.get(it.get("ç«™ç‚¹","æœªçŸ¥"), 0) + 1

    head = f"### âœ…ã€ä¸­æ ‡/æˆäº¤ç»“æœã€‘{date_start} ~ {date_end}  å…± {len(items)} æ¡"
    stat = "ï½œ".join([f"{k}:{v}" for k, v in by_site.items()]) if by_site else "æš‚æ— "
    lines = [head, f"> ç«™ç‚¹ç»Ÿè®¡ï¼š{stat}", ""]

    for idx, it in enumerate(items, 1):
        title = it.get("æ ‡é¢˜","")
        url   = it.get("ä¸­æ ‡ç½‘å€","")
        show  = _mk_link(title, url)

        awd_date = it.get("ä¸­æ ‡æ—¥æœŸ","æš‚æ— ")
        sup      = it.get("ä¸­æ ‡å…¬å¸","æš‚æ— ")
        amt      = it.get("ä¸­æ ‡é‡‘é¢","æš‚æ— ")
        score    = it.get("è¯„å®¡å¾—åˆ†","æš‚æ— ")
        content  = it.get("ä¸­æ ‡å†…å®¹","æš‚æ— ")

        src   = it.get("ä¿¡æ¯æ¥æº","æš‚æ— ")
        site  = it.get("ç«™ç‚¹","æš‚æ— ")
        pub   = it.get("å‘å¸ƒæ—¶é—´","æš‚æ— ")
        kw    = it.get("å…³é”®è¯","")

        yz = (it.get("åŸæ‹›æ ‡ç½‘å€","") or "").strip()
        yz_line = f"[ç‚¹å‡»è·³è½¬]({yz})" if yz.startswith("http") else "æš‚æ— "

        lines.append(f"**{idx}. {show}**")
        lines.append(f"- ğŸ“… ä¸­æ ‡æ—¥æœŸï¼š{md_escape(awd_date)}")
        lines.append(f"- ğŸ·ï¸ ä¸­æ ‡å•ä½ï¼š{md_escape(sup)}")
        lines.append(f"- ğŸ’° ä¸­æ ‡é‡‘é¢ï¼š{md_escape(amt)}")
        lines.append(f"- ğŸ§® è¯„å®¡å¾—åˆ†ï¼š{md_escape(score)}")
        if content and content != "æš‚æ— ":
            lines.append(f"- ğŸ“Œ ä¸­æ ‡å†…å®¹ï¼š{md_escape(content)}")
        lines.append(f"- ğŸ”— åŸæ‹›æ ‡ç½‘å€ï¼š{yz_line}")
        lines.append(f"- ğŸ—‚ï¸ æ¥æºï¼š{md_escape(site)}ï½œ{md_escape(src)}ï½œå‘å¸ƒï¼š{md_escape(pub)}ï½œå…³é”®è¯ï¼š{md_escape(kw)}")
        lines.append("")

    return "\n".join(lines).strip()

def split_and_send(title_prefix: str, full_text: str, chunk_size=DINGTALK_CHUNK):
    full_text = full_text or ""
    if not full_text.strip():
        return
    n = max(1, math.ceil(len(full_text) / chunk_size))
    for i in range(n):
        part = full_text[i*chunk_size:(i+1)*chunk_size]
        part_title = f"{title_prefix}ï¼ˆ{i+1}/{n}ï¼‰" if n > 1 else title_prefix
        send_to_dingtalk_markdown(part_title, part)


# ================== MAIN ==================
if __name__ == '__main__':
    print("Webhook çŠ¶æ€ï¼š", "å·²é…ç½®ï¼ˆåŠ ç­¾ï¼‰" if (DINGTALK_WEBHOOK and DINGTALK_SECRET) else ("å·²é…ç½®" if DINGTALK_WEBHOOK else "æœªé…ç½®"))
    date_start, date_end = get_date_range()
    print(f"é‡‡é›†æ—¥æœŸï¼š{date_start} ~ {date_end}")

    all_bidding, all_award = [], []

    if CRAWL_BEIJING:
        b1, a1 = crawl_beijing(KEYWORDS, max_pages=10, date_start=date_start, date_end=date_end)
        all_bidding.extend(b1)
        all_award.extend(a1)

    if CRAWL_ZSXTZB:
        b2, a2 = crawl_zsxtzb_search(KEYWORDS, max_pages=8, date_start=date_start, date_end=date_end)
        all_bidding.extend(b2)
        all_award.extend(a2)

    # æ±‡æ€»ï¼ˆæ›´çŸ­æ›´æ¸…æ¥šï¼‰
    # æ‹›æ ‡åªç»Ÿè®¡â€œè¢«ä¿ç•™çš„â€ï¼ˆå·²åšè¿‡æœŸ/æœªæ¥å¤©æ•°è¿‡æ»¤ï¼‰
    sum_text = (
        f"### ğŸ“£ å¤–åŒ…/æ´¾é£é‡‡é›†å®Œæˆ\n"
        f"- æ—¥æœŸï¼š{date_start} ~ {date_end}\n"
        f"- æ‹›æ ‡ï¼š{len(all_bidding)} æ¡\n"
        f"- ä¸­æ ‡/æˆäº¤ï¼š{len(all_award)} æ¡\n"
        f"- è¿‡æ»¤ï¼š{'ä¸¢å¼ƒå·²è¿‡æœŸ' if SKIP_EXPIRED else 'ä¿ç•™å·²è¿‡æœŸ'}ï¼›"
        f"{('ä»…ä¿ç•™æœªæ¥ ' + str(DUE_FILTER_DAYS) + ' å¤©å†…æˆªæ­¢') if DUE_FILTER_DAYS>0 else 'ä¸è¿‡æ»¤æœªæ¥å¤©æ•°'}\n"
    )
    send_to_dingtalk_markdown("å¤–åŒ…/æ´¾é£é‡‡é›†æ±‡æ€»", sum_text)

    # æ˜ç»†ï¼ˆæ¸…çˆ½å¡ç‰‡ï¼‰
    if all_bidding:
        md_bid = format_bidding_markdown(all_bidding, date_start, date_end)
        split_and_send("æ‹›æ ‡å…¬å‘Šæ˜ç»†", md_bid)

    if all_award:
        md_awd = format_award_markdown(all_award, date_start, date_end)
        split_and_send("ä¸­æ ‡ç»“æœæ˜ç»†", md_awd)

    print("âœ” å®Œæˆ")
