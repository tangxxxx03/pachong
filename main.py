# -*- coding: utf-8 -*-
"""
å¤–åŒ…/æ´¾é£ï¼šæ‹›æ ‡ & ä¸­æ ‡é‡‡é›†ï¼ˆåŒ—äº¬å…¬å…±èµ„æºäº¤æ˜“å¹³å° + zsxtzb.cn æœç´¢ï¼‰
â€”â€” é‡‡é›†æ›´å®Œæ•´ï¼ˆrequestsä¼˜å…ˆ+seleniumå…œåº•+PDFé™„ä»¶å›é€€ï¼‰+ è¾“å‡ºæç®€ï¼ˆåªæ¨æ˜ç»†ï¼Œä¸æ¨æ±‡æ€»ï¼‰
"""

import os, re, time, math, hmac, base64, hashlib
from datetime import datetime, timedelta
from io import BytesIO
from urllib.parse import urlparse, urljoin, quote_plus

import requests
import pandas as pd
import pdfplumber
from bs4 import BeautifulSoup

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# ================== å›ºå®šé…ç½®ï¼ˆä¸è¯»ç¯å¢ƒå˜é‡ï¼‰ ==================
DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
DINGTALK_SECRET  = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"  # è‹¥å¼€å¯â€œåŠ ç­¾â€ï¼Œå¡«å…¥å¯†é’¥ï¼›æœªå¼€å¯åˆ™ç•™ç©ºå­—ç¬¦ä¸²

KEYWORDS        = ["å¤–åŒ…", "æ´¾é£"]
CRAWL_BEIJING   = True
CRAWL_ZSXTZB    = True

# åªä¿ç•™æœªæ¥ N å¤©å†…æˆªæ­¢çš„æ‹›æ ‡ï¼›<=0 è¡¨ç¤ºä¸è¿‡æ»¤
DUE_FILTER_DAYS = 30
# ä¸¢å¼ƒå·²è¿‡æœŸçš„æ‹›æ ‡ï¼ˆä»…å½“èƒ½è§£æå‡ºæˆªæ­¢æ—¶é—´ï¼‰
SKIP_EXPIRED    = True

HEADLESS        = True

# è¾“å‡ºæ§åˆ¶ï¼šæ‘˜è¦æˆªæ–­é•¿åº¦ï¼ˆæç®€ï¼‰
BRIEF_MAX_LEN   = 80

# DingTalk å•æ¡ markdown å®‰å…¨é•¿åº¦ï¼ˆç»éªŒå€¼ï¼‰
DINGTALK_CHUNK  = 4200


# ========== HTTP ä¼šè¯ï¼ˆç¦ç”¨ç¯å¢ƒä»£ç† + é‡è¯•ï¼‰ ==========
for _k in ('http_proxy','https_proxy','HTTP_PROXY','HTTPS_PROXY','ALL_PROXY','all_proxy','NO_PROXY'):
    os.environ.pop(_k, None)

_SESSION = requests.Session()
_SESSION.trust_env = False
_retry = Retry(
    total=4,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=frozenset(["GET", "POST"])
)
_SESSION.mount("http://", HTTPAdapter(max_retries=_retry))
_SESSION.mount("https://", HTTPAdapter(max_retries=_retry))
_SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
})


# ================== DingTalk åŠ ç­¾ä¸å‘é€ ==================
def _build_signed_webhook(base_url: str, secret: str) -> str:
    base_url = (base_url or "").strip()
    if not base_url or not secret:
        return base_url
    ts = str(int(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}"
    h = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), hashlib.sha256).digest()
    sign = quote_plus(base64.b64encode(h))
    sep = "&" if ("?" in base_url) else "?"
    return f"{base_url}{sep}timestamp={ts}&sign={sign}"

def send_to_dingtalk_markdown(title: str, md_text: str):
    base_webhook = (DINGTALK_WEBHOOK or "").strip()
    if not base_webhook.startswith("http"):
        print("? Webhook æœªé…ç½®æˆ–æ— æ•ˆ"); return
    final_url = _build_signed_webhook(base_webhook, (DINGTALK_SECRET or "").strip())
    headers = {"Content-Type": "application/json"}
    data = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    try:
        resp = _SESSION.post(final_url, json=data, headers=headers, timeout=15)
        print("é’‰é’‰æ¨é€ï¼š", resp.status_code, resp.text[:180])
    except Exception as e:
        print("? å‘é€é’‰é’‰å¤±è´¥ï¼š", e)


# ================== æ—¥æœŸèŒƒå›´ï¼šé»˜è®¤â€œæ˜¨æ—¥â€ï¼Œå‘¨ä¸€æŠ“å‘¨äº” ==================
def get_date_range():
    today = datetime.now()
    if today.weekday() == 0:
        start = today - timedelta(days=3)
        end   = today - timedelta(days=1)
    else:
        start = today - timedelta(days=1)
        end   = today - timedelta(days=1)
    return start.strftime("%Y-%m-%d"), end.strftime("%Y-%m-%d")


# ================== åˆ†ç±»ï¼ˆä¿æŒä½ åŸé€»è¾‘ï¼‰ ==================
def classify(title: str) -> str:
    t = title or ""
    if any(k in t for k in ["ä¸­æ ‡", "æˆäº¤", "ç»“æœ", "å®šæ ‡", "å€™é€‰äººå…¬ç¤º", "æˆäº¤å…¬å‘Š", "ä¸­æ ‡å…¬å‘Š"]): return "ä¸­æ ‡å…¬å‘Š"
    if any(k in t for k in ["æ›´æ­£", "å˜æ›´", "æ¾„æ¸…", "è¡¥é—"]): return "æ›´æ­£å…¬å‘Š"
    if any(k in t for k in ["ç»ˆæ­¢", "åºŸæ ‡", "æµæ ‡"]): return "ç»ˆæ­¢å…¬å‘Š"
    if any(k in t for k in ["æ‹›æ ‡", "é‡‡è´­", "ç£‹å•†", "é‚€è¯·", "æ¯”é€‰", "è°ˆåˆ¤", "ç«äº‰æ€§", "å…¬å¼€æ‹›æ ‡"]): return "æ‹›æ ‡å…¬å‘Š"
    return "å…¶ä»–"


# ================== æ–‡æœ¬å·¥å…· ==================
def _safe_text(s: str) -> str:
    return (s or "").replace("\u3000", " ").replace("\xa0", " ").strip()

def _date_in_text(s: str):
    if not s: return ""
    m = re.search(r"(20\d{2}[-/.]\d{1,2}[-/.]\d{1,2})", s)
    return m.group(1).replace(".", "-").replace("/", "-") if m else ""

def _normalize_amount_text(s: str) -> str:
    if not s: return ""
    s = str(s).replace("ï¼Œ", ",").replace(",", "")
    s = re.sub(r"\s+", "", s)
    return s

def _normalize_date_string(s: str) -> str:
    if not s: return ""
    s = s.strip()
    s = s.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", " ")
    s = s.replace("/", "-").replace("ï¼š", ":").replace("ï¼", ".")
    s = re.sub(r"\s+", " ", s)

    m = re.search(r"(20\d{2})[-\.](\d{1,2})[-\.](\d{1,2})(?:\s+(\d{1,2}):(\d{2}))?", s)
    if not m:
        return ""
    y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
    hh = int(m.group(4)) if m.group(4) else None
    mm = int(m.group(5)) if m.group(5) else None
    try:
        if hh is not None and mm is not None:
            return datetime(y, mo, d, hh, mm).strftime("%Y-%m-%d %H:%M")
        return datetime(y, mo, d).strftime("%Y-%m-%d")
    except Exception:
        return ""

def _to_datetime(s: str):
    if not s: return None
    for fmt in ("%Y-%m-%d %H:%M", "%Y-%m-%d"):
        try:
            return datetime.strptime(s, fmt)
        except Exception:
            pass
    return None

def _pick_first(text: str, patterns):
    for pat in patterns:
        m = re.search(pat, text, re.S | re.I)
        if m:
            val = m.group(1).strip()
            if val:
                return val
    return ""

def _clean_line(s: str) -> str:
    s = _safe_text(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _soup_text(soup: BeautifulSoup, selector: str) -> str:
    try:
        el = soup.select_one(selector)
        if not el:
            return ""
        return _clean_line(el.get_text("\n", strip=True))
    except Exception:
        return ""


# ================== æˆªæ­¢æ—¶é—´æŠ½å–ï¼ˆæ›´ç¨³ï¼‰ ==================
def extract_deadline(detail_text: str) -> str:
    txt = _safe_text(detail_text)
    pats = [
        r"(?:æŠ•æ ‡(?:æ–‡ä»¶)?|é€’äº¤(?:å“åº”)?æ–‡ä»¶|å“åº”æ–‡ä»¶æäº¤|æŠ¥ä»·|æŠ¥å|è·å–æ‹›æ ‡æ–‡ä»¶)\s*æˆªæ­¢(?:æ—¶é—´|æ—¥æœŸ)\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})",
        r"(?:æäº¤|é€’äº¤)\s*æˆªæ­¢(?:æ—¶é—´|æ—¥æœŸ)\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})",
        r"(?:æˆªæ­¢(?:æ—¶é—´|æ—¥æœŸ))\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})(?=.*?(?:æŠ•æ ‡|é€’äº¤|å“åº”|æŠ¥ä»·|æŠ¥å))",
        r"(?:æˆªæ­¢è‡³)\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})",
    ]
    s = _pick_first(txt, pats)
    norm = _normalize_date_string(s)
    if norm:
        return norm

    s2 = _pick_first(txt, [r"(?:å¼€æ ‡(?:æ—¶é—´|æ—¥æœŸ))\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{6,40})"])
    norm2 = _normalize_date_string(s2)
    return norm2 or ""


# ================== æ‘˜è¦æŠ½å–ï¼ˆæç®€ï¼‰ ==================
def extract_project_brief(detail_text: str, max_len: int = 80) -> str:
    txt = _safe_text(detail_text)

    for pat in [
        r"(?:é¡¹ç›®æ¦‚å†µ|é¡¹ç›®åŸºæœ¬æƒ…å†µ)\s*[:ï¼š]?\s*([\s\S]{0,260}?)\n",
        r"(?:é‡‡è´­éœ€æ±‚|æœåŠ¡èŒƒå›´|æœåŠ¡å†…å®¹|é¡¹ç›®å†…å®¹)\s*[:ï¼š]?\s*([\s\S]{0,260}?)\n",
    ]:
        t = _pick_first(txt, [pat])
        t = _clean_line(t)
        if len(t) >= 12:
            return (t[:max_len] + ("..." if len(t) > max_len else "")).strip()

    plain = _clean_line(txt)
    if not plain:
        return "æš‚æ— "
    return (plain[:max_len] + ("..." if len(plain) > max_len else "")).strip()


# ================== PDF æ–‡æœ¬è¯»å– ==================
def fetch_pdf_text(url: str, referer: str = None, timeout=20) -> str:
    try:
        headers = {"User-Agent":"Mozilla/5.0"}
        if referer:
            headers["Referer"] = referer
        r = _SESSION.get(url, headers=headers, timeout=timeout)
        ct = (r.headers.get("Content-Type") or "").lower()
        if "pdf" not in ct and not url.lower().endswith(".pdf"):
            return ""
        with pdfplumber.open(BytesIO(r.content)) as pdf:
            pages = []
            for p in pdf.pages:
                try:
                    pages.append(p.extract_text() or "")
                except Exception:
                    continue
        return "\n".join([x for x in pages if x.strip()])
    except Exception as e:
        print("PDF è¯»å–å¤±è´¥ï¼š", e)
        return ""


# ================== è¯¦æƒ…é¡µï¼šrequestsä¼˜å…ˆ + seleniumå…œåº• + PDFé™„ä»¶å›é€€ ==================
CONTENT_SELECTORS = [
    "#vsb_content", "#zoom", "#xxnr", "#info",
    "article", "main", ".content", ".article", ".detail", ".cont",
]

def _extract_main_text_from_html(html: str) -> str:
    if not html:
        return ""
    soup = BeautifulSoup(html, "lxml")

    for sel in CONTENT_SELECTORS:
        t = _soup_text(soup, sel)
        if t and len(t) >= 120:
            return t

    body = soup.body.get_text("\n", strip=True) if soup.body else soup.get_text("\n", strip=True)
    return _clean_line(body)

def _extract_pdf_links_from_html(html: str, base_url: str) -> list:
    if not html:
        return []
    soup = BeautifulSoup(html, "lxml")
    pdfs = []
    for a in soup.find_all("a"):
        href = (a.get("href") or "").strip()
        if not href:
            continue
        absu = urljoin(base_url, href)
        if absu.lower().endswith(".pdf"):
            pdfs.append(absu)
    out, seen = [], set()
    for u in pdfs:
        if u in seen:
            continue
        seen.add(u)
        out.append(u)
    return out

def get_detail_text(url: str, driver=None) -> str:
    html = ""
    try:
        r = _SESSION.get(url, timeout=20)
        if r.status_code == 200 and (r.text or "").strip():
            html = r.text
    except Exception:
        html = ""

    text = _extract_main_text_from_html(html)
    if text and len(text) >= 120:
        return text

    if driver is not None:
        try:
            driver.get(url)
            WebDriverWait(driver, 12).until(lambda d: (d.page_source and len(d.page_source) > 2000))
            html2 = driver.page_source
            text2 = _extract_main_text_from_html(html2)
            if text2 and len(text2) >= 120:
                return text2
            html = html2 if html2 else html
        except Exception:
            pass

    for pdf_url in _extract_pdf_links_from_html(html, url)[:3]:
        pdf_text = fetch_pdf_text(pdf_url, referer=url)
        pdf_text = _safe_text(pdf_text)
        if pdf_text and len(pdf_text) >= 120:
            return pdf_text

    return text or _extract_main_text_from_html(html) or ""


# ================== æ‹›æ ‡å­—æ®µè§£æï¼ˆæ›´â€œæŠ“å¾—ä½â€ï¼‰ ==================
def parse_bidding_fields(detail_text: str):
    txt = _safe_text(detail_text)

    amount = _pick_first(txt, [
        r"(?:é¢„ç®—é‡‘é¢|é‡‡è´­é¢„ç®—|é¡¹ç›®é¢„ç®—)\s*[:ï¼š]?\s*([0-9\.,ï¼Œ]+\s*(?:ä¸‡å…ƒ|å…ƒ))",
        r"(?:æœ€é«˜é™ä»·|æ§åˆ¶ä»·)\s*[:ï¼š]?\s*([0-9\.,ï¼Œ]+\s*(?:ä¸‡å…ƒ|å…ƒ))",
    ])
    amount = _normalize_amount_text(amount) if amount else "æš‚æ— "

    purchaser = _pick_first(txt, [r"(?:é‡‡è´­äºº|é‡‡è´­å•ä½|æ‹›æ ‡äºº)\s*[:ï¼š]?\s*([^\n\rï¼Œã€‚;ï¼›]{2,80})"]) or "æš‚æ— "

    contact = "æš‚æ— "
    phone   = "æš‚æ— "
    m_cp = re.search(
        r"(?:é¡¹ç›®è”ç³»äºº|è”ç³»äºº)[ï¼š:\s]*([^\sã€ï¼Œã€‚;ï¼›]+)[\s\S]{0,160}?"
        r"(?:ç”µ\s*è¯|è”ç³»ç”µè¯|è”ç³»æ–¹å¼)[ï¼š:\s]*([0-9\-ï¼â€”\s]{6,})",
        txt, re.S
    )
    if m_cp:
        contact = m_cp.group(1).strip()
        phone = re.sub(r"\s+", "", m_cp.group(2)).replace("ï¼", "-").replace("â€”", "-")
    else:
        c2 = _pick_first(txt, [r"(?:è”ç³»äºº|é¡¹ç›®è”ç³»äºº|é‡‡è´­äººè”ç³»äºº)\s*[:ï¼š]?\s*([^\sã€ï¼Œã€‚;ï¼›]{2,20})"])
        p2 = _pick_first(txt, [r"(?:è”ç³»ç”µè¯|è”ç³»æ–¹å¼|ç”µ\s*è¯)\s*[:ï¼š]?\s*([0-9\-ï¼â€”\s]{6,})"])
        if c2: contact = c2
        if p2: phone = re.sub(r"\s+", "", p2).replace("ï¼", "-").replace("â€”", "-")

    deadline = extract_deadline(txt) or "æš‚æ— "
    brief    = extract_project_brief(txt, max_len=BRIEF_MAX_LEN) or "æš‚æ— "

    return {
        "é‡‘é¢": amount,
        "é‡‡è´­äºº": purchaser,
        "è”ç³»äºº": contact,
        "è”ç³»ç”µè¯": phone,
        "æŠ•æ ‡æˆªæ­¢": deadline,
        "ç®€è¦æ‘˜è¦": brief,
    }


# ================== ä¸­æ ‡è§£æï¼šè¡¨æ ¼ä¼˜å…ˆ + æ–‡æœ¬å…œåº•ï¼ˆè¾“å‡ºä¹Ÿæç®€ï¼‰ ==================
def _num_from_any(v):
    if v in (None, "", "æš‚æ— "): return None
    s = str(v).replace(",", "").replace("ï¼Œ", "")
    m = re.search(r"(-?\d+(?:\.\d+)?)", s)
    return float(m.group(1)) if m else None

def parse_award_from_tables(html: str):
    supplier = amount = score = "æš‚æ— "
    unit = ""

    try:
        tables = pd.read_html(html)
    except Exception:
        tables = []

    rows = []
    for tb in tables:
        t = tb.fillna("").astype(str)
        cols = [str(c) for c in t.columns]
        joined_cols = "".join(cols)
        if not any(k in joined_cols for k in ["ä¾›åº”å•†", "å•ä½åç§°", "ä¸­æ ‡äºº", "æˆäº¤äºº"]):
            continue

        def find_col(keys):
            for k in keys:
                for c in cols:
                    if k in c:
                        return c
            return None

        c_sup = find_col(["ä¾›åº”å•†åç§°", "ä¾›åº”å•†", "å•ä½åç§°", "ä¸­æ ‡äºº", "æˆäº¤äºº"])
        c_sco = find_col(["è¯„å®¡å¾—åˆ†", "ç»¼åˆå¾—åˆ†", "æœ€ç»ˆå¾—åˆ†", "å¾—åˆ†"])
        c_rnk = find_col(["åæ¬¡", "æ’åº", "æ’å"])
        c_pri = find_col(["è¯„å®¡æŠ¥ä»·", "ä¸­æ ‡é‡‘é¢", "æˆäº¤é‡‘é¢", "æŠ¥ä»·", "æŠ•æ ‡æŠ¥ä»·", "é‡‘é¢"])

        for _, r in t.iterrows():
            name = (r.get(c_sup, "") if c_sup else "").strip()
            if not name:
                continue

            price_val = r.get(c_pri) if c_pri else None
            score_val = r.get(c_sco) if c_sco else None
            rank_val  = r.get(c_rnk) if c_rnk else None

            row = {
                "supplier": name,
                "score": _num_from_any(score_val),
                "rank":  _num_from_any(rank_val),
                "price": _num_from_any(price_val),
            }

            if c_pri and ("ä¸‡å…ƒ" in c_pri):
                unit = "ä¸‡å…ƒ"
            if isinstance(price_val, str) and ("ä¸‡å…ƒ" in price_val):
                unit = "ä¸‡å…ƒ"
            if isinstance(price_val, str) and (price_val.strip().endswith("å…ƒ")):
                unit = "å…ƒ"

            rows.append(row)

    chosen = None
    if rows:
        with_score = [r for r in rows if r["score"] is not None]
        if with_score:
            chosen = max(with_score, key=lambda x: x["score"])
        else:
            with_rank = [r for r in rows if r["rank"] is not None]
            if with_rank:
                chosen = min(with_rank, key=lambda x: x["rank"])
            else:
                with_price = [r for r in rows if r["price"] is not None]
                chosen = min(with_price, key=lambda x: x["price"]) if with_price else rows[0]

    if chosen:
        supplier = chosen["supplier"] or "æš‚æ— "
        if chosen["price"] is not None:
            amount = str(chosen["price"])
            if unit:
                amount = f"{amount}{unit}"
        score = str(chosen["score"]) if chosen["score"] is not None else "æš‚æ— "

    return {"ä¸­æ ‡å…¬å¸": supplier, "ä¸­æ ‡é‡‘é¢": amount, "è¯„å®¡å¾—åˆ†": (score or "æš‚æ— ").rstrip("åˆ†")}

def parse_award_from_text(detail_text: str):
    txt = _safe_text(detail_text)
    supplier = _pick_first(txt, [
        r"(?:ä¸­æ ‡(?:ä¾›åº”å•†|äºº|å•ä½)|æˆäº¤(?:ä¾›åº”å•†|äºº|å•ä½)|ä¾›åº”å•†åç§°)\s*[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›;]{2,80})",
        r"(?:æˆäº¤å•ä½)\s*[ï¼š:]\s*([^\n\rï¼Œã€‚ï¼›;]{2,80})",
    ]) or "æš‚æ— "

    amount = _pick_first(txt, [
        r"(?:ä¸­æ ‡(?:ä»·|é‡‘é¢)|æˆäº¤(?:ä»·|é‡‘é¢)|è¯„å®¡æŠ¥ä»·|æˆäº¤ä»·)\s*[ï¼š:]\s*([0-9\.,ï¼Œ]+\s*(?:ä¸‡å…ƒ|å…ƒ)?)",
        r"(?:åˆåŒé‡‘é¢)\s*[ï¼š:]\s*([0-9\.,ï¼Œ]+\s*(?:ä¸‡å…ƒ|å…ƒ)?)",
    ])
    amount = _normalize_amount_text(amount) if amount else "æš‚æ— "

    score = _pick_first(txt, [r"(?:è¯„å®¡(?:å¾—åˆ†|åˆ†å€¼)|ç»¼åˆå¾—åˆ†|æœ€ç»ˆå¾—åˆ†|å¾—åˆ†)\s*[ï¼š:]\s*([0-9\.]+)"])
    score = (score or "æš‚æ— ").rstrip("åˆ†")

    return {"ä¸­æ ‡å…¬å¸": supplier, "ä¸­æ ‡é‡‘é¢": amount, "è¯„å®¡å¾—åˆ†": score}

def parse_award_fields(detail_html: str, detail_text: str):
    data = parse_award_from_tables(detail_html)
    if data.get("ä¸­æ ‡å…¬å¸") == "æš‚æ— " and data.get("ä¸­æ ‡é‡‘é¢") == "æš‚æ— ":
        data = parse_award_from_text(detail_text)

    txt = _safe_text(detail_text or "")
    award_date = _pick_first(txt, [
        r"(?:å…¬å‘Šæ—¥æœŸ|å…¬ç¤ºæ—¶é—´|å‘å¸ƒæ—¶é—´|æˆäº¤æ—¥æœŸ|ä¸­æ ‡æ—¥æœŸ)\s*[ï¼š:]\s*([0-9]{4}[-/.][0-9]{1,2}[-/.][0-9]{1,2})",
    ]) or _date_in_text(txt)
    award_date = _normalize_date_string(award_date) or award_date or "æš‚æ— "
    data["ä¸­æ ‡æ—¥æœŸ"] = award_date
    return data


# -------- Seleniumï¼ˆå®¹å™¨å‹å¥½ï¼‰ --------
def _build_driver():
    opts = Options()
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    if HEADLESS:
        opts.add_argument("--headless=new")
    try:
        driver = webdriver.Chrome(options=opts)  # Selenium Manager
    except Exception:
        from webdriver_manager.chrome import ChromeDriverManager
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)
    driver.implicitly_wait(6)
    driver.set_page_load_timeout(60)
    driver.set_script_timeout(60)
    return driver


# ================== ç«™ç‚¹ä¸€ï¼šåŒ—äº¬å…¬å…±èµ„æºäº¤æ˜“å¹³å° ==================
def crawl_beijing(keywords, max_pages=10, date_start=None, date_end=None):
    driver = _build_driver()
    all_bidding, all_award = [], []
    seen_url = set()

    try:
        for kw in keywords:
            url = f"https://ggzyfw.beijing.gov.cn/elasticsearch/index.jsp?qt={kw}"
            driver.get(url)

            try:
                WebDriverWait(driver, 12).until(EC.presence_of_all_elements_located((By.CLASS_NAME, "cs_search_content_box")))
            except Exception:
                pass

            try:
                driver.find_element(By.XPATH, "//span[contains(text(),'æ—¶é—´ä¸é™')]").click()
                time.sleep(0.4)
                driver.find_element(By.ID, "week").click()
                time.sleep(0.8)
            except Exception:
                pass

            for page in range(1, max_pages + 1):
                try:
                    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, "cs_search_content_box")))
                except Exception:
                    break

                cards = driver.find_elements(By.CLASS_NAME, "cs_search_content_box")
                if not cards:
                    break

                for c in cards:
                    try:
                        title_el = c.find_element(By.CLASS_NAME, "cs_search_title")
                        title = title_el.text.strip()
                        ann_type = classify(title)
                        if ann_type not in ("æ‹›æ ‡å…¬å‘Š", "ä¸­æ ‡å…¬å‘Š"):
                            continue

                        info_source, pub_time = "æš‚æ— ", "æš‚æ— "
                        try:
                            source_line = c.find_element(By.CLASS_NAME, "cs_search_content_time").text
                            if "å‘å¸ƒæ—¶é—´ï¼š" in source_line:
                                parts = source_line.split("å‘å¸ƒæ—¶é—´ï¼š")
                                info_source = parts[0].replace("ä¿¡æ¯æ¥æºï¼š", "").strip() or "æš‚æ— "
                                pub_time = parts[1].strip() or "æš‚æ— "
                        except Exception:
                            pass

                        pub_date = pub_time[:10] if pub_time and pub_time != "æš‚æ— " else ""
                        if date_start and date_end and pub_date:
                            if pub_date < date_start or pub_date > date_end:
                                continue

                        url_link = ""
                        try:
                            url_link = title_el.find_element(By.TAG_NAME, "a").get_attribute("href")
                        except Exception:
                            url_link = ""

                        if not url_link:
                            continue
                        if url_link in seen_url:
                            continue
                        seen_url.add(url_link)

                        detail_text = get_detail_text(url_link, driver=driver)

                        detail_html = ""
                        try:
                            r = _SESSION.get(url_link, timeout=20)
                            if r.status_code == 200:
                                detail_html = r.text or ""
                        except Exception:
                            pass
                        if not detail_html:
                            try:
                                driver.get(url_link)
                                WebDriverWait(driver, 10).until(lambda d: d.page_source and len(d.page_source) > 2000)
                                detail_html = driver.page_source
                            except Exception:
                                detail_html = ""

                        if ann_type == "æ‹›æ ‡å…¬å‘Š":
                            fields = parse_bidding_fields(detail_text)
                            due_str = fields.get("æŠ•æ ‡æˆªæ­¢", "æš‚æ— ")
                            due_dt  = _to_datetime(due_str if due_str != "æš‚æ— " else "")

                            keep = True
                            now = datetime.now()
                            if SKIP_EXPIRED and due_dt and due_dt < now:
                                keep = False
                            if keep and DUE_FILTER_DAYS > 0 and due_dt and due_dt > now + timedelta(days=DUE_FILTER_DAYS):
                                keep = False

                            if keep:
                                all_bidding.append({
                                    "ç«™ç‚¹": "åŒ—äº¬å…¬å…±èµ„æº",
                                    "å…³é”®è¯": kw,
                                    "å…¬å‘Šæ ‡é¢˜": title,
                                    "å…¬å‘Šå‘å¸ƒæ—¶é—´": pub_time,
                                    "ä¿¡æ¯æ¥æº": info_source,
                                    "æŠ•æ ‡æˆªæ­¢": due_str,
                                    "é‡‘é¢": fields["é‡‘é¢"],
                                    "é‡‡è´­äºº": fields["é‡‡è´­äºº"],
                                    "è”ç³»äºº": fields["è”ç³»äºº"],
                                    "è”ç³»ç”µè¯": fields["è”ç³»ç”µè¯"],
                                    "ç®€è¦æ‘˜è¦": fields["ç®€è¦æ‘˜è¦"],
                                    "å…¬å‘Šç½‘å€": url_link,
                                })
                        else:
                            fields = parse_award_fields(detail_html, detail_text)
                            all_award.append({
                                "ç«™ç‚¹": "åŒ—äº¬å…¬å…±èµ„æº",
                                "å…³é”®è¯": kw,
                                "æ ‡é¢˜": title,
                                "å‘å¸ƒæ—¶é—´": pub_time,
                                "ä¿¡æ¯æ¥æº": info_source,
                                "ä¸­æ ‡æ—¥æœŸ": fields.get("ä¸­æ ‡æ—¥æœŸ", pub_date or "æš‚æ— "),
                                "ä¸­æ ‡å…¬å¸": fields.get("ä¸­æ ‡å…¬å¸", "æš‚æ— "),
                                "ä¸­æ ‡é‡‘é¢": fields.get("ä¸­æ ‡é‡‘é¢", "æš‚æ— "),
                                "è¯„å®¡å¾—åˆ†": fields.get("è¯„å®¡å¾—åˆ†", "æš‚æ— "),
                                "ä¸­æ ‡ç½‘å€": url_link,
                            })

                    except Exception as ex:
                        print("è§£æä¸€æ¡å‡ºé”™ï¼š", ex)

                try:
                    next_btn = driver.find_element(By.LINK_TEXT, "ä¸‹ä¸€é¡µ")
                    cls = (next_btn.get_attribute("class") or "")
                    if "disable" in cls or next_btn.get_attribute("aria-disabled") == 'true':
                        break
                    if page < max_pages:
                        driver.execute_script("arguments[0].click();", next_btn)
                        time.sleep(0.9)
                except Exception:
                    break

    finally:
        driver.quit()

    return all_bidding, all_award


# ================== ç«™ç‚¹äºŒï¼šzsxtzb.cn èšåˆæœç´¢ ==================
def _zs_search_url(keyword, page=1):
    base = f"https://www.zsxtzb.cn/search?keyword={keyword}"
    if page > 1:
        base += f"&page={page}"
    return base

def _zs_pick_list_items_from_html(html: str, base_url: str):
    soup = BeautifulSoup(html or "", "lxml")
    items = []

    for a in soup.find_all("a"):
        title = _clean_line(a.get_text(" ", strip=True))
        href = (a.get("href") or "").strip()
        if not title or not href:
            continue
        if len(title) < 6:
            continue
        if any(x in title for x in ["é¦–é¡µ", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ", "æœ«é¡µ", "æ›´å¤š", "ä¸‹è½½", "è¿”å›"]):
            continue
        absu = urljoin(base_url, href)
        if not (absu.startswith("http") and ("/" in urlparse(absu).path)):
            continue

        parent_text = ""
        try:
            parent = a.find_parent(["li", "div", "section"])
            parent_text = parent.get_text(" ", strip=True) if parent else ""
        except Exception:
            parent_text = ""
        dt = _date_in_text(parent_text)
        items.append((title, absu, dt))

    uniq, seen = [], set()
    for t, h, d in items:
        if h in seen:
            continue
        seen.add(h)
        uniq.append((t, h, d))
    return uniq

def crawl_zsxtzb_search(keywords, max_pages=8, date_start=None, date_end=None):
    driver = _build_driver()
    all_bidding, all_award = [], []
    seen_url = set()

    try:
        for kw in keywords:
            for page in range(1, max_pages + 1):
                url = _zs_search_url(kw, page)
                print(f"[zsxtzb] {kw} ç¬¬{page}é¡µ -> {url}")

                html = ""
                try:
                    r = _SESSION.get(url, timeout=20)
                    if r.status_code == 200:
                        html = r.text or ""
                except Exception:
                    html = ""

                if not html.strip():
                    try:
                        driver.get(url)
                        WebDriverWait(driver, 10).until(lambda d: d.page_source and len(d.page_source) > 2000)
                        html = driver.page_source
                    except Exception:
                        html = ""

                items = _zs_pick_list_items_from_html(html, url)
                if not items:
                    break

                for title, href, dt in items:
                    ann_type = classify(title)
                    if ann_type not in ("æ‹›æ ‡å…¬å‘Š", "ä¸­æ ‡å…¬å‘Š"):
                        continue

                    pub_date = dt[:10] if dt else ""
                    if date_start and date_end and pub_date:
                        if pub_date < date_start or pub_date > date_end:
                            continue

                    if href in seen_url:
                        continue
                    seen_url.add(href)

                    detail_text = get_detail_text(href, driver=driver)

                    detail_html = ""
                    try:
                        rr = _SESSION.get(href, timeout=20)
                        if rr.status_code == 200:
                            detail_html = rr.text or ""
                    except Exception:
                        pass
                    if not detail_html:
                        try:
                            driver.get(href)
                            WebDriverWait(driver, 10).until(lambda d: d.page_source and len(d.page_source) > 2000)
                            detail_html = driver.page_source
                        except Exception:
                            detail_html = ""

                    if ann_type == "æ‹›æ ‡å…¬å‘Š":
                        fields = parse_bidding_fields(detail_text)
                        due_str = fields.get("æŠ•æ ‡æˆªæ­¢", "æš‚æ— ")
                        due_dt  = _to_datetime(due_str if due_str != "æš‚æ— " else "")

                        keep = True
                        now = datetime.now()
                        if SKIP_EXPIRED and due_dt and due_dt < now:
                            keep = False
                        if keep and DUE_FILTER_DAYS > 0 and due_dt and due_dt > now + timedelta(days=DUE_FILTER_DAYS):
                            keep = False

                        if keep:
                            all_bidding.append({
                                "ç«™ç‚¹": "zsxtzbèšåˆ",
                                "å…³é”®è¯": kw,
                                "å…¬å‘Šæ ‡é¢˜": title,
                                "å…¬å‘Šå‘å¸ƒæ—¶é—´": pub_date or "æš‚æ— ",
                                "ä¿¡æ¯æ¥æº": "zsxtzbèšåˆæœç´¢",
                                "æŠ•æ ‡æˆªæ­¢": due_str,
                                "é‡‘é¢": fields["é‡‘é¢"],
                                "é‡‡è´­äºº": fields["é‡‡è´­äºº"],
                                "è”ç³»äºº": fields["è”ç³»äºº"],
                                "è”ç³»ç”µè¯": fields["è”ç³»ç”µè¯"],
                                "ç®€è¦æ‘˜è¦": fields["ç®€è¦æ‘˜è¦"],
                                "å…¬å‘Šç½‘å€": href,
                            })
                    else:
                        fields = parse_award_fields(detail_html, detail_text)
                        all_award.append({
                            "ç«™ç‚¹": "zsxtzbèšåˆ",
                            "å…³é”®è¯": kw,
                            "æ ‡é¢˜": title,
                            "å‘å¸ƒæ—¶é—´": pub_date or "æš‚æ— ",
                            "ä¿¡æ¯æ¥æº": "zsxtzbèšåˆæœç´¢",
                            "ä¸­æ ‡æ—¥æœŸ": fields.get("ä¸­æ ‡æ—¥æœŸ", pub_date or "æš‚æ— "),
                            "ä¸­æ ‡å…¬å¸": fields.get("ä¸­æ ‡å…¬å¸", "æš‚æ— "),
                            "ä¸­æ ‡é‡‘é¢": fields.get("ä¸­æ ‡é‡‘é¢", "æš‚æ— "),
                            "è¯„å®¡å¾—åˆ†": fields.get("è¯„å®¡å¾—åˆ†", "æš‚æ— "),
                            "ä¸­æ ‡ç½‘å€": href,
                        })

    finally:
        driver.quit()

    return all_bidding, all_award


# ================== Markdown è¾“å‡ºï¼ˆæç®€å¡ç‰‡å¼ï¼‰ ==================
def md_escape(s: str) -> str:
    if not isinstance(s, str):
        s = str(s)
    return s.replace("|", "\\|")

def _mk_link(text: str, url: str):
    t = md_escape(text or "")
    u = (url or "").strip()
    return f"[{t}]({u})" if u.startswith("http") else t

def _sort_key_time(s: str):
    if not s or s == "æš‚æ— ":
        return datetime(1970, 1, 1)
    ns = _normalize_date_string(s)
    dt = _to_datetime(ns)
    return dt or datetime(1970, 1, 1)

def _merge_by_url(items, url_field, kw_field="å…³é”®è¯"):
    mp = {}
    for it in items:
        u = (it.get(url_field) or "").strip()
        if not u:
            continue
        if u not in mp:
            mp[u] = it
            mp[u][kw_field] = str(it.get(kw_field, "") or "").strip()
        else:
            old = mp[u]
            kws = set([x.strip() for x in (old.get(kw_field, "") or "").split("ï¼Œ") if x.strip()])
            kws2 = set([x.strip() for x in (it.get(kw_field, "") or "").split("ï¼Œ") if x.strip()])
            merged = [x for x in (list(kws | kws2)) if x]
            old[kw_field] = "ï¼Œ".join(sorted(merged))

            t_old = _sort_key_time(old.get("å…¬å‘Šå‘å¸ƒæ—¶é—´") or old.get("å‘å¸ƒæ—¶é—´"))
            t_new = _sort_key_time(it.get("å…¬å‘Šå‘å¸ƒæ—¶é—´") or it.get("å‘å¸ƒæ—¶é—´"))
            if t_new > t_old:
                for k, v in it.items():
                    if k not in (kw_field,):
                        old[k] = v
    return list(mp.values())

def format_bidding_markdown(items, date_start, date_end):
    items = _merge_by_url(items, "å…¬å‘Šç½‘å€", "å…³é”®è¯")
    items = sorted(items, key=lambda x: _sort_key_time(x.get("å…¬å‘Šå‘å¸ƒæ—¶é—´")), reverse=True)

    by_site = {}
    for it in items:
        by_site[it.get("ç«™ç‚¹","æœªçŸ¥")] = by_site.get(it.get("ç«™ç‚¹","æœªçŸ¥"), 0) + 1

    head = f"### ğŸ§¾ã€æ‹›æ ‡å…¬å‘Šã€‘{date_start} ~ {date_end}  å…± {len(items)} æ¡"
    stat = "ï½œ".join([f"{k}:{v}" for k, v in by_site.items()]) if by_site else "æš‚æ— "
    lines = [head, f"> ç«™ç‚¹ç»Ÿè®¡ï¼š{stat}", ""]

    for idx, it in enumerate(items, 1):
        show = _mk_link(it.get("å…¬å‘Šæ ‡é¢˜",""), it.get("å…¬å‘Šç½‘å€",""))
        due  = it.get("æŠ•æ ‡æˆªæ­¢","æš‚æ— ")
        amt  = it.get("é‡‘é¢","æš‚æ— ")
        pur  = it.get("é‡‡è´­äºº","æš‚æ— ")
        ctc  = it.get("è”ç³»äºº","æš‚æ— ")
        tel  = it.get("è”ç³»ç”µè¯","æš‚æ— ")
        pub  = it.get("å…¬å‘Šå‘å¸ƒæ—¶é—´","æš‚æ— ")
        src  = it.get("ä¿¡æ¯æ¥æº","æš‚æ— ")
        site = it.get("ç«™ç‚¹","æš‚æ— ")
        kw   = it.get("å…³é”®è¯","")
        brief = it.get("ç®€è¦æ‘˜è¦","æš‚æ— ")

        lines.append(f"**{idx}. {show}**")
        lines.append(f"- â±ï¸ æˆªæ­¢ï¼š{md_escape(due)}")
        lines.append(f"- ğŸ’° é¢„ç®—/é™ä»·ï¼š{md_escape(amt)}")
        lines.append(f"- ğŸ§© é‡‡è´­äººï¼š{md_escape(pur)}")
        lines.append(f"- ğŸ‘¤ è”ç³»ï¼š{md_escape(ctc)}ï¼ˆ{md_escape(tel)}ï¼‰")
        lines.append(f"- ğŸ—‚ï¸ æ¥æºï¼š{md_escape(site)}ï½œ{md_escape(src)}ï½œå‘å¸ƒï¼š{md_escape(pub)}ï½œå…³é”®è¯ï¼š{md_escape(kw)}")
        lines.append(f"- ğŸ“ æ‘˜è¦ï¼š{md_escape(brief)}")
        lines.append("")

    return "\n".join(lines).strip()

def format_award_markdown(items, date_start, date_end):
    items = _merge_by_url(items, "ä¸­æ ‡ç½‘å€", "å…³é”®è¯")
    items = sorted(items, key=lambda x: _sort_key_time(x.get("å‘å¸ƒæ—¶é—´")), reverse=True)

    by_site = {}
    for it in items:
        by_site[it.get("ç«™ç‚¹","æœªçŸ¥")] = by_site.get(it.get("ç«™ç‚¹","æœªçŸ¥"), 0) + 1

    head = f"### âœ…ã€ä¸­æ ‡/æˆäº¤ç»“æœã€‘{date_start} ~ {date_end}  å…± {len(items)} æ¡"
    stat = "ï½œ".join([f"{k}:{v}" for k, v in by_site.items()]) if by_site else "æš‚æ— "
    lines = [head, f"> ç«™ç‚¹ç»Ÿè®¡ï¼š{stat}", ""]

    for idx, it in enumerate(items, 1):
        show = _mk_link(it.get("æ ‡é¢˜",""), it.get("ä¸­æ ‡ç½‘å€",""))
        awd_date = it.get("ä¸­æ ‡æ—¥æœŸ","æš‚æ— ")
        sup      = it.get("ä¸­æ ‡å…¬å¸","æš‚æ— ")
        amt      = it.get("ä¸­æ ‡é‡‘é¢","æš‚æ— ")
        score    = it.get("è¯„å®¡å¾—åˆ†","æš‚æ— ")
        pub      = it.get("å‘å¸ƒæ—¶é—´","æš‚æ— ")
        src      = it.get("ä¿¡æ¯æ¥æº","æš‚æ— ")
        site     = it.get("ç«™ç‚¹","æš‚æ— ")
        kw       = it.get("å…³é”®è¯","")

        lines.append(f"**{idx}. {show}**")
        lines.append(f"- ğŸ“… ä¸­æ ‡æ—¥æœŸï¼š{md_escape(awd_date)}")
        lines.append(f"- ğŸ·ï¸ ä¸­æ ‡å•ä½ï¼š{md_escape(sup)}")
        lines.append(f"- ğŸ’° ä¸­æ ‡é‡‘é¢ï¼š{md_escape(amt)}")
        if score and score != "æš‚æ— ":
            lines.append(f"- ğŸ§® è¯„å®¡å¾—åˆ†ï¼š{md_escape(score)}")
        lines.append(f"- ğŸ—‚ï¸ æ¥æºï¼š{md_escape(site)}ï½œ{md_escape(src)}ï½œå‘å¸ƒï¼š{md_escape(pub)}ï½œå…³é”®è¯ï¼š{md_escape(kw)}")
        lines.append("")

    return "\n".join(lines).strip()

def split_and_send(title_prefix: str, full_text: str, chunk_size=DINGTALK_CHUNK):
    full_text = full_text or ""
    if not full_text.strip():
        return
    n = max(1, math.ceil(len(full_text) / chunk_size))
    for i in range(n):
        part = full_text[i*chunk_size:(i+1)*chunk_size]
        part_title = f"{title_prefix}ï¼ˆ{i+1}/{n}ï¼‰" if n > 1 else title_prefix
        send_to_dingtalk_markdown(part_title, part)


# ================== MAIN ==================
if __name__ == '__main__':
    date_start, date_end = get_date_range()
    print(f"é‡‡é›†æ—¥æœŸï¼š{date_start} ~ {date_end}")

    all_bidding, all_award = [], []

    if CRAWL_BEIJING:
        b1, a1 = crawl_beijing(KEYWORDS, max_pages=10, date_start=date_start, date_end=date_end)
        all_bidding.extend(b1)
        all_award.extend(a1)

    if CRAWL_ZSXTZB:
        b2, a2 = crawl_zsxtzb_search(KEYWORDS, max_pages=8, date_start=date_start, date_end=date_end)
        all_bidding.extend(b2)
        all_award.extend(a2)

    # âœ… ä¸æ¨æ±‡æ€»ï¼åªæ¨æ˜ç»†ï¼ˆæœ‰å†…å®¹æ‰æ¨ï¼‰
    all_bidding_u = _merge_by_url(all_bidding, "å…¬å‘Šç½‘å€", "å…³é”®è¯")
    all_award_u   = _merge_by_url(all_award, "ä¸­æ ‡ç½‘å€", "å…³é”®è¯")

    if all_bidding_u:
        md_bid = format_bidding_markdown(all_bidding_u, date_start, date_end)
        split_and_send("æ‹›æ ‡å…¬å‘Š", md_bid)

    if all_award_u:
        md_awd = format_award_markdown(all_award_u, date_start, date_end)
        split_and_send("ä¸­æ ‡/æˆäº¤ç»“æœ", md_awd)

    print("âœ” å®Œæˆ")

