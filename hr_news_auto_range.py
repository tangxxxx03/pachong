# -*- coding: utf-8 -*-
"""
hr_news_auto_range.py  ï¼ˆå®Œæ•´ç‰ˆ Â· ä¿®å¤â€œæŠ“ä¸åˆ°â€ï¼‰

å…³é”®æ”¹åŠ¨ï¼š
1) parse_dt_smartï¼šå…¼å®¹ YYYY-MM-DD / YYYY/MM/DD / YYYY.MM.DD / YYYYå¹´MMæœˆDDæ—¥ /
   MM-DD / MM/DD / MM.DD / MæœˆDæ—¥ï¼ˆå¯å¸¦ HH:MMï¼‰ï¼›æ— å¹´ä»½ â†’ ç»“åˆ ref_date.year è¡¥å…¨å¹´ä»½ï¼Œ
   è‹¥è¡¥ä»Šå¹´åè½åœ¨æœªæ¥åˆ™å›é€€ä¸€å¹´ï¼ˆè§£å†³è·¨å¹´åˆ—è¡¨ï¼‰ã€‚
2) â€œæ˜¨æ—¥å…œåº•â€ï¼šè‹¥ä»æœªè§£æåˆ°æ—¥æœŸä¸”å¤„äºâ€œæ˜¨æ—¥ä¸“è¾‘â€æ¨¡å¼ï¼ˆä¼ å…¥ ref_date=æ˜¨æ—¥ï¼‰ï¼Œ
   ä¸´æ—¶èµ‹å€¼ä¸ºâ€œæ˜¨æ—¥ 12:00â€ï¼Œé¿å…è¢«æ—¶é—´çª—å£å…¨éƒ¨åˆ·æ‰ã€‚
3) é€‰æ‹©å™¨åšäº†è½»é‡å…œåº•ï¼›å…¶ä½™é€»è¾‘ä¿æŒä¸å˜ã€‚

ä¾èµ–ï¼š
  pip install requests beautifulsoup4 urllib3
"""

import os, re, time, hmac, base64, hashlib, argparse
from dataclasses import dataclass
from typing import List, Tuple, Optional
from urllib.parse import urljoin, urlparse, quote
from datetime import datetime, timedelta

try:
    from zoneinfo import ZoneInfo
except Exception:  # pragma: no cover
    from backports.zoneinfo import ZoneInfo

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

# ===================== å…¨å±€é…ç½® =====================
TZ = ZoneInfo("Asia/Shanghai")
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
      "AppleWebKit/537.36 (KHTML, like Gecko) "
      "Chrome/123.0.0.0 Safari/537.36")

# â€”â€” äººç¤¾éƒ¨æ ç›® â€”â€”
MOHRSS_BNYW = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/buneiyaowen/"  # éƒ¨å†…è¦é—»
MOHRSS_RSXW = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/rsxw/"        # äººç¤¾æ–°é—»
MOHRSS_DFDT = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/dfdt/"        # åœ°æ–¹åŠ¨æ€
MOHRSS_SECTIONS = [MOHRSS_BNYW, MOHRSS_RSXW, MOHRSS_DFDT]

# â€”â€” å…¬å…±æ‹›è˜ç½‘ï¼ˆä¸»åˆ—è¡¨ï¼‰ â€”â€”
JOB_ZXSS = "http://job.mohrss.gov.cn/zxss/index.jhtml"

# â€”â€” å¼ºåˆ¶ï¼šå¿…é¡»æœ‰æ—¥æœŸï¼ˆåˆ—è¡¨é¡µå¯è§ï¼‰ â€”â€”
REQUIRE_DATE_MOHRSS = True
REQUIRE_DATE_JOB = True

# â€”â€” Debug è¾“å‡ºå¼€å…³ â€”â€”
DEBUG = os.getenv("DEBUG", "").lower() in ("1", "true", "yes", "on")

# â€”â€” é’‰é’‰ï¼ˆA ç‰ˆå˜é‡åï¼›å¯è¢«ç¯å¢ƒè¦†ç›–ï¼‰ â€”â€”
DEFAULT_WEBHOOK = (
    "https://oapi.dingtalk.com/robot/send?"
    "access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
)
DEFAULT_SECRET = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"

def _first_env(*keys: str, default: str = "") -> str:
    for k in keys:
        v = os.getenv(k, "")
        if v and v.strip():
            return v.strip()
    return default

DINGTALK_WEBHOOK = _first_env("DINGTALK_WEBHOOKA", "DINGTALK_BASEA", "WEBHOOKA", default=DEFAULT_WEBHOOK)
DINGTALK_SECRET  = _first_env("DINGTALK_SECRETA",  "SECRETA",        default=DEFAULT_SECRET)

# ===================== HTTP å·¥å…· =====================
def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": UA,
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Accept": "text/html,application/xhtml+xml,*/*;q=0.8",
    })
    retries = Retry(
        total=3,
        backoff_factor=0.6,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "POST"]),
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    s.mount("https://", adapter)
    s.mount("http://", adapter)
    s.trust_env = False
    return s

# ===================== DingTalk æ¨é€ =====================
def _sign_webhook(base_webhook: str, secret: str) -> str:
    if not base_webhook:
        return ""
    if not secret:
        return base_webhook
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = quote(base64.b64encode(hmac_code))
    sep = "&" if "?" in base_webhook else "?"
    return f"{base_webhook}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
    if not webhook:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    try:
        r = requests.post(webhook, json=payload, timeout=20)
        ok = (r.status_code == 200 and isinstance(r.json(), dict) and r.json().get("errcode") == 0)
        print("DingTalk resp:", r.status_code, r.text[:200])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

# ===================== æ—¶é—´è§£æ =====================
def parse_dt_smart(text: str, *, tz=TZ, ref_date=None) -> Optional[datetime]:
    """
    å…¼å®¹ï¼š
      - YYYY-MM-DD / YYYY/MM/DD / YYYY.MM.DD
      - YYYYå¹´MMæœˆDDæ—¥
      - MM-DD / MM/DD / MM.DD
      - MæœˆDæ—¥ / MMæœˆDDæ—¥
      - å¯é€‰ HH:MM
    æ— å¹´ä»½ â†’ ç”¨ ref_date.yearï¼ˆè‹¥è½åœ¨æœªæ¥ â†’ å›é€€ä¸€å¹´ï¼‰
    """
    if not text:
        return None
    s = re.sub(r"\s+", " ", text.strip())

    # 1) å¸¦å¹´ï¼ˆ- / . / /ï¼‰
    m = re.search(r"^(\d{4})[-/.](\d{1,2})[-/.](\d{1,2})(?:\s+(\d{1,2}):(\d{1,2}))?$", s)
    if m:
        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
        hh, mm = (int(m.group(4)), int(m.group(5))) if m.group(4) and m.group(5) else (12, 0)
        return datetime(y, mo, d, hh, mm, tzinfo=tz)

    # 2) å¸¦å¹´ï¼ˆä¸­æ–‡ï¼šYYYYå¹´MMæœˆDDæ—¥ï¼‰
    m = re.search(r"^(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(?:\s+(\d{1,2}):(\d{1,2}))?$", s)
    if m:
        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
        hh, mm = (int(m.group(4)), int(m.group(5))) if m.group(4) and m.group(5) else (12, 0)
        return datetime(y, mo, d, hh, mm, tzinfo=tz)

    # 3) åªæœ‰æœˆæ—¥ï¼ˆ- / . / /ï¼‰
    m = re.search(r"^(\d{1,2})[-/.](\d{1,2})(?:\s+(\d{1,2}):(\d{1,2}))?$", s)
    if m:
        mo, d = int(m.group(1)), int(m.group(2))
        hh, mm = (int(m.group(3)), int(m.group(4))) if m.group(3) and m.group(4) else (12, 0)
        base = datetime.now(tz).date() if ref_date is None else ref_date
        y = base.year
        cand = datetime(y, mo, d, hh, mm, tzinfo=tz)
        if cand.date() > base:  # è·¨å¹´å›é€€
            cand = datetime(y - 1, mo, d, hh, mm, tzinfo=tz)
        return cand

    # 4) åªæœ‰æœˆæ—¥ï¼ˆä¸­æ–‡ï¼šMæœˆDæ—¥ï¼‰
    m = re.search(r"^(\d{1,2})æœˆ(\d{1,2})æ—¥(?:\s+(\d{1,2}):(\d{1,2}))?$", s)
    if m:
        mo, d = int(m.group(1)), int(m.group(2))
        hh, mm = (int(m.group(3)), int(m.group(4))) if m.group(3) and m.group(4) else (12, 0)
        base = datetime.now(tz).date() if ref_date is None else ref_date
        y = base.year
        cand = datetime(y, mo, d, hh, mm, tzinfo=tz)
        if cand.date() > base:
            cand = datetime(y - 1, mo, d, hh, mm, tzinfo=tz)
        return cand

    return None

def day_range(date_str: str) -> Tuple[datetime, datetime]:
    if date_str.lower() == "yesterday":
        base = datetime.now(TZ).date() - timedelta(days=1)
    else:
        base = datetime.strptime(date_str, "%Y-%m-%d").date()
    start = datetime(base.year, base.month, base.day, 0, 0, 0, tzinfo=TZ)
    end   = datetime(base.year, base.month, base.day, 23, 59, 59, tzinfo=TZ)
    return start, end

def auto_range() -> Tuple[datetime, datetime, str]:
    start, end = day_range("yesterday")
    return start, end, "æ˜¨æ—¥ä¸“è¾‘"

# ===================== æ•°æ®ç»“æ„ =====================
@dataclass
class Item:
    title: str
    url: str
    dt: Optional[datetime]
    content: str
    source: str

# ===================== ç«™ç‚¹ 1ï¼šäººç¤¾éƒ¨ =====================
class MohrssList:
    BASE = "https://www.mohrss.gov.cn"

    def __init__(self, session: requests.Session, list_url: str, delay: float = 1.0, ref_date=None):
        self.session = session
        self.list_url = list_url
        self.delay = delay
        self.ref_date = ref_date  # date

    def _fetch(self, url: str) -> str:
        r = self.session.get(url, timeout=20)
        r.encoding = r.apparent_encoding or "utf-8"
        time.sleep(self.delay)
        return r.text

    def _page_url(self, page: int) -> str:
        if page <= 1:
            return self.list_url
        tail = "" if self.list_url.endswith("/") else "/"
        return urljoin(self.list_url, f"{tail}index_{page}.html")

    def parse_list(self, html: str) -> List[Item]:
        soup = BeautifulSoup(html, "html.parser")
        items: List[Item] = []

        # â€”â€” å¸¸è§å®¹å™¨ï¼šdiv.serviceMainListTxtCon â€”â€”
        cards = soup.select("div.serviceMainListTxtCon")
        for card in cards:
            a = card.select_one(".serviceMainListTxtLink a[href]") or card.select_one("a[href]")
            if not a:
                continue
            title = a.get_text(" ", strip=True)
            if not title:
                continue
            url = urljoin(self.BASE, a["href"].strip())

            # æ—¥æœŸï¼šå°è¯•å¤šä¸ªä½ç½® + æ–‡æœ¬å…œåº•
            date_el = (card.select_one(".organMenuTxtLink")
                       or card.select_one(".organGeneralNewListTxtConTime")
                       or card.select_one(".time") or card.select_one(".date"))
            dt_txt = date_el.get_text(" ", strip=True) if date_el else ""
            if not dt_txt:
                # ä»æ•´æ®µæ–‡æœ¬é‡Œå…œåº•æŠ“ä¸€ä¸ªâ€œå¸¦å¹´æˆ–æœˆæ—¥â€çš„ç‰‡æ®µ
                m_any = re.search(r"(20\d{2}[-/.]\d{1,2}[-/.]\d{1,2})|(\d{1,2}[-/.]\d{1,2})|(\d{1,2}æœˆ\d{1,2}æ—¥)", card.get_text(" ", strip=True))
                if m_any:
                    dt_txt = m_any.group(0)

            # â€”â€” è§£æ + æ˜¨æ—¥å…œåº• â€”â€”
            dt = parse_dt_smart(dt_txt, ref_date=self.ref_date)
            if not dt and self.ref_date is not None:
                try:
                    dt = datetime(self.ref_date.year, self.ref_date.month, self.ref_date.day, 12, 0, tzinfo=TZ)
                except Exception:
                    dt = None
            if REQUIRE_DATE_MOHRSS and not dt:
                continue

            items.append(Item(title=title, url=url, dt=dt, content="", source="äººç¤¾éƒ¨"))

        if items:
            return items

        # â€”â€” å…œåº•ï¼štable ç»“æ„ â€”â€”
        rows = soup.select("table tr")
        for tr in rows:
            a = tr.find("a", href=True)
            if not a:
                continue
            title = a.get_text(" ", strip=True)
            if not title:
                continue
            url = urljoin(self.BASE, a["href"].strip())
            tds = tr.find_all("td")
            dt_txt = tds[-1].get_text(" ", strip=True) if len(tds) >= 2 else tr.get_text(" ", strip=True)

            dt = parse_dt_smart(dt_txt, ref_date=self.ref_date)
            if not dt and self.ref_date is not None:
                try:
                    dt = datetime(self.ref_date.year, self.ref_date.month, self.ref_date.day, 12, 0, tzinfo=TZ)
                except Exception:
                    dt = None
            if REQUIRE_DATE_MOHRSS and not dt:
                continue

            items.append(Item(title=title, url=url, dt=dt, content="", source="äººç¤¾éƒ¨(table)"))

        if items:
            return items

        # â€”â€” å…œåº•ï¼šul/li ç»“æ„ â€”â€”
        lis = soup.select("ul li")
        for li in lis:
            a = li.find("a", href=True)
            if not a:
                continue
            title = a.get_text(" ", strip=True)
            if not title:
                continue
            url = urljoin(self.BASE, a["href"].strip())
            dt_txt = ""
            for sel in ["span", "em", ".date", ".time"]:
                node = li.select_one(sel)
                if node:
                    dt_txt = node.get_text(" ", strip=True); break
            if not dt_txt:
                dt_txt = li.get_text(" ", strip=True)

            dt = parse_dt_smart(dt_txt, ref_date=self.ref_date)
            if not dt and self.ref_date is not None:
                try:
                    dt = datetime(self.ref_date.year, self.ref_date.month, self.ref_date.day, 12, 0, tzinfo=TZ)
                except Exception:
                    dt = None
            if REQUIRE_DATE_MOHRSS and not dt:
                continue

            items.append(Item(title=title, url=url, dt=dt, content="", source="äººç¤¾éƒ¨(ul)"))

        return items

    def run(self, pages: int) -> List[Item]:
        all_items: List[Item] = []
        for p in range(1, pages + 1):
            url = self._page_url(p)
            html = self._fetch(url)
            part = self.parse_list(html)
            if not part and p == 1:
                break
            all_items.extend(part)
        return all_items

# ===================== ç«™ç‚¹ 2ï¼šå…¬å…±æ‹›è˜ç½‘ï¼ˆä¸»åˆ—è¡¨+å³ä¾§æ—¥æœŸï¼‰ =====================
class JobZxss:
    BASE = "http://job.mohrss.gov.cn"
    LIST = JOB_ZXSS

    def __init__(self, session: requests.Session, delay: float = 1.0, ref_date=None):
        self.session = session
        self.delay = delay
        self.ref_date = ref_date  # date

    def _fetch(self, url: str) -> str:
        r = self.session.get(url, timeout=20)
        r.encoding = r.apparent_encoding or "utf-8"
        time.sleep(self.delay)
        return r.text

    def _page_url(self, page: int) -> str:
        if page <= 1:
            return self.LIST
        sep = "&" if "?" in self.LIST else "?"
        return self.LIST + f"{sep}pageNo={page}"

    def parse_list(self, html: str) -> List[Item]:
        soup = BeautifulSoup(html, "html.parser")
        items: List[Item] = []

        lis = soup.select("div.zp-listnavbox ul li")
        if not lis:
            lis = [li for li in soup.select("ul li")
                   if li.find("span", class_=re.compile(r"floatright.*gray"))]

        for li in lis:
            a = li.find("a", href=True)
            if not a:
                continue
            title = a.get_text(" ", strip=True)
            if not title:
                continue
            url = urljoin(self.BASE, a["href"].strip())
            host = urlparse(url).netloc.lower()
            if not host.endswith("mohrss.gov.cn"):
                continue

            span = li.find("span", class_=re.compile(r"floatright.*gray"))
            dt_txt = span.get_text(" ", strip=True) if span else ""

            dt = parse_dt_smart(dt_txt, ref_date=self.ref_date)
            if not dt and self.ref_date is not None:
                try:
                    dt = datetime(self.ref_date.year, self.ref_date.month, self.ref_date.day, 12, 0, tzinfo=TZ)
                except Exception:
                    dt = None
            if REQUIRE_DATE_JOB and not dt:
                continue

            items.append(Item(title=title, url=url, dt=dt, content="", source="å…¬å…±æ‹›è˜ç½‘Â·èµ„è®¯"))
        return items

    def run(self, pages: int) -> List[Item]:
        all_items: List[Item] = []
        for p in range(1, pages + 1):
            url = self._page_url(p)
            html = self._fetch(url)
            part = self.parse_list(html)
            if not part and p == 1:
                break
            all_items.extend(part)
        return all_items

# ===================== æ±‡æ€»/è¿‡æ»¤/è¾“å‡º =====================
def dedup_by_url(items: List[Item]) -> List[Item]:
    seen = set(); out: List[Item] = []
    for it in items:
        if it.url and it.url not in seen:
            seen.add(it.url)
            out.append(it)
    return out

def filter_by_range(items: List[Item], start: datetime, end: datetime) -> List[Item]:
    return [it for it in items if it.dt and (start <= it.dt <= end)]

def build_markdown(items: List[Item], title_prefix: str) -> str:
    now_dt = datetime.now(TZ)
    wd = ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][now_dt.weekday()]
    lines = [
        f"**æ—¥æœŸï¼š{now_dt.strftime('%Y-%m-%d')}ï¼ˆ{wd}ï¼‰**",
        "",
        f"**æ ‡é¢˜ï¼š{title_prefix}ï½œäººç¤¾éƒ¨ & å…¬å…±æ‹›è˜ç½‘ï¼ˆå›ºå®šæ ç›®ï¼‰**",
        "",
        "**ä¸»è¦å†…å®¹**",
    ]
    if not items:
        lines.append("> æš‚æ— æ›´æ–°ã€‚")
        return "\n".join(lines)
    for i, it in enumerate(items, 1):
        if it.dt:
            dt_str = it.dt.strftime("%Y-%m-%d %H:%M") if (it.dt.hour or it.dt.minute) else it.dt.strftime("%Y-%m-%d")
        else:
            dt_str = ""
        lines.append(f"{i}. [{it.title}]({it.url})ã€€â€”ã€€*{it.source}*ã€€`{dt_str}`")
        lines.append("")
    return "\n".join(lines)

# ===================== ä¸»æµç¨‹ =====================
def main():
    ap = argparse.ArgumentParser(description="äººç¤¾éƒ¨æ–°é—»/åŠ¨æ€ + å…¬å…±æ‹›è˜ç½‘ï¼ˆæ˜¨æ—¥ & åˆ—è¡¨é¡µæœ‰æ—¥æœŸï¼‰â†’ é’‰é’‰æ¨é€")
    ap.add_argument("--pages", type=int, default=int(os.getenv("PAGES", "2")), help="æ¯ç«™ç¿»é¡µæ•°ï¼ˆé»˜è®¤2ï¼‰")
    ap.add_argument("--delay", type=float, default=float(os.getenv("DELAY", "0.8")), help="è¯·æ±‚é—´éš”ç§’ï¼ˆé»˜è®¤0.8ï¼‰")
    ap.add_argument("--limit", type=int, default=int(os.getenv("LIMIT", "50")), help="å±•ç¤ºä¸Šé™ï¼ˆé»˜è®¤50ï¼‰")
    ap.add_argument("--date", default=os.getenv("DATE", ""), help="æŒ‡å®šæ—¥æœŸï¼ˆyesterday/2025-09-24ï¼‰ï¼›ä¸ºç©ºå¯ç”¨ --auto-range")
    ap.add_argument("--auto-range", default=os.getenv("AUTO_RANGE", "true").lower()=="true",
                    action="store_true", help="å¯ç”¨è‡ªåŠ¨èŒƒå›´ï¼ˆé»˜è®¤â€˜æ˜¨å¤©â€™ï¼‰")
    ap.add_argument("--window-hours", type=int, default=int(os.getenv("WINDOW_HOURS", "48")),
                    help="å½“ä¸å¯ç”¨è‡ªåŠ¨èŒƒå›´æ—¶ä½¿ç”¨æ»šåŠ¨çª—å£å°æ—¶æ•°ï¼ˆé»˜è®¤48ï¼‰")
    ap.add_argument("--no-push", action="store_true", help="åªæ‰“å°ä¸æ¨é€é’‰é’‰")
    args = ap.parse_args()

    session = make_session()

    # æ—¶é—´èŒƒå›´ï¼šä¼˜å…ˆ --dateï¼›å¦åˆ™æ˜¨æ—¥ï¼›å¦åˆ™æ»šåŠ¨çª—å£
    if args.date:
        start, end = day_range(args.date)
        title_prefix = f"{args.date} ä¸“é¢˜"
    elif args.auto_range:
        start, end, title_prefix = auto_range()
    else:
        now = datetime.now(TZ)
        start = now - timedelta(hours=args.window_hours)
        end = now
        title_prefix = f"è¿‘{args.window_hours}å°æ—¶"

    # è§£ææ—¶å‚è€ƒçš„æ—¥æœŸï¼ˆç”¨äºâ€œåªæœ‰æœˆæ—¥â€çš„æƒ…å†µè¡¥å…¨å¹´ä»½æ›´ç¨³å®šï¼‰
    ref_date = start.date()

    # äººç¤¾éƒ¨
    mohrss_items: List[Item] = []
    for url in MOHRSS_SECTIONS:
        mohr = MohrssList(session, url, delay=args.delay, ref_date=ref_date)
        got = mohr.run(args.pages)
        if DEBUG:
            print(f"[DEBUG] MOHRSS: {url} â†’ parsed {len(got)} items (before filter)")
            for x in got[:5]:
                print(f"        Â· {(x.dt.strftime('%Y-%m-%d') if x.dt else 'NO-DATE')} | {x.title[:60]}")
        mohrss_items.extend(got)

    # å…¬å…±æ‹›è˜ç½‘
    job = JobZxss(session, delay=args.delay, ref_date=ref_date)
    job_items = job.run(args.pages)
    if DEBUG:
        print(f"[DEBUG] JOB.ZXSS â†’ parsed {len(job_items)} items (before filter)")
        for x in job_items[:5]:
            print(f"        Â· {(x.dt.strftime('%Y-%m-%d') if x.dt else 'NO-DATE')} | {x.title[:60]}")

    all_items_raw = mohrss_items + job_items
    print(f"âœ… åŸå§‹æŠ“å– {len(all_items_raw)} æ¡ï¼ˆæœªå»é‡/æœªè¿‡æ»¤ï¼‰")

    # å»é‡ + å‘½ä¸­æ—¶é—´çª—å£
    all_items = dedup_by_url(all_items_raw)
    kept = filter_by_range(all_items, start, end)
    if DEBUG:
        print(f"[DEBUG] Time window: {start.strftime('%Y-%m-%d %H:%M:%S')} ~ {end.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"[DEBUG] After time-filter: {len(kept)} items")

    # æ’åº + æˆªæ–­
    kept.sort(key=lambda x: (x.dt or datetime(1970,1,1, tzinfo=TZ)), reverse=True)
    show = kept[:args.limit] if args.limit and args.limit > 0 else kept

    md = build_markdown(show, title_prefix)
    print("\n--- Markdown Preview ---\n")
    print(md)

    # è½ç›˜
    try:
        with open("hr_news.md", "w", encoding="utf-8") as f:
            f.write(md)
    except Exception as e:
        print("write md error:", e)

    if not args.no_push:
        ok = send_dingtalk_markdown(f"{title_prefix}ï½œäººç¤¾éƒ¨&å…¬å…±æ‹›è˜ç½‘ï¼ˆå›ºå®šæ ç›®ï¼‰", md)
        print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥/æœªæ¨é€ âŒ")

if __name__ == "__main__":
    main()
