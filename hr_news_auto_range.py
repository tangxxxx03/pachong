# -*- coding: utf-8 -*-
"""
hr_news_detail_first.py  ï¼ˆå…¨æ–°æ–¹æ¡ˆï¼šè¯¦æƒ…é¡µå–å‘å¸ƒæ—¶é—´ï¼‰
ç›®æ ‡ç«™ç‚¹ï¼ˆå›ºå®šæ¥æºï¼‰ï¼š
  1) äººç¤¾éƒ¨æ–°é—»/åŠ¨æ€ï¼šéƒ¨å†…è¦é—» / äººç¤¾æ–°é—» / åœ°æ–¹åŠ¨æ€
  2) ä¸­å›½å…¬å…±æ‹›è˜ç½‘ï¼šèµ„è®¯é¦–é¡µä¸»åˆ—è¡¨

æ ¸å¿ƒæ€è·¯ï¼š
- åˆ—è¡¨é¡µåªè´Ÿè´£â€œå‘ç°é“¾æ¥â€ï¼›çœŸå®å‘å¸ƒæ—¶é—´ä»â€œè¯¦æƒ…é¡µâ€è§£æï¼ˆæ­£æ–‡æ—¶é—´ã€metaã€URLã€Last-Modified å¤šé‡å…œåº•ï¼‰ã€‚
- æ—¶é—´è¿‡æ»¤ä¼˜å…ˆä½¿ç”¨ --window-hoursï¼ˆé»˜è®¤ 48 å°æ—¶ï¼‰ï¼›ä¹Ÿæ”¯æŒ --date=yesterday åšâ€œæ˜¨æ—¥ä¸“è¾‘â€ã€‚
- å½»åº•è§„é¿åˆ—è¡¨é¡µæ—¥æœŸç¼ºå¤±/æ ¼å¼äº”èŠ±å…«é—¨å¯¼è‡´çš„â€œå…¨è¢«åˆ·æ‰â€ã€‚

ä¾èµ–ï¼š
  pip install requests beautifulsoup4 urllib3
"""

import os, re, time, hmac, base64, hashlib, argparse
from dataclasses import dataclass
from typing import List, Tuple, Optional, Iterable
from urllib.parse import urljoin, urlparse, quote
from datetime import datetime, timedelta

try:
    from zoneinfo import ZoneInfo
except Exception:  # pragma: no cover
    from backports.zoneinfo import ZoneInfo

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

# =============== å…¨å±€é…ç½® ===============
TZ = ZoneInfo("Asia/Shanghai")
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
      "(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36")

MOHRSS_BNYW = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/buneiyaowen/"
MOHRSS_RSXW = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/rsxw/"
MOHRSS_DFDT = "https://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/dfdt/"
MOHRSS_SECTIONS = [MOHRSS_BNYW, MOHRSS_RSXW, MOHRSS_DFDT]

JOB_ZXSS = "http://job.mohrss.gov.cn/zxss/index.jhtml"

DEBUG = os.getenv("DEBUG", "").lower() in ("1", "true", "yes", "on")

# é’‰é’‰ï¼ˆAç‰ˆå˜é‡åï¼Œä¹Ÿæ¥å—é»˜è®¤å€¼ï¼‰
DEFAULT_WEBHOOK = (
    "https://oapi.dingtalk.com/robot/send?"
    "access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
)
DEFAULT_SECRET = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"

def _first_env(*keys: str, default: str = "") -> str:
    for k in keys:
        v = os.getenv(k, "")
        if v and v.strip():
            return v.strip()
    return default

DINGTALK_WEBHOOK = _first_env("DINGTALK_WEBHOOKA", "DINGTALK_BASEA", "WEBHOOKA", default=DEFAULT_WEBHOOK)
DINGTALK_SECRET  = _first_env("DINGTALK_SECRETA",  "SECRETA",        default=DEFAULT_SECRET)

# =============== HTTP å·¥å…· ===============
def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": UA,
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Accept": "text/html,application/xhtml+xml,*/*;q=0.8",
    })
    retries = Retry(
        total=3,
        backoff_factor=0.6,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "HEAD"]),
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    s.mount("https://", adapter)
    s.mount("http://", adapter)
    s.trust_env = False
    return s

# =============== DingTalk ===============
def _sign_webhook(base_webhook: str, secret: str) -> str:
    if not base_webhook:
        return ""
    if not secret:
        return base_webhook
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = quote(base64.b64encode(hmac_code))
    sep = "&" if "?" in base_webhook else "?"
    return f"{base_webhook}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
    if not webhook:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    try:
        r = requests.post(webhook, json={"msgtype":"markdown","markdown":{"title":title,"text":md_text}}, timeout=20)
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        if DEBUG:
            print("DingTalk resp:", r.status_code, r.text[:200])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

# =============== æ—¶é—´å·¥å…· ===============
DATE_PATTS = [
    r"(\d{4})[-/.](\d{1,2})[-/.](\d{1,2})\s*(\d{1,2}):(\d{1,2})",
    r"(\d{4})[-/.](\d{1,2})[-/.](\d{1,2})",
    r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥\s*(\d{1,2}):(\d{1,2})",
    r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",
]
MONTHDAY_PATTS = [
    r"(\d{1,2})[-/.](\d{1,2})",
    r"(\d{1,2})æœˆ(\d{1,2})æ—¥",
]

def build_dt(y:int, m:int, d:int, hh:int=12, mm:int=0) -> datetime:
    return datetime(y, m, d, hh, mm, tzinfo=TZ)

def parse_any_datetime(text: str, *, ref_date: Optional[datetime]=None) -> Optional[datetime]:
    if not text:
        return None
    s = re.sub(r"\s+", " ", text.strip())
    # å¸¦å¹´
    for p in DATE_PATTS:
        m = re.search(p, s)
        if m:
            g = [int(x) for x in m.groups() if x]
            if len(g) == 5:
                y, mo, d, hh, mm = g
                return build_dt(y, mo, d, hh, mm)
            elif len(g) == 3:
                y, mo, d = g
                return build_dt(y, mo, d)
    # æœˆæ—¥
    base = (ref_date or datetime.now(TZ))
    for p in MONTHDAY_PATTS:
        m = re.search(p, s)
        if m:
            mo, d = int(m.group(1)), int(m.group(2))
            cand = build_dt(base.year, mo, d)
            if cand > base:
                cand = build_dt(base.year - 1, mo, d)
            return cand
    return None

def day_range(date_str: str) -> Tuple[datetime, datetime]:
    if date_str.lower() == "yesterday":
        base = datetime.now(TZ).date() - timedelta(days=1)
    else:
        base = datetime.strptime(date_str, "%Y-%m-%d").date()
    return (datetime(base.year, base.month, base.day, 0,0,0, tzinfo=TZ),
            datetime(base.year, base.month, base.day, 23,59,59, tzinfo=TZ))

# =============== æ•°æ®ç»“æ„ ===============
@dataclass
class Item:
    title: str
    url: str
    dt: Optional[datetime]
    source: str

# =============== è§£æå™¨ï¼ˆè¯¦æƒ…ä¼˜å…ˆï¼‰ ===============
def discover_links_from_mohrss_list(html: str, base: str) -> Iterable[Tuple[str,str]]:
    soup = BeautifulSoup(html, "html.parser")
    # è¿™äº›å®¹å™¨é‡Œä¸€èˆ¬å°±æ˜¯æ–°é—»åˆ—è¡¨
    containers = soup.select("div.serviceMainListTxtCon, ul, table")
    seen = set()
    for root in containers:
        for a in root.find_all("a", href=True):
            href = a["href"].strip()
            if any(x in href for x in ("javascript:", "#")):
                continue
            url = urljoin(base, href)
            if url in seen: 
                continue
            seen.add(url)
            title = a.get_text(" ", strip=True)
            if title:
                yield title, url

def discover_links_from_job_list(html: str, base: str) -> Iterable[Tuple[str,str]]:
    soup = BeautifulSoup(html, "html.parser")
    seen = set()
    # ä¸»åˆ—è¡¨
    for li in soup.select("div.zp-listnavbox ul li"):
        a = li.find("a", href=True)
        if not a: 
            continue
        url = urljoin(base, a["href"].strip())
        if url in seen: 
            continue
        seen.add(url)
        title = a.get_text(" ", strip=True)
        if title:
            yield title, url

def extract_publish_dt_from_detail(html: str, url: str) -> Optional[datetime]:
    soup = BeautifulSoup(html, "html.parser")
    text_blocks = []

    # 1) ç›´æ¥çœ‹å¸¸è§æ—¶é—´èŠ‚ç‚¹
    cand_nodes = soup.select("time, .time, .date, .pubtime, .publish-time, .source, .info, .article-info, .xxgk-info")
    for n in cand_nodes:
        text_blocks.append(n.get_text(" ", strip=True))

    # 2) meta
    for sel in ["meta[name='PubDate']", "meta[name='publishdate']", "meta[property='article:published_time']",
                "meta[name='weibo: article:create_at']", "meta[name='releaseDate']"]:
        m = soup.select_one(sel)
        if m and m.get("content"):
            text_blocks.append(m["content"])

    # 3) æ ‡é¢˜æ /æ­£æ–‡é¦–æ®µ
    header = soup.select_one("h1, .title, .articleTitle")
    if header:
        text_blocks.append(header.get_text(" ", strip=True))
    body = soup.select_one("article, .article, .TRS_Editor, .content, #content")
    if body:
        text_blocks.append(body.get_text(" ", strip=True)[:400])  # å–å‰ä¸€æ®µ

    # 4) URL ä¸­çš„æ—¥æœŸ
    url_txt = url
    text_blocks.append(url_txt.replace("/", " ").replace("_", " "))

    # 5) å“åº”å¤´ï¼ˆLast-Modifiedï¼‰ä¼šåœ¨ fetch æ—¶ä¼ å…¥ï¼ˆç”±å¤–å±‚è¡¥ï¼‰
    # è¿™é‡Œåªç•™é’©å­ï¼šå¦‚æœä¸Šå±‚ä¼ äº†ï¼Œä¼šåŠ åˆ° blocks é‡Œ
    # -> é€šè¿‡è¿”å› None è®©ä¸Šå±‚å…œåº•å¤„ç†

    # ç»Ÿä¸€æ‹¼æˆä¸€ä¸ªå¤§å­—ç¬¦ä¸²å»åŒ¹é…
    big = " | ".join([t for t in text_blocks if t])
    dt = parse_any_datetime(big, ref_date=datetime.now(TZ))
    return dt

# =============== æŠ“å–ä¸»æµç¨‹ï¼ˆè¯¦æƒ…ä¼˜å…ˆï¼‰ ===============
def fetch_list_and_details(session: requests.Session, list_url: str, pages: int, base: str, site: str,
                           discover_fn) -> List[Item]:
    items: List[Item] = []
    for p in range(1, pages + 1):
        # ç¿»é¡µï¼šäººç¤¾éƒ¨ index_{p}.htmlï¼›å…¬å…±æ‹›è˜ç½‘ ?pageNo=p
        if "mohrss.gov.cn" in list_url:
            url = list_url if p == 1 else urljoin(list_url, f"index_{p}.html")
        else:
            if p == 1: url = list_url
            else:
                sep = "&" if "?" in list_url else "?"
                url = list_url + f"{sep}pageNo={p}"

        r = session.get(url, timeout=20)
        r.encoding = r.apparent_encoding or "utf-8"
        html = r.text

        if DEBUG: print(f"[DEBUG] list {site} p{p} len={len(html)}")

        for title, link in discover_fn(html, base):
            # è¯¦æƒ…é¡µ
            try:
                rr = session.get(link, timeout=20, headers={"Referer": url})
                rr.encoding = rr.apparent_encoding or "utf-8"
                dt = extract_publish_dt_from_detail(rr.text, link)

                # å…œåº•ï¼šLast-Modified
                if not dt:
                    lm = rr.headers.get("Last-Modified", "") or rr.headers.get("last-modified","")
                    if lm:
                        try:
                            dt = datetime.fromtimestamp(time.mktime(time.strptime(lm, "%a, %d %b %Y %H:%M:%S %Z")), tz=TZ)
                        except Exception:
                            pass

                items.append(Item(title=title, url=link, dt=dt, source=site))
                time.sleep(0.5)
            except Exception as e:
                if DEBUG: print("[DEBUG] detail error:", e)
                continue
    return items

def dedup(items: List[Item]) -> List[Item]:
    seen = set(); out=[]
    for it in items:
        k = it.url.split("#")[0]
        if k in seen: 
            continue
        seen.add(k)
        out.append(it)
    return out

def filter_by_time(items: List[Item], start: datetime, end: datetime, allow_nodate: bool=False) -> List[Item]:
    kept=[]
    for it in items:
        if it.dt:
            if start <= it.dt <= end:
                kept.append(it)
        elif allow_nodate:
            kept.append(it)
    return kept

def build_markdown(items: List[Item], title_prefix: str) -> str:
    now_dt = datetime.now(TZ)
    wd = ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][now_dt.weekday()]
    lines = [
        f"**æ—¥æœŸï¼š{now_dt.strftime('%Y-%m-%d')}ï¼ˆ{wd}ï¼‰**",
        "",
        f"**æ ‡é¢˜ï¼š{title_prefix}ï½œäººç¤¾éƒ¨ & å…¬å…±æ‹›è˜ç½‘ï¼ˆå›ºå®šæ ç›®ï¼‰**",
        "",
        "**ä¸»è¦å†…å®¹**",
    ]
    if not items:
        lines.append("> æš‚æ— æ›´æ–°ã€‚")
        return "\n".join(lines)

    for i, it in enumerate(items, 1):
        ds = it.dt.strftime("%Y-%m-%d %H:%M") if it.dt else "ï¼ˆæ—¶é—´æœªçŸ¥ï¼‰"
        lines.append(f"{i}. [{it.title}]({it.url})  â€”  *{it.source}*  `{ds}`")
        lines.append("")
    return "\n".join(lines)

# =============== CLI ===============
def main():
    ap = argparse.ArgumentParser(description="äººç¤¾éƒ¨ + å…¬å…±æ‹›è˜ç½‘ï¼ˆè¯¦æƒ…ä¼˜å…ˆè§£æå‘å¸ƒæ—¶é—´ï¼‰â†’ é’‰é’‰æ¨é€")
    ap.add_argument("--pages", type=int, default=int(os.getenv("PAGES","2")))
    ap.add_argument("--delay", type=float, default=float(os.getenv("DELAY","0.6")))
    ap.add_argument("--limit", type=int, default=int(os.getenv("LIMIT","50")))
    ap.add_argument("--date", default=os.getenv("DATE",""), help="yesterday / YYYY-MM-DD")
    ap.add_argument("--auto-range", default=os.getenv("AUTO_RANGE","").lower()=="true", action="store_true")
    ap.add_argument("--window-hours", type=int, default=int(os.getenv("WINDOW_HOURS","48")),
                    help="æ»šåŠ¨çª—å£å°æ—¶æ•°ï¼Œé»˜è®¤48ï¼›å½“æœªè®¾ç½® --date ä¸”æœªå¼€å¯ --auto-range æ—¶ç”Ÿæ•ˆ")
    ap.add_argument("--allow-nodate", action="store_true", help="å…è®¸æ— æ—¥æœŸçš„æ¡ç›®è¿›å…¥ï¼ˆæç«¯å…œåº•ï¼‰")
    ap.add_argument("--no-push", action="store_true")
    args = ap.parse_args()

    session = make_session()

    # æ—¶é—´çª—å£
    if args.date:
        start, end = day_range(args.date)
        title_prefix = f"{args.date} ä¸“é¢˜"
    elif args.auto_range:
        start, end = day_range("yesterday")
        title_prefix = "æ˜¨æ—¥ä¸“è¾‘"
    else:
        now = datetime.now(TZ)
        start, end = (now - timedelta(hours=args.window_hours)), now
        title_prefix = f"è¿‘{args.window_hours}å°æ—¶"

    # â€”â€” æŠ“å–ï¼ˆè¯¦æƒ…ä¼˜å…ˆï¼‰
    mohrss_items: List[Item] = []
    for url in MOHRSS_SECTIONS:
        got = fetch_list_and_details(session, url, args.pages, base="https://www.mohrss.gov.cn", site="äººç¤¾éƒ¨",
                                     discover_fn=discover_links_from_mohrss_list)
        if DEBUG: print(f"[DEBUG] MOHRSS got {len(got)}")
        mohrss_items.extend(got)
        time.sleep(args.delay)

    job_items = fetch_list_and_details(session, JOB_ZXSS, args.pages, base="http://job.mohrss.gov.cn",
                                       site="å…¬å…±æ‹›è˜ç½‘Â·èµ„è®¯", discover_fn=discover_links_from_job_list)
    if DEBUG: print(f"[DEBUG] JOB got {len(job_items)}")

    all_items = dedup(mohrss_items + job_items)
    if DEBUG: print(f"[DEBUG] merged {len(all_items)}")

    kept = filter_by_time(all_items, start, end, allow_nodate=args.allow_nodate)
    kept.sort(key=lambda x: (x.dt or datetime(1970,1,1, tzinfo=TZ)), reverse=True)
    if args.limit > 0:
        kept = kept[:args.limit]

    md = build_markdown(kept, title_prefix)
    print("\n--- Markdown Preview ---\n")
    print(md)

    # ä¿å­˜
    try:
        with open("hr_news.md", "w", encoding="utf-8") as f:
            f.write(md)
    except Exception as e:
        print("write md error:", e)

    if not args.no_push:
        ok = send_dingtalk_markdown(f"{title_prefix}ï½œäººç¤¾éƒ¨&å…¬å…±æ‹›è˜ç½‘ï¼ˆå›ºå®šæ ç›®ï¼‰", md)
        print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥/æœªæ¨é€ âŒ")

if __name__ == "__main__":
    main()
