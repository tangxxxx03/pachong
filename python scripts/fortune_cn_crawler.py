# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆç»“æ„ï¼‰- å¢å¼ºç‰ˆ

åŠŸèƒ½ï¼š
1. æ”¯æŒå¤šé¡µåˆ—è¡¨æŠ“å–ï¼ˆé»˜è®¤å‰ 3 é¡µï¼‰ã€‚
2. è‡ªåŠ¨è·å–æ¯ç¯‡æ–‡ç« çš„å®Œæ•´æ­£æ–‡ã€‚
3. å°†æ‰€æœ‰æ–‡ç« æ•°æ®ï¼ˆæ ‡é¢˜ã€é“¾æ¥ã€æ—¥æœŸã€æ­£æ–‡ï¼‰æ”¶é›†èµ·æ¥ã€‚
"""

import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# --- é…ç½®å‚æ•° ---
BASE = "https://www.fortunechina.com"
MAX_PAGES = 3  # è®¾ç½®æ‚¨å¸Œæœ›æŠ“å–çš„æœ€å¤§é¡µæ•°
# ----------------

session = requests.Session()
session.headers.update({
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120 Safari/537.36"
    ),
    "Accept-Encoding": "gzip, deflate, br",
})


def fetch_list(page=1):
    """
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ã€‚
    """
    # è´¢å¯Œä¸­æ–‡ç½‘çš„åˆ—è¡¨é¡µ URL è§„å¾‹
    url = f"{BASE}/shangye/" if page == 1 else f"{BASE}/shangye/?page={page}"
    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ---")

    try:
        r = session.get(url, timeout=15)
        r.raise_for_status() # æ£€æŸ¥ HTTP çŠ¶æ€ç 
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥ ({url}): {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []
    
    # PC ç‰ˆæ ¸å¿ƒé€‰æ‹©å™¨ï¼šul.news-list li
    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")
        
        # ç¡®ä¿å…³é”®å…ƒç´ å­˜åœ¨
        if not (h2 and a):
            continue
            
        href = a["href"].strip()
        
        # ä»…æŠ“å–ç¬¦åˆæ–‡ç« é“¾æ¥æ ¼å¼çš„ URL (ä¾‹å¦‚: /2025-12/07/content_470761.htm)
        if not re.search(r"/\d{4}-\d{2}/\d{2}/content_\d+\.htm", href):
            continue

        title = h2.get_text(strip=True)
        url_full = urljoin(BASE, href)
        pub_date = date_div.get_text(strip=True) if date_div else ""

        items.append({
            "title": title,
            "url": url_full,
            "date": pub_date,
            "content": "", # é¢„ç•™å­—æ®µï¼Œç¨åå¡«å……æ­£æ–‡
        })

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°æ–‡ç« æ•°ï¼š{len(items)}")
    return items


def fetch_article_content(item):
    """
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œå¹¶æ›´æ–° item å­—å…¸ã€‚
    """
    url = item["url"]
    # print(f"  -> è¯·æ±‚æ­£æ–‡: {url}") # æ³¨é‡Šæ‰ï¼Œé¿å…è¿‡å¤šè¾“å‡º

    try:
        r = session.get(url, timeout=15)
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"  âš ï¸ æ­£æ–‡è¯·æ±‚å¤±è´¥ ({url}): {e}")
        item["content"] = f"[æ­£æ–‡è·å–å¤±è´¥: {e}]"
        return

    soup = BeautifulSoup(r.text, "html.parser")
    # PC ç‰ˆæ­£æ–‡æ ¸å¿ƒé€‰æ‹©å™¨ï¼šdiv.article-mod div.word-text-con
    container = soup.select_one("div.article-mod div.word-text-con")

    if not container:
        item["content"] = "[æ­£æ–‡å†…å®¹å®¹å™¨æœªæ‰¾åˆ°]"
        return

    # æå–æ‰€æœ‰ <p> æ ‡ç­¾å¹¶æ‹¼æ¥æˆå®Œæ•´æ­£æ–‡
    paras = [p.get_text(strip=True) for p in container.find_all("p") if p.get_text(strip=True)]
    
    # å°†æ­£æ–‡å­˜å‚¨å› item å­—å…¸
    item["content"] = "\n".join(paras)
    
    # ç¨å¾®åœé¡¿ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
    time.sleep(0.5)


def main():
    all_articles = []
    
    print(f"=== ğŸš€ è´¢å¯Œä¸­æ–‡ç½‘çˆ¬è™«å¼€å§‹æ‰§è¡Œï¼ˆç›®æ ‡é¡µæ•°ï¼š{MAX_PAGES}ï¼‰ ===")

    # 1. å®ç°å¤šé¡µæŠ“å–å¾ªç¯
    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)
        
        # å¦‚æœæŸä¸€é¡µæ²¡æœ‰æŠ“åˆ°æ–‡ç« ï¼Œåˆ™åœæ­¢ï¼ˆå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µï¼‰
        if not list_items:
            print(f"--- ç¬¬ {page} é¡µæ²¡æœ‰æŠ“åˆ°æ–‡ç« ï¼Œåœæ­¢ç¿»é¡µ ---")
            break
            
        all_articles.extend(list_items)
        # ç¿»é¡µä¹‹é—´å»ºè®®æœ‰ä¸€ä¸ªç¨é•¿çš„ç­‰å¾…ï¼Œé¿å…è¢«å°
        time.sleep(1) 
    
    print(f"\n=== ğŸ“¥ åˆ—è¡¨æŠ“å–å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(all_articles)} ç¯‡æ–‡ç« é“¾æ¥ã€‚===")

    # 2. éå†æ‰€æœ‰æ–‡ç« ï¼ŒæŠ“å–æ­£æ–‡
    count = 0
    for item in all_articles:
        count += 1
        print(f"ğŸ”¥ æ­£åœ¨å¤„ç†ç¬¬ {count}/{len(all_articles)} ç¯‡ï¼š{item['title']}")
        fetch_article_content(item)
        
    print("\n=== ğŸ¯ æ­£æ–‡æŠ“å–å®Œæˆï¼Œé¢„è§ˆå‰ 5 ç¯‡æ–‡ç« ï¼š===")

    # 3. æ‰“å°å‰ 5 ç¯‡æ–‡ç« ï¼ŒéªŒè¯æ•°æ®å®Œæ•´æ€§
    for item in all_articles[:5]:
        print("---")
        print(f"æ ‡é¢˜: {item['title']}")
        print(f"æ—¥æœŸ: {item['date']}")
        print(f"é“¾æ¥: {item['url']}")
        # æ‰“å°æ­£æ–‡çš„å¼€å¤´éƒ¨åˆ†ï¼ŒéªŒè¯æ˜¯å¦æŠ“å–æˆåŠŸ
        content_preview = item["content"][:200] + "..." if len(item["content"]) > 200 else item["content"]
        print(f"æ­£æ–‡é¢„è§ˆ ({len(item['content'])} å­—): {content_preview}")


if __name__ == "__main__":
    main()
