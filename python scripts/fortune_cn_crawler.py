# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆç»“æ„ï¼‰- V7 ä¿®æ­£æ—¥æœŸé™å®šä¸ç»ˆæä¿®å¤ç‰ˆ

åŠŸèƒ½ï¼š
1. ã€æ ¸å¿ƒä¿®æ­£ã€‘æ—¥æœŸé™å®šåŠŸèƒ½ï¼šæ”¹ä¸ºæŠ“å–é…ç½®ä¸­æŒ‡å®šçš„å›ºå®šæ—¥æœŸï¼ˆé»˜è®¤ä¸º 2025-12-07ï¼‰ã€‚
2. ä½¿ç”¨åŠ¨æ€ã€é€¼çœŸçš„å¤´éƒ¨è¿›è¡Œæ­£æ–‡è¯·æ±‚ï¼Œè§£å†³ 404 é”™è¯¯ã€‚
3. æ”¯æŒå¤šé¡µåˆ—è¡¨æŠ“å–ï¼ˆé»˜è®¤å‰ 3 é¡µï¼‰ã€‚
4. è‡ªåŠ¨è·å–æ¯ç¯‡æ–‡ç« çš„å®Œæ•´æ­£æ–‡ã€‚
5. å°†ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶ã€‚
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆç»“æ„ï¼‰- V8 è·¯å¾„æ‹¼æ¥ç»ˆæä¿®æ­£ç‰ˆ

æ ¸å¿ƒä¿®å¤ï¼š
1. ã€å…³é”®ã€‘URL æ‹¼æ¥ä¸å†åŸºäº BASEï¼Œè€Œæ˜¯åŸºäºåˆ—è¡¨é¡µ URL (current_list_url)ã€‚
   è§£å†³äº† href="c/..." ç›¸å¯¹è·¯å¾„å¯¼è‡´ä¸¢å¤± /shangye/ ç›®å½•çš„é—®é¢˜ã€‚
2. æ—¥æœŸé™å®šï¼šä¸¥æ ¼æŠ“å– 2025-12-07 çš„æ–‡ç« ã€‚
3. å¤´éƒ¨å¢å¼ºï¼šç»§ç»­ä¿æŒé«˜ä»¿çœŸ User-Agentã€‚
"""

import re
@@ -20,14 +19,15 @@

# --- é…ç½®å‚æ•° ---
BASE = "https://www.fortunechina.com"
# ç¡®ä¿åˆ—è¡¨é¡µ URL ç»“å°¾æœ‰æ–œæ ï¼Œè¿™å¯¹ urljoin å¤„ç†ç›¸å¯¹è·¯å¾„éå¸¸é‡è¦
LIST_URL_BASE = "https://www.fortunechina.com/shangye/" 
MAX_PAGES = 3  
MAX_RETRY = 3 
OUTPUT_FILENAME = "fortunechina_articles.csv"
# **ã€å…³é”®ä¿®æ­£ã€‘**ï¼šå°†æ—¥æœŸè®¾å®šä¸ºå›ºå®šçš„ 2025-12-07
# é™å®šæ—¥æœŸ (æ ¹æ®ä½ çš„æˆªå›¾ï¼Œç›®æ ‡æ˜¯ 2025-12-07)
TARGET_DATE = "2025-12-07" 
# ----------------

# åˆ—è¡¨é¡µè¯·æ±‚å¤´éƒ¨
DEFAULT_HEADERS = {
"User-Agent": (
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
@@ -37,21 +37,26 @@
"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
"Accept-Encoding": "gzip, deflate, br",
"Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": "no-cache",
}


def fetch_list(page=1):
"""
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ï¼Œå¹¶é™å®šæ—¥æœŸä¸º TARGET_DATEã€‚
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ï¼Œä½¿ç”¨æ­£ç¡®çš„ç›¸å¯¹è·¯å¾„æ‹¼æ¥ã€‚
   """
    url = f"{BASE}/shangye/" if page == 1 else f"{BASE}/shangye/?page={page}"
    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ---")
    # æ„é€ å½“å‰åˆ—è¡¨é¡µçš„å®Œæ•´ URL
    if page == 1:
        current_list_url = LIST_URL_BASE
    else:
        current_list_url = f"{LIST_URL_BASE}?page={page}"
        
    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ({current_list_url}) ---")

try:
        r = requests.get(url, headers=DEFAULT_HEADERS, timeout=15)
        r = requests.get(current_list_url, headers=DEFAULT_HEADERS, timeout=15)
r.raise_for_status() 
except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥ ({url}): {e}")
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
return []

soup = BeautifulSoup(r.text, "html.parser")
@@ -65,145 +70,120 @@ def fetch_list(page=1):
if not (h2 and a and date_div):
continue

        # è·å–åŸå§‹ href (ä¾‹å¦‚: "c/2025-12/07/content_470761.htm")
href = a["href"].strip()
pub_date = date_div.get_text(strip=True) if date_div else ""

        # **ã€é™å®šæ—¥æœŸåŠŸèƒ½ä¿®æ­£ã€‘**ï¼šåªæŠ“å–å‘å¸ƒæ—¥æœŸç­‰äº TARGET_DATE çš„æ–‡ç« 
        # 1. æ—¥æœŸè¿‡æ»¤ï¼šåªå¤„ç† 2025-12-07
if pub_date != TARGET_DATE:
continue

        # ä¸¥æ ¼æ ¡éªŒé“¾æ¥æ ¼å¼ï¼š/YYYY-MM/DD/content_ID.htm
        match = re.search(r"(/(\d{4}-\d{2}/\d{2})/content_\d+\.htm)", href)
        if not match:
        # 2. ç®€å•çš„æ­£åˆ™æ£€æŸ¥ï¼Œåªè¦åŒ…å« content_æ•°å­— å³å¯
        if not re.search(r"content_\d+\.htm", href):
continue

        # å°è¯•ä¸¤ç§å¯èƒ½çš„æ­£ç¡® URL æ ¼å¼
        url_full_standard = urljoin(BASE, href) 
        content_id_path = re.search(r"/content_\d+\.htm", href)
        url_full_alternate = urljoin(BASE, content_id_path.group(0)) if content_id_path else ""

        # 3. ã€æ ¸å¿ƒä¿®æ­£ã€‘ä½¿ç”¨ current_list_url è¿›è¡Œæ‹¼æ¥
        # å¦‚æœ href æ˜¯ "c/2025..."ï¼Œlist_url æ˜¯ ".../shangye/"
        # ç»“æœè‡ªåŠ¨å˜ä¸º ".../shangye/c/2025..."
        url_full = urljoin(current_list_url, href)
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œç¡®ä¿è·¯å¾„çœ‹èµ·æ¥æ­£ç¡®
        # print(f"  [è°ƒè¯•] åŸå§‹href: {href} -> æ‹¼æ¥å: {url_full}")

items.append({
"title": h2.get_text(strip=True),
            "url_standard": url_full_standard,
            "url_alternate": url_full_alternate, 
            "url": url_full_standard,
            "url": url_full,
"date": pub_date,
"content": "",
})

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°æ–‡ç« æ•°ï¼š{len(items)} (é™å®šæ—¥æœŸ: {TARGET_DATE})")
    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°ç›®æ ‡æ—¥æœŸ({TARGET_DATE})æ–‡ç« æ•°ï¼š{len(items)}")
return items


def fetch_article_content(item):
"""
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œå¹¶åŒ…å«å¤±è´¥é‡è¯•æœºåˆ¶å’Œ Referer å¤´éƒ¨ã€‚
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹
   """
    url = item["url"]
headers = DEFAULT_HEADERS.copy()
    headers["Referer"] = f"{BASE}/shangye/"
    headers["Sec-Fetch-Site"] = "same-origin" 
    headers["Sec-Fetch-Mode"] = "navigate"
    
    # å°è¯•è®¿é—®çš„ URL åˆ—è¡¨ï¼Œå…ˆå°è¯•æ ‡å‡† URLï¼Œå¤±è´¥åå°è¯•å¤‡ç”¨ URL
    urls_to_try = [item["url_standard"], item["url_alternate"]]
    # åŠ ä¸Š Refererï¼Œæ¨¡æ‹Ÿä»åˆ—è¡¨é¡µç‚¹è¿‡å»
    headers["Referer"] = LIST_URL_BASE 

    for current_url in urls_to_try:
        if not current_url:
            continue
            
        for attempt in range(MAX_RETRY):
            try:
                r = requests.get(current_url, headers=headers, timeout=15)
                r.raise_for_status() 

                soup = BeautifulSoup(r.text, "html.parser")
                container = soup.select_one("div.article-mod div.word-text-con")

                if not container:
                    item["content"] = "[æ­£æ–‡å†…å®¹å®¹å™¨æœªæ‰¾åˆ°]"
                    return
                
                item["url"] = current_url
                paras = [p.get_text(strip=True) for p in container.find_all("p") if p.get_text(strip=True)]
                item["content"] = "\n".join(paras)
                time.sleep(0.5) 
    for attempt in range(MAX_RETRY):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status() 

            soup = BeautifulSoup(r.text, "html.parser")
            # å°è¯•å¤šç§æ­£æ–‡é€‰æ‹©å™¨ï¼Œä»¥é˜²é¡µé¢ç»“æ„å¾®è°ƒ
            container = soup.select_one("div.article-mod div.word-text-con")
            if not container:
                container = soup.select_one("div.article-content") # å¤‡ç”¨é€‰æ‹©å™¨

            if not container:
                item["content"] = "[æ­£æ–‡å®¹å™¨æœªæ‰¾åˆ°]"
                print(f"  âš ï¸ è­¦å‘Šï¼šURL {url} è®¿é—®æˆåŠŸä½†æœªæ‰¾åˆ°æ­£æ–‡å®¹å™¨")
return

            except requests.exceptions.HTTPError as e:
                if r.status_code == 404:
                    print(f"  âš ï¸ 404 é“¾æ¥æ— æ•ˆï¼Œå°è¯•ä¸‹ä¸€ä¸ª URL æˆ–é‡è¯•ï¼š{current_url}")
                    break 
                
                print(f"  âŒ æ­£æ–‡è¯·æ±‚å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•ç¬¬ {attempt + 1}/{MAX_RETRY} æ¬¡ ({current_url}): {e}")
                time.sleep(2 ** attempt) 
            
            except requests.exceptions.RequestException as e:
                print(f"  âŒ æ­£æ–‡è¯·æ±‚å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•ç¬¬ {attempt + 1}/{MAX_RETRY} æ¬¡ ({current_url}): {e}")
                time.sleep(2 ** attempt) 
                
    item["content"] = f"[æ­£æ–‡è·å–å¤±è´¥: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°æˆ–æ‰€æœ‰ URL 404]"
    print(f"  â›”ï¸ æ­£æ–‡è·å–å¤±è´¥ï¼Œæ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼š{item['title']}")
            paras = [p.get_text(strip=True) for p in container.find_all("p") if p.get_text(strip=True)]
            item["content"] = "\n".join(paras)
            time.sleep(0.5) 
            return

        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRY - 1:
                print(f"  âŒ è¯·æ±‚å¤±è´¥ ({r.status_code if 'r' in locals() else 'Error'}), é‡è¯•ä¸­...: {url}")
                time.sleep(1)
            else:
                print(f"  â›”ï¸ æœ€ç»ˆå¤±è´¥: {url} | é”™è¯¯: {e}")
                item["content"] = f"[è·å–å¤±è´¥: {e}]"


def save_to_csv(data: list, filename: str):
    """
    å°†æ–‡ç« æ•°æ®åˆ—è¡¨ä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­ã€‚
    """
if not data:
print("ğŸ’¡ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ã€‚")
return
        
fieldnames = ["title", "date", "url", "content"]
    
try:
with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
writer.writeheader()
writer.writerows(data)
            
        print(f"\nğŸ‰ æ•°æ®ä¿å­˜æˆåŠŸï¼æ–‡ä»¶åä¸ºï¼š{filename}ï¼Œå…± {len(data)} æ¡è®°å½•ã€‚")
        print(f"\nğŸ‰ æˆåŠŸä¿å­˜åˆ° CSVï¼š{filename}ï¼Œå…± {len(data)} æ¡ã€‚")
except Exception as e:
        print(f"\nâŒ CSV æ–‡ä»¶å†™å…¥å¤±è´¥ï¼š{e}")
        print(f"\nâŒ CSV ä¿å­˜å¤±è´¥ï¼š{e}")


def main():
all_articles = []
    
    print(f"=== ğŸš€ è´¢å¯Œä¸­æ–‡ç½‘çˆ¬è™«å¼€å§‹æ‰§è¡Œï¼ˆç›®æ ‡é¡µæ•°ï¼š{MAX_PAGES}ï¼Œé™å®šæ—¥æœŸï¼š{TARGET_DATE}ï¼‰ ===")
    print(f"=== ğŸš€ çˆ¬è™«å¯åŠ¨ (ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}) ===")
    print(f"=== ğŸ› ï¸ ä¿®å¤ç­–ç•¥: åŸºäºåˆ—è¡¨é¡µ URL ({LIST_URL_BASE}) è¿›è¡Œç›¸å¯¹è·¯å¾„æ‹¼æ¥ ===")

    # 1. å®ç°å¤šé¡µæŠ“å–å¾ªç¯
    # 1. æŠ“å–åˆ—è¡¨
for page in range(1, MAX_PAGES + 1):
list_items = fetch_list(page)
        
if not list_items:
            if page == 1:
                print(f"--- ç¬¬ 1 é¡µæœªæŠ“åˆ°ä»»ä½• {TARGET_DATE} å‘å¸ƒçš„æ–‡ç« ï¼Œåœæ­¢ ---")
            else:
                print(f"--- ç¬¬ {page} é¡µæ²¡æœ‰æŠ“åˆ°æ–‡ç« ï¼Œåœæ­¢ç¿»é¡µ ---")
            # å¦‚æœç¬¬ä¸€é¡µå°±æ²¡æ•°æ®ï¼Œå¯èƒ½æ˜¯æ—¥æœŸä¸å¯¹ï¼Œæˆ–è€…æ²¡åŠ è½½å‡ºæ¥
            if page == 1: 
                print(f"âš ï¸ ç¬¬ 1 é¡µæœªæ‰¾åˆ° {TARGET_DATE} çš„æ–‡ç« ï¼Œè¯·ç¡®è®¤ç½‘ç«™ä¸Šç¡®å®æœ‰è¯¥æ—¥æœŸçš„å†…å®¹ã€‚")
break
            
all_articles.extend(list_items)
time.sleep(1) 

    print(f"\n=== ğŸ“¥ åˆ—è¡¨æŠ“å–å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(all_articles)} ç¯‡ç¬¦åˆæ—¥æœŸçš„æ–‡ç« é“¾æ¥ã€‚===")
    print(f"\n=== ğŸ“¥ é“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡ã€‚å¼€å§‹æŠ“å–æ­£æ–‡... ===")

    # 2. éå†æ‰€æœ‰æ–‡ç« ï¼ŒæŠ“å–æ­£æ–‡
    # 2. æŠ“å–æ­£æ–‡
count = 0
for item in all_articles:
count += 1
        print(f"ğŸ”¥ æ­£åœ¨å¤„ç†ç¬¬ {count}/{len(all_articles)} ç¯‡ï¼š{item['title']}")
        print(f"ğŸ”¥ ({count}/{len(all_articles)}) å¤„ç†: {item['title']}")
fetch_article_content(item)

    # 3. ç»Ÿè®¡å¤±è´¥æ–‡ç« æ•°
    failed_count = sum(1 for item in all_articles if item["content"].startswith("[æ­£æ–‡è·å–å¤±è´¥"))
    print(f"\n=== ç»Ÿè®¡ç»“æœï¼šæˆåŠŸè·å– {len(all_articles) - failed_count} ç¯‡ï¼Œå¤±è´¥ {failed_count} ç¯‡ã€‚===")

    # 4. ä¿å­˜ä¸º CSV æ–‡ä»¶
    # 3. ç»Ÿè®¡ä¸ä¿å­˜
    success_count = sum(1 for item in all_articles if "è·å–å¤±è´¥" not in item["content"] and item["content"])
    print(f"\n=== ç»Ÿè®¡: æˆåŠŸ {success_count} ç¯‡ï¼Œå¤±è´¥ {len(all_articles) - success_count} ç¯‡ ===")
save_to_csv(all_articles, OUTPUT_FILENAME)
    
    return all_articles


if __name__ == "__main__":
main()
