# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆç»“æ„ï¼‰- V8 + AI æ ‡é¢˜ç‰ˆ

åŠŸèƒ½ï¼š
1. åˆ—è¡¨é¡µï¼šæ”¯æŒå¤šé¡µæŠ“å–ï¼ˆé»˜è®¤å‰ 3 é¡µï¼‰ï¼ŒåŸºäºåˆ—è¡¨é¡µ URL åšç›¸å¯¹è·¯å¾„æ‹¼æ¥ï¼Œé¿å…ä¸¢å¤± /shangye/ ç›®å½•ã€‚
2. æ—¥æœŸé™å®šï¼šåªæŠ“å–æŒ‡å®šæ—¥æœŸï¼ˆé»˜è®¤ 2025-12-07ï¼‰çš„æ–‡ç« ã€‚
3. æ­£æ–‡æŠ“å–ï¼šå¸¦ Refererã€æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨å¤´ï¼Œæ”¯æŒç®€å•é‡è¯•ã€‚
4. AI æ¦‚æ‹¬ï¼šç”¨å¤§æ¨¡å‹æ ¹æ®æ­£æ–‡ç”Ÿæˆä¸€å¥ã€Œå†…éƒ¨ç”¨ã€æ ‡é¢˜ï¼Œå‡†ç¡®æ¦‚æ‹¬å†…å®¹ï¼Œé¿å…æ ‡é¢˜å…šã€‚
5. è¾“å‡ºï¼šä¿å­˜ä¸º CSVï¼Œå­—æ®µåŒ…æ‹¬ï¼šåŸå§‹æ ‡é¢˜ã€AI æ ‡é¢˜ã€æ—¥æœŸã€URLã€æ­£æ–‡ã€‚
"""

import os
import re
import time
import csv
import json
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ================== åŸºæœ¬é…ç½® ==================
BASE = "https://www.fortunechina.com"

# åˆ—è¡¨é¡µåŸºå‡† URLï¼ˆç”¨äº urljoinï¼ŒåŠ¡å¿…ä»¥ / ç»“å°¾ï¼‰
LIST_URL_BASE = "https://www.fortunechina.com/shangye/"

# æœ€å¤§ç¿»é¡µæ•°
MAX_PAGES = 3

# æ­£æ–‡è¯·æ±‚æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_RETRY = 3

# è¾“å‡ºæ–‡ä»¶å
OUTPUT_FILENAME = "fortunechina_articles_with_ai_title.csv"

# ç›®æ ‡æ—¥æœŸï¼ˆåªæŠ“è¿™ä¸€æ—¥çš„æ–‡ç« ï¼‰
TARGET_DATE = "2025-12-07"   # æ ¼å¼ï¼šYYYY-MM-DD

# ================== AI æ¥å£é…ç½® ==================
# å»ºè®®åœ¨ç³»ç»Ÿç¯å¢ƒå˜é‡é‡Œé…ç½®ï¼šOPENAI_API_KEY
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()

# è¿™é‡Œä»¥ OpenAI å…¼å®¹æ¥å£ä¸ºä¾‹ï¼Œä½ å¯ä»¥æ”¹æˆè‡ªå·±çš„ç½‘å…³åœ°å€
OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
OPENAI_MODEL = "gpt-4.1-mini"  # æˆ–ä½ è‡ªå·±çš„æ¨¡å‹åç§°

# ================== HTTP å¤´ ==================
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": "no-cache",
}


# ================== åˆ—è¡¨æŠ“å– ==================
def fetch_list(page: int):
    """
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ï¼Œé™å®šæ—¥æœŸä¸º TARGET_DATEï¼Œå¹¶ç”¨åˆ—è¡¨é¡µ URL åšç›¸å¯¹è·¯å¾„æ‹¼æ¥ã€‚
    è¿”å›ï¼š[{title, url, date, content, ai_title}, ...]
    """
    if page == 1:
        current_list_url = LIST_URL_BASE
    else:
        current_list_url = f"{LIST_URL_BASE}?page={page}"

    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ({current_list_url}) ---")

    try:
        r = requests.get(current_list_url, headers=DEFAULT_HEADERS, timeout=15)
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")

    items = []

    # è¿™é‡Œæ ¹æ®å®é™…ç»“æ„é€‰ä¸€ä¸ªå°½é‡ç¨³çš„é€‰æ‹©å™¨
    # è´¢å¯Œä¸­æ–‡ç½‘å•†ä¸šé¢‘é“åˆ—è¡¨å¤§è‡´ç»“æ„ï¼šdiv.list-mod / div.mod-list / li
    # å¦‚æœ‰åå·®ï¼Œä½ å¯ä»¥å¯¹ç…§é¡µé¢è°ƒä¸€ä¸‹é€‰æ‹©å™¨
    for box in soup.select("div.list-mod li, div.mod-list li, div.list-item"):
        # å°è¯•å–æ ‡é¢˜å’Œé“¾æ¥
        h2 = box.find("h2") or box.find("h3")
        a = h2.find("a") if h2 else box.find("a")
        date_div = box.find("span", class_="time") or box.find("div", class_="time") or box.find("span", class_="date")

        if not (h2 and a and date_div):
            continue

        href = a.get("href", "").strip()
        pub_date_raw = date_div.get_text(strip=True)

        # æœ‰çš„ç«™ä¼šå¸¦æ—¶é—´ï¼Œæ¯”å¦‚ "2025-12-07 10:23"
        # ç”¨ startswithï¼Œå®½æ¾åŒ¹é…åŒä¸€å¤©
        if not pub_date_raw.startswith(TARGET_DATE):
            continue

        # href ä¸­å¿…é¡»å«æœ‰ content_xxx.htm æ‰å½“æˆæ–‡ç« é“¾æ¥
        if not re.search(r"content_\d+\.htm", href):
            continue

        # ã€å…³é”®ã€‘åŸºäºå½“å‰åˆ—è¡¨é¡µ URL æ‹¼æ¥ï¼Œä¿è¯ä¿ç•™ /shangye/
        url_full = urljoin(current_list_url, href)

        items.append({
            "title": h2.get_text(strip=True),
            "url": url_full,
            "date": TARGET_DATE,
            "content": "",
            "ai_title": "",  # é¢„ç•™å­—æ®µï¼Œç¨åå¡« AI æ¦‚æ‹¬æ ‡é¢˜
        })

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°ç›®æ ‡æ—¥æœŸ({TARGET_DATE})æ–‡ç« æ•°ï¼š{len(items)}")
    return items


# ================== æ­£æ–‡æŠ“å– ==================
def fetch_article_content(item: dict):
    """
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œå¸¦ç®€å•é‡è¯•ã€‚
    æˆåŠŸåå†™å…¥ item["content"]ã€‚
    """
    url = item["url"]
    headers = DEFAULT_HEADERS.copy()

    # æ¨¡æ‹Ÿä»åˆ—è¡¨é¡µç‚¹å‡»è¿›å…¥
    headers["Referer"] = LIST_URL_BASE
    headers["Sec-Fetch-Site"] = "same-origin"
    headers["Sec-Fetch-Mode"] = "navigate"

    for attempt in range(MAX_RETRY):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()

            soup = BeautifulSoup(r.text, "html.parser")

            # ä¸»é€‰æ‹©å™¨
            container = soup.select_one("div.article-mod div.word-text-con")
            # å¤‡ç”¨é€‰æ‹©å™¨
            if not container:
                container = soup.select_one("div.article-content")

            if not container:
                item["content"] = "[æ­£æ–‡å®¹å™¨æœªæ‰¾åˆ°]"
                print(f"  âš ï¸ è­¦å‘Šï¼šURL {url} è®¿é—®æˆåŠŸä½†æœªæ‰¾åˆ°æ­£æ–‡å®¹å™¨")
                return

            paras = [
                p.get_text(strip=True)
                for p in container.find_all("p")
                if p.get_text(strip=True)
            ]
            item["content"] = "\n".join(paras)
            time.sleep(0.5)
            return

        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRY - 1:
                print(f"  âŒ æ­£æ–‡è¯·æ±‚å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{MAX_RETRY} ... -> {url} | é”™è¯¯: {e}")
                time.sleep(1)
            else:
                print(f"  â›”ï¸ æœ€ç»ˆå¤±è´¥: {url} | é”™è¯¯: {e}")
                item["content"] = f"[è·å–å¤±è´¥: {e}]"


# ================== AI æ¦‚æ‹¬æ ‡é¢˜ ==================
def ai_summarize_title(content: str, fallback_title: str) -> str:
    """
    ç”¨å¤§æ¨¡å‹æŠŠæ­£æ–‡æ¦‚æ‹¬æˆä¸€å¥æ ‡é¢˜ã€‚
    è¦æ±‚ï¼šä¸­æ–‡ã€å‡†ç¡®ã€éæ ‡é¢˜å…šï¼Œæ§åˆ¶åœ¨ 25 å­—ä»¥å†…ã€‚
    å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œåˆ™è¿”å›åŸå§‹æ ‡é¢˜ã€‚
    """
    if not OPENAI_API_KEY:
        print("âš ï¸ æœªé…ç½® OPENAI_API_KEYï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜ã€‚")
        return fallback_title

    # æ–‡æœ¬å¤ªé•¿ä¼šè´µä¹Ÿä¼šè¶… tokenï¼Œè¿™é‡Œæˆªä¸€æ®µä¸Šä¸‹æ–‡å°±å¤Ÿæ¦‚æ‹¬äº†
    if not content or content.startswith("[è·å–å¤±è´¥"):
        # æ²¡æ­£æ–‡å°±æ²¡æ³•æ¦‚æ‹¬ï¼Œåªèƒ½ç”¨åŸæ ‡é¢˜
        return fallback_title

    snippet = content[:2000]

    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæ–°é—»ç¼–è¾‘ï¼Œè¯·æ ¹æ®ä¸‹é¢çš„æ–‡ç« å†…å®¹ï¼Œç”¨ä¸­æ–‡å†™ä¸€ä¸ªä¸è¶…è¿‡ 25 ä¸ªå­—çš„æ–°é—»æ ‡é¢˜ï¼Œ"
        "è¦æ±‚ï¼šå‡†ç¡®æ¦‚æ‹¬æ ¸å¿ƒä¿¡æ¯ï¼Œé¿å…å¤¸å¼ å’Œæ ‡é¢˜å…šï¼Œä¸è¦åŠ å¼•å·ï¼š\n\n"
        f"{snippet}"
    )

    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸¥è°¨çš„ä¸­æ–‡æ–°é—»ç¼–è¾‘ï¼Œåªè¾“å‡ºæ ‡é¢˜æ–‡æœ¬ã€‚"},
            {"role": "user", "content": prompt},
        ],
        "temperature": 0.3,
        "max_tokens": 64,
    }

    try:
        resp = requests.post(OPENAI_API_URL, headers=headers, data=json.dumps(payload), timeout=30)
        resp.raise_for_status()
        data = resp.json()

        # å…¼å®¹ chat/completions ç»“æ„
        ai_title = data["choices"][0]["message"]["content"].strip()
        # é˜²å¾¡ï¼šæœ‰äº›æ¨¡å‹ä¼šè¾“å‡ºå¤šè¡Œè¯´æ˜ï¼Œè¿™é‡Œå–é¦–è¡Œ
        ai_title = ai_title.splitlines()[0].strip()

        # æç«¯æƒ…å†µä¸‹æ¨¡å‹è¿”å›ç©ºï¼Œå°±å›é€€
        return ai_title or fallback_title

    except Exception as e:
        print(f"âš ï¸ AI æ¦‚æ‹¬å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜ã€‚é”™è¯¯: {e}")
        return fallback_title


# ================== ä¿å­˜ CSV ==================
def save_to_csv(data: list, filename: str):
    """
    å°†æ–‡ç« æ•°æ®åˆ—è¡¨ä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­ã€‚
    """
    if not data:
        print("ğŸ’¡ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ã€‚")
        return

    fieldnames = ["title", "ai_title", "date", "url", "content"]

    try:
        with open(filename, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)

        print(f"\nğŸ‰ æˆåŠŸä¿å­˜åˆ° CSVï¼š{filename}ï¼Œå…± {len(data)} æ¡ã€‚")
    except Exception as e:
        print(f"\nâŒ CSV ä¿å­˜å¤±è´¥ï¼š{e}")


# ================== ä¸»æµç¨‹ ==================
def main():
    all_articles = []

    print(f"=== ğŸš€ è´¢å¯Œä¸­æ–‡ç½‘çˆ¬è™«å¯åŠ¨ (ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}) ===")
    print(f"=== ğŸ› ï¸ è·¯å¾„ç­–ç•¥: åŸºäºåˆ—è¡¨é¡µ URL ({LIST_URL_BASE}) è¿›è¡Œç›¸å¯¹è·¯å¾„æ‹¼æ¥ ===")

    # 1. æŠ“å–åˆ—è¡¨
    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)

        if not list_items:
            if page == 1:
                print(f"--- ç¬¬ 1 é¡µæœªæŠ“åˆ°ä»»ä½• {TARGET_DATE} å‘å¸ƒçš„æ–‡ç« ï¼Œåœæ­¢ ---")
                print(f"âš ï¸ è¯·ç¡®è®¤ç½‘ç«™ä¸Šæ˜¯å¦å­˜åœ¨ {TARGET_DATE} çš„å†…å®¹ã€‚")
            else:
                print(f"--- ç¬¬ {page} é¡µæ²¡æœ‰æŠ“åˆ°æ–‡ç« ï¼Œåœæ­¢ç¿»é¡µ ---")
            break

        all_articles.extend(list_items)
        time.sleep(1)

    print(f"\n=== ğŸ“¥ é“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡ã€‚å¼€å§‹æŠ“å–æ­£æ–‡... ===")

    # 2. æŠ“å–æ­£æ–‡å¹¶ç”¨ AI æ¦‚æ‹¬æ ‡é¢˜
    for idx, item in enumerate(all_articles, start=1):
        print(f"\nğŸ”¥ ({idx}/{len(all_articles)}) å¤„ç†ï¼š{item['title']}")
        fetch_article_content(item)

        # AI æ¦‚æ‹¬æ ‡é¢˜
        item["ai_title"] = ai_summarize_title(item["content"], item["title"])
        print(f"   ğŸ§  AI æ ‡é¢˜ï¼š{item['ai_title']}")

    # 3. ç»Ÿè®¡ + ä¿å­˜
    success_count = sum(
        1
        for item in all_articles
        if item["content"] and not item["content"].startswith("[è·å–å¤±è´¥")
    )
    print(f"\n=== ç»Ÿè®¡: æ­£æ–‡æˆåŠŸ {success_count} ç¯‡ï¼Œå¤±è´¥ {len(all_articles) - success_count} ç¯‡ ===")

    save_to_csv(all_articles, OUTPUT_FILENAME)

    return all_articles


if __name__ == "__main__":
    main()
