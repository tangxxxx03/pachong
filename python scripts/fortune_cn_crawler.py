# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ Â· å•†ä¸šé¢‘é“ çˆ¬è™« + é’‰é’‰æ¨é€
https://www.fortunechina.com/shangye/

åŠŸèƒ½ï¼š
1. æŠ“å–å•†ä¸šé¢‘é“åˆ—è¡¨é¡µï¼ˆå¯å¤šé¡µï¼‰
2. æå–ï¼šæ ‡é¢˜ / é“¾æ¥ / æ—¥æœŸ
3. ï¼ˆå¯é€‰ï¼‰æŠ“å–æ¯ç¯‡æ–‡ç« æ­£æ–‡å†…å®¹
4. æŠŠæŠ“åˆ°çš„æ–‡ç« æ•´ç†æˆ Markdownï¼Œæ¨é€åˆ°ä¸€ä¸ªæˆ–å¤šä¸ªé’‰é’‰ç¾¤

ç¯å¢ƒå˜é‡ï¼ˆå»ºè®®é€šè¿‡ GitHub Secrets é…ç½®ï¼‰ï¼š
  DINGTALK_BASES   = "url1,url2"          # å¤šä¸ªç¾¤çš„ webhookï¼Œç”¨é€—å·éš”å¼€
  DINGTALK_SECRETS = "sec1,sec2"         # å¯¹åº”æ¯ä¸ªç¾¤çš„ secretï¼ˆæ•°é‡å¯ä»¥æ˜¯ 1 ä¸ªæˆ– N ä¸ªï¼‰
  â€”â€” æˆ–è€…åªé…å•ä¸ªï¼š
  DINGTALK_BASE    = "å•ä¸ªç¾¤ webhook"
  DINGTALK_SECRET  = "å•ä¸ªç¾¤ secret"
"""

import os
import re
import time
import hmac
import base64
import hashlib
import urllib.parse

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE = "https://www.fortunechina.com"

# --- åˆ›å»ºå¸¦ UA çš„ sessionï¼Œç¨å¾®å‹å¥½ä¸€ç‚¹ ---
session = requests.Session()
session.headers.update({
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120 Safari/537.36"
    ),
    "Accept-Language": "zh-CN,zh;q=0.9",
})


# ================== é’‰é’‰æ¨é€ç›¸å…³ ==================

def _sign_webhook(base: str, secret: str) -> str:
    """
    ç»™å•ä¸ª webhook åŠ ç­¾ï¼Œè¿”å›å®Œæ•´è¯·æ±‚ URL
    """
    if not base:
        return ""
    if not secret:
        return base

    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(
        base64.b64encode(
            hmac.new(secret.encode("utf-8"), string_to_sign, hashlib.sha256).digest()
        )
    )
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"


def send_dingtalk_markdown(title: str, md: str) -> bool:
    """
    åŒä¸€æ¡ markdown æ¶ˆæ¯æ¨é€åˆ°å¤šä¸ªé’‰é’‰ç¾¤ã€‚

    ç¯å¢ƒå˜é‡ï¼š
      DINGTALK_BASES   = "url1,url2"
      DINGTALK_SECRETS = "sec1,sec2"

      æˆ–å•ä¸ªï¼š
      DINGTALK_BASE
      DINGTALK_SECRET
    """
    bases_str = os.getenv("DINGTALK_BASES") or os.getenv("DINGTALK_BASE") or ""
    secrets_str = os.getenv("DINGTALK_SECRETS") or os.getenv("DINGTALK_SECRET") or ""

    bases = [b.strip() for b in bases_str.split(",") if b.strip()]
    secrets = [s.strip() for s in secrets_str.split(",") if s.strip()]

    if not bases:
        print("ğŸ”• æœªé…ç½® DINGTALK_BASES/DINGTALK_BASEï¼Œè·³è¿‡æ¨é€ã€‚")
        return False

    # åªé…ç½®äº†ä¸€ä¸ª secretï¼Œä½†æœ‰å¤šä¸ª webhookï¼šå¤ç”¨è¿™ä¸€ä¸ª
    if len(secrets) == 1 and len(bases) > 1:
        secrets = secrets * len(bases)

    # é•¿åº¦ä¸ä¸€è‡´æ—¶ï¼Œç”¨ç©ºå­—ç¬¦ä¸²è¡¥é½ï¼ˆè¡¨ç¤ºä¸åŠ ç­¾ï¼‰
    if secrets and len(secrets) != len(bases):
        print("âš ï¸ DINGTALK_BASES ä¸ DINGTALK_SECRETS æ•°é‡ä¸ä¸€è‡´ï¼Œç¼ºå¤±çš„å°†ä¸åŠ ç­¾ã€‚")
        while len(secrets) < len(bases):
            secrets.append("")

    ok_any = False
    for i, base in enumerate(bases):
        secret = secrets[i] if i < len(secrets) else ""
        full_url = _sign_webhook(base, secret)
        payload = {
            "msgtype": "markdown",
            "markdown": {
                "title": title,
                "text": md
            }
        }

        try:
            resp = requests.post(full_url, json=payload, timeout=20)
            data = {}
            try:
                data = resp.json()
            except Exception:
                pass
            ok = (resp.status_code == 200 and data.get("errcode") == 0)
            print(f"[DingTalk #{i+1}] push={ok} code={resp.status_code}")
            if not ok:
                print("  resp:", resp.text[:300])
            ok_any = ok_any or ok
        except Exception as e:
            print(f"[DingTalk #{i+1}] error:", e)

    return ok_any


# ================== çˆ¬è™«æ ¸å¿ƒé€»è¾‘ ==================

def fetch_list(page: int = 1):
    """
    æŠ“å–å•†ä¸šé¢‘é“æŸä¸€é¡µçš„æ–‡ç« åˆ—è¡¨ï¼ˆæ ‡é¢˜ã€é“¾æ¥ã€æ—¥æœŸï¼‰

    è¿”å›ï¼šlist[dict]ï¼Œæ¯ä¸ªå…ƒç´ ï¼š
    {
        "title": æ ‡é¢˜,
        "url": è¯¦æƒ…é“¾æ¥,
        "date": æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
    }
    """
    if page == 1:
        url = f"{BASE}/shangye/"
    else:
        # â€œæ›´å¤šæ–‡ç« â€åçš„åˆ†é¡µ URL è§„å¾‹
        url = f"{BASE}/shangye/node_12143_{page}.htm"

    print(f"\n=== æŠ“å–åˆ—è¡¨é¡µï¼šç¬¬ {page} é¡µ ===")
    print("URL:", url)

    r = session.get(url, timeout=20)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "html.parser")

    items = []

    # åˆ—è¡¨é¡µä¸­ï¼Œæ¯ç¯‡æ–‡ç« ä¸€èˆ¬åœ¨ <h2><a href="...">æ ‡é¢˜</a></h2>
    for h2 in soup.find_all("h2"):
        a = h2.find("a", href=True)
        if not a:
            continue

        href = a["href"]
        # åªä¿ç•™çœŸæ­£çš„å•†ä¸šé¢‘é“æ–‡ç« é“¾æ¥
        if "/shangye/c/" not in href:
            continue

        title = a.get_text(strip=True)
        full_url = urljoin(BASE, href)

        # å°è¯•åœ¨æ‰€åœ¨å—ä¸­æŠ“æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
        block_text = " ".join(h2.parent.get_text(" ", strip=True).split())
        m = re.search(r"\d{4}-\d{2}-\d{2}", block_text)
        pub_date = m.group(0) if m else ""

        items.append({
            "title": title,
            "url": full_url,
            "date": pub_date,
        })

    print(f"æœ¬é¡µæŠ“åˆ° {len(items)} ç¯‡æ–‡ç« ")
    return items


def fetch_detail(url: str) -> dict:
    """
    æŠ“ä¸€ç¯‡æ–‡ç« è¯¦æƒ…ï¼šæ ‡é¢˜ + æ—¥æœŸ + æ­£æ–‡ï¼ˆçº¯æ–‡æœ¬ï¼‰

    è¿”å›ï¼š
    {
        "title": æ ‡é¢˜,
        "date": æ—¥æœŸï¼ˆå¯èƒ½ä¸ºç©ºï¼‰,
        "content": æ­£æ–‡çº¯æ–‡æœ¬ï¼ˆå¤šæ®µç”¨æ¢è¡Œæ‹¼æ¥ï¼‰
    }
    """
    print("  -> æŠ“å–è¯¦æƒ…é¡µï¼š", url)
    r = session.get(url, timeout=20)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "html.parser")

    # æ ‡é¢˜ï¼šä¸€èˆ¬åœ¨ <h1> æˆ– <h2>
    h1 = soup.find(["h1", "h2"])
    title = h1.get_text(strip=True) if h1 else ""

    # ä¸»å†…å®¹å—ï¼šç®€å•ç”¨ class ååŒ¹é… content/article ä¹‹ç±»
    main = soup.find("div", class_=re.compile("content|article", re.I)) or soup

    paras = [p.get_text(strip=True) for p in main.find_all("p")]
    content = "\n".join(p for p in paras if p)

    # é¡µé¢ä¸­ç³Šä¸€éæ‰¾æ—¥æœŸ
    all_text = soup.get_text(" ", strip=True)
    m = re.search(r"\d{4}-\d{2}-\d2", all_text)
    pub_date = m.group(0) if m else ""

    return {
        "title": title,
        "date": pub_date,
        "content": content,
    }


def crawl_pages(max_page: int = 1, with_detail: bool = False):
    """
    ä¸€æ¬¡æ€§æŠ“å¤šé¡µå•†ä¸šé¢‘é“æ–‡ç« åˆ—è¡¨ï¼Œå¿…è¦æ—¶é¡ºä¾¿æŠ“æ­£æ–‡

    :param max_page: æŠ“å–çš„åˆ—è¡¨é¡µæ•°é‡ï¼ˆä»ç¬¬ 1 é¡µå¼€å§‹ï¼‰
    :param with_detail: æ˜¯å¦åŒæ—¶æŠ“æ­£æ–‡
    :return: list[dict]
             æ¯ä¸ªå…ƒç´ ï¼š
             {
                 "title": ...,
                 "url": ...,
                 "date": ...,
                 "content": ... (å¦‚æœ with_detail=True æ‰æœ‰)
             }
    """
    all_items = []

    for page in range(1, max_page + 1):
        items = fetch_list(page)
        for it in items:
            if with_detail:
                # æŠ“æ­£æ–‡å†…å®¹
                detail = fetch_detail(it["url"])
                it["date"] = it["date"] or detail["date"]
                it["content"] = detail["content"]
                # é˜²æ­¢é¢‘ç‡å¤ªé«˜ï¼Œå¯ä»¥é€‚å½“ sleep ä¸€ä¸‹
                time.sleep(1)

            all_items.append(it)

    return all_items


# ================== Markdown æ„é€  ==================

def build_markdown(articles, max_items: int = 10) -> str:
    """
    æŠŠæ–‡ç« åˆ—è¡¨è½¬æˆé€‚åˆé’‰é’‰çš„ Markdown æ–‡æœ¬
    """
    out = []
    out.append("**è´¢å¯Œä¸­æ–‡ç½‘ Â· å•†ä¸šé¢‘é“ Â· æ¯æ—¥ç²¾é€‰**  ")
    out.append("")
    if not articles:
        out.append("> ä»Šæ—¥æœªæŠ“åˆ°å•†ä¸šé¢‘é“æ–‡ç« ã€‚")
        return "\n".join(out)

    for idx, art in enumerate(articles[:max_items], 1):
        title = art.get("title", "")
        date_str = art.get("date", "")
        line = f"{idx}. **{title}**"
        if date_str:
            line += f"ï¼ˆ{date_str}ï¼‰"
        out.append(line + "  ")
        out.append(f"> {art.get('url', '')}  ")
        out.append("")

    return "\n".join(out)


# ================== ä¸»å…¥å£ ==================

if __name__ == "__main__":
    print("æ‰§è¡Œ fortune_cn_crawler.pyï¼ˆå•†ä¸šé¢‘é“åˆ—è¡¨æŠ“å– + é’‰é’‰æ¨é€ï¼‰")

    # æŠ“å– 1 é¡µåˆ—è¡¨ï¼›æƒ³å¤šä¸€ç‚¹å¯ä»¥æ”¹æˆ max_page=2/3...
    articles = crawl_pages(max_page=1, with_detail=False)

    print("\n=== æ§åˆ¶å°é¢„è§ˆï¼ˆå‰ 5 æ¡ï¼‰ ===")
    for art in articles[:5]:
        print(f"{art['date']} | {art['title']}")
        print(f"  {art['url']}")

    md = build_markdown(articles, max_items=10)

    print("\n===== Markdown Preview =====\n")
    print(md)

    # æ¨é€åˆ°é’‰é’‰ï¼ˆéœ€è¦æå‰é…ç½®ç¯å¢ƒå˜é‡ï¼‰
    send_dingtalk_markdown("è´¢å¯Œå•†ä¸š Â· æ¯æ—¥ç²¾é€‰", md)
