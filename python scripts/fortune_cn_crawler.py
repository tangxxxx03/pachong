# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆç»“æ„ï¼‰- SiliconFlow AI æ‘˜è¦ & Markdown ç‰ˆ

åŠŸèƒ½ï¼š
1. æŠ“å–è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“æŒ‡å®šæ—¥æœŸçš„æ–°é—»ï¼ˆé»˜è®¤æŠ“â€œåŒ—äº¬æ—¶é—´æ˜¨å¤©â€çš„ï¼‰ã€‚
2. ä¿®å¤åˆ—è¡¨é¡µ href ç›¸å¯¹è·¯å¾„ï¼ˆc/2025-12/07/...ï¼‰ä¸¢å¤± /shangye/ çš„é—®é¢˜ã€‚
3. è°ƒç”¨ SiliconFlow OpenAI å…¼å®¹æ¥å£ç”Ÿæˆã€Œä¸€å¥è¯ä¸­æ–‡æ‘˜è¦ã€ã€‚
4. å¯¼å‡º CSVï¼ˆåŒ…å«åŸå§‹æ ‡é¢˜ + AI æ‘˜è¦ + æ—¥æœŸ + URL + æ­£æ–‡ï¼‰ã€‚
5. ç”Ÿæˆ Markdown åˆ—è¡¨ï¼ˆæ¯æ¡ [AI æ‘˜è¦](URL)ï¼‰ï¼Œé€‚åˆé’‰é’‰ Markdown ç¾¤å‘ã€‚

ç¯å¢ƒå˜é‡ï¼ˆå»ºè®®ç”¨ GitHub Secrets é…ç½®ï¼‰ï¼š
- OPENAI_API_KEY : ä½ çš„ SiliconFlow API Keyï¼ˆsk-å¼€å¤´çš„é‚£ä¸²ï¼‰ã€‚
- AI_API_BASE    : å¯é€‰ï¼ŒSiliconFlow åŸºç¡€åœ°å€ï¼Œé»˜è®¤ https://api.siliconflow.cn/v1
- AI_MODEL       : å¯é€‰ï¼Œä½¿ç”¨çš„æ¨¡å‹åï¼Œé»˜è®¤ deepseek-ai/DeepSeek-V2-Chat
- TARGET_DATE    : å¯é€‰ï¼ŒæŒ‡å®šæŠ“å–å“ªä¸€å¤©ï¼ˆYYYY-MM-DDï¼‰ï¼Œä¸è®¾åˆ™é»˜è®¤â€œåŒ—äº¬æ—¶é—´æ˜¨å¤©â€ã€‚
"""

import os
import re
import time
import csv
from datetime import datetime, timedelta, timezone

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ================== åŸºæœ¬é…ç½® ==================

BASE = "https://www.fortunechina.com"
# åˆ—è¡¨é¡µ URLï¼ŒåŠ¡å¿…ä»¥ / ç»“å°¾ï¼Œæ–¹ä¾¿ urljoin
LIST_URL_BASE = "https://www.fortunechina.com/shangye/"
MAX_PAGES = 3
MAX_RETRY = 3

OUTPUT_FILENAME = "fortunechina_articles_with_ai_title.csv"
OUTPUT_MD = "fortunechina_articles_with_ai_title.md"


def get_target_date() -> str:
    """
    å†³å®šè¦æŠ“å–çš„ç›®æ ‡æ—¥æœŸï¼š
    1. å¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡ TARGET_DATEï¼ˆä¾‹å¦‚ "2025-12-07"ï¼‰ï¼Œä¼˜å…ˆç”¨å®ƒï¼›
    2. å¦åˆ™é»˜è®¤æŠ“ã€ŒåŒ—äº¬æ—¶é—´æ˜¨å¤©ã€ï¼Œæ ¼å¼ YYYY-MM-DDã€‚
    """
    env_date = os.getenv("TARGET_DATE", "").strip()
    if env_date:
        return env_date

    tz_cn = timezone(timedelta(hours=8))
    yesterday_cn = (datetime.now(tz_cn) - timedelta(days=1)).strftime("%Y-%m-%d")
    return yesterday_cn


TARGET_DATE = get_target_date()

# ================== SiliconFlow AI é…ç½® ==================

# ä»ç¯å¢ƒå˜é‡è¯»å– Keyï¼ˆGitHub Secrets: OPENAI_API_KEYï¼‰
AI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()

# SiliconFlow åŸºç¡€åœ°å€ï¼Œé»˜è®¤ https://api.siliconflow.cn/v1
AI_API_BASE = os.getenv("AI_API_BASE", "https://api.siliconflow.cn/v1").rstrip("/")

# ChatCompletions URL
AI_CHAT_URL = f"{AI_API_BASE}/chat/completions"

# æ¨¡å‹åç§°ï¼ˆå¯ä»¥åœ¨ SiliconFlow æ§åˆ¶å°çœ‹æ”¯æŒçš„æ¨¡å‹ï¼‰
AI_MODEL = os.getenv("AI_MODEL", "deepseek-ai/DeepSeek-V2-Chat")

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": "no-cache",
}

# ================== AI æ‘˜è¦å‡½æ•° ==================


def get_ai_summary(content: str, fallback_title: str = "") -> str:
    """
    ä½¿ç”¨ SiliconFlow OpenAI å…¼å®¹æ¥å£ç”Ÿæˆä¸€å¥è¯æ‘˜è¦ã€‚
    - content: æ–‡ç« æ­£æ–‡
    - fallback_title: è‹¥ AI è°ƒç”¨å¤±è´¥åˆ™é€€å›çš„æ ‡é¢˜ï¼ˆå¯ä»¥ä¼ åŸå§‹æ ‡é¢˜ï¼‰
    """
    if not content or len(content) < 30:
        return fallback_title or "å†…å®¹è¿‡çŸ­ï¼Œæ— éœ€æ‘˜è¦"

    if not AI_API_KEY:
        print("  âš ï¸ æœªé…ç½® OPENAI_API_KEYï¼ˆSiliconFlow API Keyï¼‰ï¼Œè·³è¿‡ AI æ‘˜è¦ã€‚")
        return fallback_title or "ï¼ˆæœªé…ç½® AI æ‘˜è¦ï¼‰"

    headers = {
        "Accept": "application/json",
        "Authorization": f"Bearer {AI_API_KEY}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": AI_MODEL,
        "messages": [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¸­æ–‡æ–°é—»ç¼–è¾‘ï¼Œè¯·å°†æ–°é—»æ­£æ–‡æç‚¼æˆä¸€å¥ä¸­æ–‡æ‘˜è¦ï¼Œ"
                    "è¦æ±‚ï¼šå®¢è§‚ã€ä¸å¤¸å¼ ã€ä¸æ ‡é¢˜å…šï¼Œé•¿åº¦æ§åˆ¶åœ¨ 25 ä¸ªå­—ä»¥å†…ã€‚"
                ),
            },
            {
                "role": "user",
                "content": content[:2000],
            },
        ],
        "max_tokens": 120,
        "temperature": 0.3,
    }

    print(f"  ğŸ¤– æ­£åœ¨è°ƒç”¨ AIï¼ˆ{AI_CHAT_URL}ï¼‰ç”Ÿæˆæ‘˜è¦...")

    try:
        resp = requests.post(AI_CHAT_URL, headers=headers, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()

        # å…¼å®¹ OpenAI é£æ ¼å“åº”
        summary = data["choices"][0]["message"]["content"].strip()
        summary = summary.splitlines()[0].strip()  # åªå–ç¬¬ä¸€è¡Œ
        print(f"  âœ¨ AI æ‘˜è¦ï¼š{summary}")
        return summary or (fallback_title or "ï¼ˆAI æ‘˜è¦ä¸ºç©ºï¼‰")

    except Exception as e:
        print(f"  âš ï¸ AI è°ƒç”¨å¤±è´¥ï¼š{e}")
        return fallback_title or f"[AI è°ƒç”¨å¤±è´¥: {e}]"


# ================== åˆ—è¡¨æŠ“å– ==================


def fetch_list(page: int = 1):
    """
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ï¼Œä½¿ç”¨æ­£ç¡®çš„ç›¸å¯¹è·¯å¾„æ‹¼æ¥ã€‚
    ä¿ç•™ä½ åŸæ¥ V8 ç‰ˆæœ¬çš„è§£æé€»è¾‘ã€‚
    """
    # æ„é€ å½“å‰åˆ—è¡¨é¡µçš„å®Œæ•´ URL
    if page == 1:
        current_list_url = LIST_URL_BASE
    else:
        current_list_url = f"{LIST_URL_BASE}?page={page}"

    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ({current_list_url}) ---")

    try:
        r = requests.get(current_list_url, headers=DEFAULT_HEADERS, timeout=15)
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []

    # è¿™é‡Œä½¿ç”¨ä½ ä¹‹å‰éªŒè¯è¿‡èƒ½ç”¨çš„é€‰æ‹©å™¨
    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")

        if not (h2 and a and date_div):
            continue

        # è·å–åŸå§‹ href (ä¾‹å¦‚: "c/2025-12/07/content_470761.htm")
        href = a["href"].strip()
        pub_date = date_div.get_text(strip=True) if date_div else ""

        # 1. æ—¥æœŸè¿‡æ»¤ï¼šåªå¤„ç† TARGET_DATE
        if pub_date != TARGET_DATE:
            continue

        # 2. ç®€å•çš„æ­£åˆ™æ£€æŸ¥ï¼Œåªè¦åŒ…å« content_æ•°å­— å³å¯
        if not re.search(r"content_\d+\.htm", href):
            continue

        # 3. ä½¿ç”¨ current_list_url è¿›è¡Œæ‹¼æ¥
        url_full = urljoin(current_list_url, href)

        items.append(
            {
                "title": h2.get_text(strip=True),
                "url": url_full,
                "date": pub_date,
                "content": "",
                "ai_summary": "",
            }
        )

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°ç›®æ ‡æ—¥æœŸ({TARGET_DATE})æ–‡ç« æ•°ï¼š{len(items)}")
    return items


# ================== æ­£æ–‡æŠ“å– ==================


def fetch_article_content(item: dict):
    """
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹
    """
    url = item["url"]
    headers = DEFAULT_HEADERS.copy()
    # åŠ ä¸Š Refererï¼Œæ¨¡æ‹Ÿä»åˆ—è¡¨é¡µç‚¹è¿‡å»
    headers["Referer"] = LIST_URL_BASE

    for attempt in range(MAX_RETRY):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()

            soup = BeautifulSoup(r.text, "html.parser")
            # å°è¯•å¤šç§æ­£æ–‡é€‰æ‹©å™¨ï¼Œä»¥é˜²é¡µé¢ç»“æ„å¾®è°ƒ
            container = soup.select_one("div.article-mod div.word-text-con")
            if not container:
                container = soup.select_one("div.article-content")  # å¤‡ç”¨é€‰æ‹©å™¨

            if not container:
                item["content"] = "[æ­£æ–‡å®¹å™¨æœªæ‰¾åˆ°]"
                print(f"  âš ï¸ è­¦å‘Šï¼šURL {url} è®¿é—®æˆåŠŸä½†æœªæ‰¾åˆ°æ­£æ–‡å®¹å™¨")
                return

            paras = [
                p.get_text(strip=True)
                for p in container.find_all("p")
                if p.get_text(strip=True)
            ]
            item["content"] = "\n".join(paras)
            time.sleep(0.5)
            return

        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRY - 1:
                print(
                    f"  âŒ è¯·æ±‚å¤±è´¥ ({r.status_code if 'r' in locals() else 'Error'}), é‡è¯•ä¸­...: {url}"
                )
                time.sleep(1)
            else:
                print(f"  â›”ï¸ æœ€ç»ˆå¤±è´¥: {url} | é”™è¯¯: {e}")
                item["content"] = f"[è·å–å¤±è´¥: {e}]"


# ================== CSV ä¿å­˜ ==================


def save_to_csv(data: list, filename: str):
    if not data:
        print("ğŸ’¡ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ã€‚")
        return
    fieldnames = ["title", "ai_summary", "date", "url", "content"]
    try:
        with open(filename, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"\nğŸ‰ æˆåŠŸä¿å­˜åˆ° CSVï¼š{filename}ï¼Œå…± {len(data)} æ¡ã€‚")
    except Exception as e:
        print(f"\nâŒ CSV ä¿å­˜å¤±è´¥ï¼š{e}")


# ================== ç”Ÿæˆ Markdown ==================


def build_markdown(items: list) -> str:
    """
    ç”Ÿæˆä¸€ä¸ª Markdown å­—ç¬¦ä¸²ï¼š
    - é¡¶éƒ¨æ˜¯æ ‡é¢˜
    - æ¯ä¸€è¡Œéƒ½æ˜¯ï¼š1. [AI æ‘˜è¦](URL)
    """
    if not items:
        return f"### è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{TARGET_DATE}ï¼‰\n\nä»Šæ—¥æœªæŠ“åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°é—»ã€‚"

    lines = [
        f"### è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{TARGET_DATE}ï¼‰",
        "",
    ]

    for idx, item in enumerate(items, start=1):
        title = item.get("ai_summary") or item.get("title") or "ï¼ˆæ— æ ‡é¢˜ï¼‰"
        url = item.get("url", "")
        lines.append(f"{idx}. [{title}]({url})")

    return "\n".join(lines)


def save_markdown(content: str, filename: str):
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"\nğŸ“„ å·²ä¿å­˜ Markdown åˆ°æ–‡ä»¶ï¼š{filename}")
    except Exception as e:
        print(f"\nâŒ Markdown æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{e}")


# ================== ä¸»æµç¨‹ ==================


def main():
    all_articles = []
    print(f"=== ğŸš€ çˆ¬è™«å¯åŠ¨ (ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}) ===")
    print(f"=== ğŸ› ï¸ è·¯å¾„ç­–ç•¥: åŸºäºåˆ—è¡¨é¡µ URL ({LIST_URL_BASE}) è¿›è¡Œç›¸å¯¹è·¯å¾„æ‹¼æ¥ ===")

    # 1. æŠ“å–åˆ—è¡¨
    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)
        if not list_items:
            if page == 1:
                print(
                    f"âš ï¸ ç¬¬ 1 é¡µæœªæ‰¾åˆ° {TARGET_DATE} çš„æ–‡ç« ï¼Œè¯·ç¡®è®¤ç½‘ç«™ä¸Šç¡®å®æœ‰è¯¥æ—¥æœŸçš„å†…å®¹ã€‚"
                )
            break
        all_articles.extend(list_items)
        time.sleep(1)

    print(
        f"\n=== ğŸ“¥ é“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡ã€‚å¼€å§‹æŠ“å–æ­£æ–‡ + ç”Ÿæˆ AI æ‘˜è¦... ==="
    )

    # 2. æŠ“å–æ­£æ–‡ + AI æ‘˜è¦
    count = 0
    for item in all_articles:
        count += 1
        print(f"\nğŸ”¥ ({count}/{len(all_articles)}) å¤„ç†: {item['title']}")
        fetch_article_content(item)
        item["ai_summary"] = get_ai_summary(item["content"], item["title"])

    # 3. ç»Ÿè®¡ä¸ä¿å­˜ CSV
    success_count = sum(
        1
        for item in all_articles
        if "è·å–å¤±è´¥" not in item["content"] and item["content"]
    )
    print(f"\n=== ç»Ÿè®¡: æˆåŠŸ {success_count} ç¯‡ï¼Œå¤±è´¥ {len(all_articles) - success_count} ç¯‡ ===")
    save_to_csv(all_articles, OUTPUT_FILENAME)

    # 4. ç”Ÿæˆ Markdown é¢„è§ˆ & ä¿å­˜
    md_content = build_markdown(all_articles)
    print("\n=== Markdown é¢„è§ˆï¼ˆå¯ç”¨äºé’‰é’‰ Markdown æ¶ˆæ¯ï¼‰ ===\n")
    print(md_content)
    save_markdown(md_content, OUTPUT_MD)


if __name__ == "__main__":
    main()
