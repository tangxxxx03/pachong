# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆæ”¯æŒ GB ç¼–ç  + æ­£æ–‡æŠ“å–ï¼‰
"""

import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE = "https://www.fortunechina.com"

session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120 Safari/537.36",
})


def fetch_html(url):
    """æŠ“å–ç½‘é¡µå¹¶è‡ªåŠ¨è½¬æˆæ­£ç¡®ä¸­æ–‡ç¼–ç """
    r = session.get(url, timeout=20)
    r.encoding = "GB18030"   # ğŸ‘ˆ å¼ºåˆ¶ä¸­æ–‡ç¼–ç ï¼ˆå…³é”®ï¼ï¼‰
    return r.text


def fetch_list():
    """æŠ“æ–‡ç« åˆ—è¡¨"""
    url = f"{BASE}/shangye/"
    print("è¯·æ±‚åˆ—è¡¨é¡µï¼š", url)

    html = fetch_html(url)
    soup = BeautifulSoup(html, "lxml")

    items = []

    # åˆ—è¡¨ç»“æ„å¾ˆç»Ÿä¸€ï¼šli > h2 > a
    for li in soup.select("div.mod-list li"):
        a = li.find("a", href=True)
        if not a:
            continue

        href = a["href"]
        if not href.startswith("/shangye/c/"):
            continue

        title = a.get_text(strip=True)
        link = urljoin(BASE, href)

        # æ—¥æœŸ
        date_span = li.find("span", class_=re.compile("time|date"))
        pub_date = date_span.get_text(strip=True) if date_span else ""

        items.append({
            "title": title,
            "url": link,
            "date": pub_date,
        })

    print("æˆåŠŸæŠ“åˆ°æ–‡ç« ï¼š", len(items))
    return items


def fetch_article(url):
    """æŠ“æ­£æ–‡"""
    html = fetch_html(url)
    soup = BeautifulSoup(html, "lxml")

    # å†…å®¹åœ¨ <div class="article-content"> æˆ– <div id="ContentBody">
    body = soup.select_one("div.article-content") or soup.select_one("#ContentBody")

    if body:
        text = "\n".join(p.get_text(strip=True) for p in body.find_all("p"))
    else:
        text = "(æœªæ‰¾åˆ°æ­£æ–‡)"

    return text


if __name__ == "__main__":
    items = fetch_list()
    print("\n=== æŠ“å–å‰ 5 ç¯‡æ–‡ç« æ­£æ–‡ ===")
    for art in items[:5]:
        print("\næ ‡é¢˜ï¼š", art["title"])
        print("é“¾æ¥ï¼š", art["url"])
        content = fetch_article(art["url"])
        print("æ­£æ–‡å‰ 100 å­—ï¼š", content[:100])
