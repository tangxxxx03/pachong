# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆï¼‰ + SiliconFlow AI æ‘˜è¦ + é’‰é’‰ Markdown æ¨é€

åŠŸèƒ½ï¼š
1. æŠ“å–è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“æŒ‡å®šæ—¥æœŸçš„æ–°é—»ï¼ˆé»˜è®¤æŠ“â€œåŒ—äº¬æ—¶é—´æ˜¨å¤©â€çš„ï¼‰ã€‚
2. ä¿®å¤åˆ—è¡¨é¡µ href ç›¸å¯¹è·¯å¾„ï¼ˆc/2025-12/07/...ï¼‰ä¸¢å¤± /shangye/ çš„é—®é¢˜ã€‚
3. è°ƒç”¨ SiliconFlowï¼ˆOpenAI å…¼å®¹æ¥å£ï¼‰ç”Ÿæˆã€Œä¸€å¥è¯ä¸­æ–‡æ‘˜è¦ã€ã€‚
4. å¯¼å‡º CSVï¼ˆæ ‡é¢˜ + AI æ‘˜è¦ + æ—¥æœŸ + URL + æ­£æ–‡ï¼‰ã€‚
5. ç”Ÿæˆ Markdown åˆ—è¡¨ï¼ˆæ¯æ¡ [AI æ‘˜è¦](URL)ï¼‰ã€‚
6. å°† Markdown å†…å®¹é€šè¿‡é’‰é’‰æœºå™¨äººæ¨é€åˆ°ç¾¤é‡Œï¼ˆæ”¯æŒå¤šæœºå™¨äººï¼‰ã€‚

ä¾èµ–ï¼ˆrequirements.txtï¼‰ï¼š
- requests
- beautifulsoup4
"""

import os
import re
import time
import csv
import hmac
import base64
import hashlib
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin, quote_plus

import requests
from bs4 import BeautifulSoup

# ============= æŠ“å–åŸºç¡€é…ç½® =============

BASE = "https://www.fortunechina.com"
LIST_URL_BASE = "https://www.fortunechina.com/shangye/"
MAX_PAGES = 1
MAX_RETRY = 3

OUTPUT_CSV = "fortunechina_articles_with_ai_title.csv"
OUTPUT_MD = "fortunechina_articles_with_ai_title.md"


def get_target_date() -> str:
    """
    å†³å®šè¦æŠ“å–çš„ç›®æ ‡æ—¥æœŸï¼š
    1. å¦‚æœè®¾ç½®äº†ç¯å¢ƒå˜é‡ TARGET_DATEï¼ˆä¾‹å¦‚ "2025-12-07"ï¼‰ï¼Œä¼˜å…ˆç”¨å®ƒï¼›
    2. å¦åˆ™é»˜è®¤æŠ“ã€ŒåŒ—äº¬æ—¶é—´æ˜¨å¤©ã€ï¼Œæ ¼å¼ YYYY-MM-DDã€‚
    """
    env_date = os.getenv("TARGET_DATE", "").strip()
    if env_date:
        return env_date

    tz_cn = timezone(timedelta(hours=8))
    yesterday_cn = (datetime.now(tz_cn) - timedelta(days=1)).strftime("%Y-%m-%d")
    return yesterday_cn


TARGET_DATE = get_target_date()

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": "no-cache",
}

# ============= SiliconFlow AI é…ç½® =============

# ä½ çš„ sk- å¼€å¤´çš„ Keyï¼ˆä» GitHub Secrets çš„ OPENAI_API_KEY ä¼ è¿›æ¥ï¼‰
AI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()

# å•†å®¶ç»™çš„åŸºç¡€åœ°å€ï¼šhttps://api.siliconflow.cn/v1
AI_API_BASE = os.getenv("AI_API_BASE", "https://api.siliconflow.cn/v1").rstrip("/")

# ChatCompletions å®Œæ•´ URL
AI_CHAT_URL = f"{AI_API_BASE}/chat/completions"

# æ¨¡å‹ï¼šå¦‚æœä½ åœ¨å•†å®¶åå°çœ‹åˆ°åˆ«çš„ï¼Œå°±å¡«å®Œæ•´æ¨¡å‹ååˆ° Secrets çš„ AI_MODEL
AI_MODEL = os.getenv("AI_MODEL", "Qwen/Qwen2.5-7B-Instruct")


def get_ai_summary(content: str, fallback_title: str = "") -> str:
    """
    ä½¿ç”¨ SiliconFlow ç”Ÿæˆä¸€å¥è¯æ‘˜è¦ã€‚
    - content: æ–‡ç« æ­£æ–‡
    - fallback_title: AI å¤±è´¥æ—¶ç”¨åŸå§‹æ ‡é¢˜å…œåº•
    """
    if not content or len(content) < 30:
        return fallback_title or "å†…å®¹è¿‡çŸ­ï¼Œæ— éœ€æ‘˜è¦"

    if not AI_API_KEY:
        print("  âš ï¸ æœªé…ç½® OPENAI_API_KEYï¼Œè·³è¿‡ AI æ‘˜è¦ã€‚")
        return fallback_title or "ï¼ˆæœªé…ç½® AI æ‘˜è¦ï¼‰"

    headers = {
        "Accept": "application/json",
        "Authorization": f"Bearer {AI_API_KEY}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": AI_MODEL,
        "messages": [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¸­æ–‡æ–°é—»ç¼–è¾‘ï¼Œè¯·å°†æ–°é—»æ­£æ–‡æç‚¼æˆä¸€å¥ä¸­æ–‡æ‘˜è¦ï¼Œ"
                    "è¦æ±‚ï¼šå®¢è§‚ã€åŠ¡å®ã€ä¸æ ‡é¢˜å…šï¼Œé•¿åº¦æ§åˆ¶åœ¨ 25 ä¸ªå­—ä»¥å†…ã€‚"
                ),
            },
            {
                "role": "user",
                "content": content[:2000],
            },
        ],
        "max_tokens": 120,
        "temperature": 0.3,
    }

    print(f"  ğŸ¤– æ­£åœ¨è°ƒç”¨ AIï¼ˆ{AI_CHAT_URL}ï¼Œæ¨¡å‹={AI_MODEL}ï¼‰ç”Ÿæˆæ‘˜è¦...")

    try:
        resp = requests.post(AI_CHAT_URL, headers=headers, json=payload, timeout=30)

        if resp.status_code != 200:
            print(f"  âŒ AI çŠ¶æ€ç ï¼š{resp.status_code}")
            try:
                print("  âŒ AI è¿”å›å†…å®¹ï¼š", resp.text)
            except Exception:
                pass
            resp.raise_for_status()

        data = resp.json()
        summary = data["choices"][0]["message"]["content"].strip()
        summary = summary.splitlines()[0].strip()
        print(f"  âœ¨ AI æ‘˜è¦ï¼š{summary}")
        return summary or (fallback_title or "ï¼ˆAI æ‘˜è¦ä¸ºç©ºï¼‰")

    except Exception as e:
        print(f"  âš ï¸ AI è°ƒç”¨å¤±è´¥ï¼š{e}")
        return fallback_title or f"[AI è°ƒç”¨å¤±è´¥: {e}]"


# ============= åˆ—è¡¨é¡µæŠ“å– =============


def fetch_list(page: int = 1):
    """
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ï¼ˆä½¿ç”¨ current_list_url ä¿®å¤ç›¸å¯¹è·¯å¾„ï¼‰ã€‚
    """
    if page == 1:
        current_list_url = LIST_URL_BASE
    else:
        current_list_url = f"{LIST_URL_BASE}?page={page}"

    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ({current_list_url}) ---")

    try:
        r = requests.get(current_list_url, headers=DEFAULT_HEADERS, timeout=15)
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []

    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")

        if not (h2 and a and date_div):
            continue

        href = a["href"].strip()
        pub_date = date_div.get_text(strip=True) if date_div else ""

        # åªè¦ç›®æ ‡æ—¥æœŸçš„
        if pub_date != TARGET_DATE:
            continue

        # åªè¦åŒ…å« content_æ•°å­— çš„é“¾æ¥
        if not re.search(r"content_\d+\.htm", href):
            continue

        url_full = urljoin(current_list_url, href)

        items.append(
            {
                "title": h2.get_text(strip=True),
                "url": url_full,
                "date": pub_date,
                "content": "",
                "ai_summary": "",
            }
        )

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°ç›®æ ‡æ—¥æœŸ({TARGET_DATE})æ–‡ç« æ•°ï¼š{len(items)}")
    return items


# ============= æ­£æ–‡æŠ“å– =============


def fetch_article_content(item: dict):
    """
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹
    """
    url = item["url"]
    headers = DEFAULT_HEADERS.copy()
    headers["Referer"] = LIST_URL_BASE

    for attempt in range(MAX_RETRY):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()

            soup = BeautifulSoup(r.text, "html.parser")
            container = soup.select_one("div.article-mod div.word-text-con")
            if not container:
                container = soup.select_one("div.article-content")

            if not container:
                item["content"] = "[æ­£æ–‡å®¹å™¨æœªæ‰¾åˆ°]"
                print(f"  âš ï¸ è­¦å‘Šï¼šURL {url} è®¿é—®æˆåŠŸä½†æœªæ‰¾åˆ°æ­£æ–‡å®¹å™¨")
                return

            paras = [
                p.get_text(strip=True)
                for p in container.find_all("p")
                if p.get_text(strip=True)
            ]
            item["content"] = "\n".join(paras)
            time.sleep(0.5)
            return

        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRY - 1:
                print(
                    f"  âŒ è¯·æ±‚å¤±è´¥ ({r.status_code if 'r' in locals() else 'Error'}), é‡è¯•ä¸­...: {url}"
                )
                time.sleep(1)
            else:
                print(f"  â›”ï¸ æœ€ç»ˆå¤±è´¥: {url} | é”™è¯¯: {e}")
                item["content"] = f"[è·å–å¤±è´¥: {e}]"


# ============= ä¿å­˜ CSV =============


def save_to_csv(data: list, filename: str):
    if not data:
        print("ğŸ’¡ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ã€‚")
        return

    fieldnames = ["title", "ai_summary", "date", "url", "content"]
    try:
        with open(filename, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"\nğŸ‰ æˆåŠŸä¿å­˜åˆ° CSVï¼š{filename}ï¼Œå…± {len(data)} æ¡ã€‚")
    except Exception as e:
        print(f"\nâŒ CSV ä¿å­˜å¤±è´¥ï¼š{e}")


# ============= ç”Ÿæˆ Markdown =============


def build_markdown(items: list) -> str:
    """
    ç”Ÿæˆé€‚åˆé’‰é’‰å‘é€çš„ Markdown æ–‡æœ¬ã€‚
    """
    if not items:
        return f"### è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{TARGET_DATE}ï¼‰\n\nä»Šæ—¥æœªæŠ“åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°é—»ã€‚"

    lines = [
        f"### è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{TARGET_DATE}ï¼‰",
        "",
    ]

    for idx, item in enumerate(items, start=1):
        title = item.get("ai_summary") or item.get("title") or "ï¼ˆæ— æ ‡é¢˜ï¼‰"
        url = item.get("url", "")
        lines.append(f"{idx}. [{title}]({url})")

    return "\n".join(lines)


def save_markdown(content: str, filename: str):
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"\nğŸ“„ å·²ä¿å­˜ Markdown æ–‡ä»¶ï¼š{filename}")
    except Exception as e:
        print(f"\nâŒ Markdown ä¿å­˜å¤±è´¥ï¼š{e}")


# ============= é’‰é’‰ Markdown æ¨é€ =============


def sign_dingtalk(secret: str, timestamp_ms: int) -> str:
    """
    æŒ‰é’‰é’‰å®˜æ–¹æ–‡æ¡£ç”Ÿæˆç­¾åã€‚
    """
    string_to_sign = f"{timestamp_ms}\n{secret}"
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign.encode("utf-8"), digestmod=hashlib.sha256).digest()
    return quote_plus(base64.b64encode(hmac_code))


def send_dingtalk_markdown(title: str, text: str):
    """
    å°† Markdown æ–‡æœ¬å‘é€åˆ°ä¸€ä¸ªæˆ–å¤šä¸ªé’‰é’‰æœºå™¨äººã€‚
    éœ€è¦ç¯å¢ƒå˜é‡ï¼š
    - DINGTALK_BASES   : webhook åŸºç¡€ URLï¼Œå¤šä¸ªç”¨è‹±æ–‡é€—å·åˆ†éš”
    - DINGTALK_SECRETS : å¯¹åº”çš„ secretï¼Œå¤šä¸ªç”¨è‹±æ–‡é€—å·åˆ†éš”
    """
    bases_raw = os.getenv("DINGTALK_BASES", "").strip()
    secrets_raw = os.getenv("DINGTALK_SECRETS", "").strip()

    if not bases_raw or not secrets_raw:
        print("ğŸ’¡ æœªé…ç½® DINGTALK_BASES / DINGTALK_SECRETSï¼Œè·³è¿‡é’‰é’‰æ¨é€ã€‚")
        return

    bases = [b.strip() for b in bases_raw.split(",") if b.strip()]
    secrets = [s.strip() for s in secrets_raw.split(",") if s.strip()]

    if not bases or len(bases) != len(secrets):
        print("âš ï¸ DINGTALK_BASES ä¸ DINGTALK_SECRETS æ•°é‡ä¸ä¸€è‡´ï¼Œè·³è¿‡é’‰é’‰æ¨é€ã€‚")
        return

    for idx, (base_url, secret) in enumerate(zip(bases, secrets), start=1):
        try:
            ts = int(time.time() * 1000)
            sign = sign_dingtalk(secret, ts)
            full_url = f"{base_url}&timestamp={ts}&sign={sign}"

            payload = {
                "msgtype": "markdown",
                "markdown": {
                    "title": title,
                    "text": text,
                },
                "at": {
                    "isAtAll": False,
                },
            }

            print(f"\nğŸ“¨ æ­£åœ¨å‘ç¬¬ {idx} ä¸ªé’‰é’‰æœºå™¨äººå‘é€æ¶ˆæ¯...")
            resp = requests.post(full_url, json=payload, timeout=10)
            print(f"  é’‰é’‰è¿”å›çŠ¶æ€ç ï¼š{resp.status_code}")
            try:
                print("  é’‰é’‰è¿”å›ï¼š", resp.text)
            except Exception:
                pass

        except Exception as e:
            print(f"  âš ï¸ ç¬¬ {idx} ä¸ªé’‰é’‰æœºå™¨äººå‘é€å¤±è´¥ï¼š{e}")


# ============= ä¸»æµç¨‹ =============


def main():
    all_articles = []
    print(f"=== ğŸš€ çˆ¬è™«å¯åŠ¨ (ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}) ===")
    print(f"=== ğŸ› ï¸ è·¯å¾„ç­–ç•¥: åŸºäºåˆ—è¡¨é¡µ URL ({LIST_URL_BASE}) è¿›è¡Œç›¸å¯¹è·¯å¾„æ‹¼æ¥ ===")

    # 1. æŠ“å–åˆ—è¡¨
    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)
        if not list_items:
            if page == 1:
                print(
                    f"âš ï¸ ç¬¬ 1 é¡µæœªæ‰¾åˆ° {TARGET_DATE} çš„æ–‡ç« ï¼Œè¯·ç¡®è®¤ç½‘ç«™ä¸Šç¡®å®æœ‰è¯¥æ—¥æœŸçš„å†…å®¹ã€‚"
                )
            break
        all_articles.extend(list_items)
        time.sleep(1)

    print(
        f"\n=== ğŸ“¥ é“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡ã€‚å¼€å§‹æŠ“å–æ­£æ–‡ + ç”Ÿæˆ AI æ‘˜è¦... ==="
    )

    # 2. æŠ“å–æ­£æ–‡ + AI æ‘˜è¦
    count = 0
    for item in all_articles:
        count += 1
        print(f"\nğŸ”¥ ({count}/{len(all_articles)}) å¤„ç†: {item['title']}")
        fetch_article_content(item)
        item["ai_summary"] = get_ai_summary(item["content"], item["title"])

    # 3. ç»Ÿè®¡ä¸ä¿å­˜ CSV
    success_count = sum(
        1
        for item in all_articles
        if "è·å–å¤±è´¥" not in item["content"] and item["content"]
    )
    print(f"\n=== ç»Ÿè®¡: æˆåŠŸ {success_count} ç¯‡ï¼Œå¤±è´¥ {len(all_articles) - success_count} ç¯‡ ===")
    save_to_csv(all_articles, OUTPUT_CSV)

    # 4. ç”Ÿæˆ Markdown
    md_content = build_markdown(all_articles)
    print("\n=== Markdown é¢„è§ˆï¼ˆå¯ç”¨äºé’‰é’‰ Markdown æ¶ˆæ¯ï¼‰ ===\n")
    print(md_content)
    save_markdown(md_content, OUTPUT_MD)

    # 5. æ¨é€åˆ°é’‰é’‰
    md_title = f"è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{TARGET_DATE}ï¼‰"
    send_dingtalk_markdown(md_title, md_content)


if __name__ == "__main__":
    main()
