# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆç»“æ„ï¼‰- V3 å¢å¼ºç‰ˆ

åŠŸèƒ½ï¼š
1. ä¼˜åŒ– URL æå–é€»è¾‘ï¼Œè§£å†³ 404 é”™è¯¯ã€‚
2. æ”¯æŒå¤šé¡µåˆ—è¡¨æŠ“å–ï¼ˆé»˜è®¤å‰ 3 é¡µï¼‰ã€‚
3. è‡ªåŠ¨è·å–æ¯ç¯‡æ–‡ç« çš„å®Œæ•´æ­£æ–‡ã€‚
4. **æ–°å¢ï¼šå°†ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶ã€‚**
"""

import re
import time
import requests
import csv
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# --- é…ç½®å‚æ•° ---
BASE = "https://www.fortunechina.com"
MAX_PAGES = 3  # è®¾ç½®æ‚¨å¸Œæœ›æŠ“å–çš„æœ€å¤§é¡µæ•°
MAX_RETRY = 3  # æ­£æ–‡è¯·æ±‚å¤±è´¥æœ€å¤§é‡è¯•æ¬¡æ•°
OUTPUT_FILENAME = "fortunechina_articles.csv" # CSV æ–‡ä»¶å
# ----------------

session = requests.Session()
session.headers.update({
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120 Safari/537.36"
    ),
    "Accept-Encoding": "gzip, deflate, br",
})


def fetch_list(page=1):
    """
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ã€‚
    """
    url = f"{BASE}/shangye/" if page == 1 else f"{BASE}/shangye/?page={page}"
    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ---")

    try:
        r = session.get(url, timeout=15)
        r.raise_for_status() 
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥ ({url}): {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []
    
    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")
        
        if not (h2 and a):
            continue
            
        href = a["href"].strip()
        
        # ä¸¥æ ¼æ ¡éªŒé“¾æ¥æ ¼å¼ï¼š/YYYY-MM/DD/content_ID.htm
        if not re.search(r"/\d{4}-\d{2}/\d{2}/content_\d+\.htm", href):
            continue

        title = h2.get_text(strip=True)
        url_full = urljoin(BASE, href) 
        pub_date = date_div.get_text(strip=True) if date_div else ""

        items.append({
            "title": title,
            "url": url_full,
            "date": pub_date,
            "content": "",
        })

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°æ–‡ç« æ•°ï¼š{len(items)}")
    return items


def fetch_article_content(item):
    """
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œå¹¶åŒ…å«å¤±è´¥é‡è¯•æœºåˆ¶ã€‚
    """
    url = item["url"]
    
    for attempt in range(MAX_RETRY):
        try:
            r = session.get(url, timeout=15)
            r.raise_for_status() 

            soup = BeautifulSoup(r.text, "html.parser")
            container = soup.select_one("div.article-mod div.word-text-con")

            if not container:
                item["content"] = "[æ­£æ–‡å†…å®¹å®¹å™¨æœªæ‰¾åˆ°]"
                return

            paras = [p.get_text(strip=True) for p in container.find_all("p") if p.get_text(strip=True)]
            item["content"] = "\n".join(paras)
            time.sleep(0.5) 
            return

        except requests.exceptions.RequestException as e:
            if r.status_code == 404:
                print(f"  âš ï¸ 404 é“¾æ¥æ— æ•ˆï¼Œæ”¾å¼ƒé‡è¯•ï¼š{url}")
                item["content"] = f"[æ­£æ–‡è·å–å¤±è´¥: 404 Not Found]"
                return
            
            print(f"  âŒ æ­£æ–‡è¯·æ±‚å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•ç¬¬ {attempt + 1}/{MAX_RETRY} æ¬¡ ({url}): {e}")
            time.sleep(2 ** attempt) 
            
    item["content"] = f"[æ­£æ–‡è·å–å¤±è´¥: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°]"
    print(f"  â›”ï¸ æ­£æ–‡è·å–å¤±è´¥ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼š{url}")


def save_to_csv(data: list, filename: str):
    """
    å°†æ–‡ç« æ•°æ®åˆ—è¡¨ä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­ã€‚
    """
    if not data:
        print("ğŸ’¡ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ã€‚")
        return
        
    # å®šä¹‰ CSV æ–‡ä»¶çš„è¡¨å¤´ï¼ˆåˆ—åï¼‰
    fieldnames = ["title", "date", "url", "content"]
    
    try:
        # 'w' å†™å…¥æ¨¡å¼ï¼Œnewline='' ç¡®ä¿åœ¨ Windows ä¸Šä¸ä¼šå‡ºç°é¢å¤–çš„ç©ºè¡Œ
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            # å†™å…¥è¡¨å¤´
            writer.writeheader()

            # å†™å…¥æ¯ä¸€è¡Œæ•°æ®
            writer.writerows(data)
            
        print(f"\nğŸ‰ æ•°æ®ä¿å­˜æˆåŠŸï¼æ–‡ä»¶åä¸ºï¼š{filename}ï¼Œå…± {len(data)} æ¡è®°å½•ã€‚")
    except Exception as e:
        print(f"\nâŒ CSV æ–‡ä»¶å†™å…¥å¤±è´¥ï¼š{e}")


def main():
    all_articles = []
    
    print(f"=== ğŸš€ è´¢å¯Œä¸­æ–‡ç½‘çˆ¬è™«å¼€å§‹æ‰§è¡Œï¼ˆç›®æ ‡é¡µæ•°ï¼š{MAX_PAGES}ï¼‰ ===")

    # 1. å®ç°å¤šé¡µæŠ“å–å¾ªç¯
    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)
        
        if not list_items:
            print(f"--- ç¬¬ {page} é¡µæ²¡æœ‰æŠ“åˆ°æ–‡ç« ï¼Œåœæ­¢ç¿»é¡µ ---")
            break
            
        all_articles.extend(list_items)
        time.sleep(1) 
    
    print(f"\n=== ğŸ“¥ åˆ—è¡¨æŠ“å–å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(all_articles)} ç¯‡æ–‡ç« é“¾æ¥ã€‚===")

    # 2. éå†æ‰€æœ‰æ–‡ç« ï¼ŒæŠ“å–æ­£æ–‡
    count = 0
    for item in all_articles:
        count += 1
        print(f"ğŸ”¥ æ­£åœ¨å¤„ç†ç¬¬ {count}/{len(all_articles)} ç¯‡ï¼š{item['title']}")
        fetch_article_content(item)
        
    print("\n=== ğŸ¯ æ­£æ–‡æŠ“å–å®Œæˆï¼Œé¢„è§ˆå‰ 5 ç¯‡æ–‡ç« ï¼š===")

    # 3. æ‰“å°å‰ 5 ç¯‡æ–‡ç« ï¼ŒéªŒè¯æ•°æ®å®Œæ•´æ€§ (çœç•¥ï¼Œç¡®ä¿æµç¨‹æµç•…)
    for item in all_articles[:5]:
        print("---")
        print(f"æ ‡é¢˜: {item['title']}")
        content_preview = item["content"][:200] + "..." if len(item["content"]) > 200 else item["content"]
        print(f"æ­£æ–‡é¢„è§ˆ: {content_preview}")
        
    # 4. ç»Ÿè®¡å¤±è´¥æ–‡ç« æ•°
    failed_count = sum(1 for item in all_articles if item["content"].startswith("[æ­£æ–‡è·å–å¤±è´¥"))
    print(f"\n=== ç»Ÿè®¡ç»“æœï¼šæˆåŠŸè·å– {len(all_articles) - failed_count} ç¯‡ï¼Œå¤±è´¥ {failed_count} ç¯‡ã€‚===")

    # 5. ã€æ–°å¢ã€‘ä¿å­˜ä¸º CSV æ–‡ä»¶
    save_to_csv(all_articles, OUTPUT_FILENAME)
    
    return all_articles


if __name__ == "__main__":
    main()
