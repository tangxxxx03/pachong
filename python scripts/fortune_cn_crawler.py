# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆç»“æ„ï¼‰- V8 è·¯å¾„æ‹¼æ¥ç»ˆæä¿®æ­£ç‰ˆ

æ ¸å¿ƒä¿®å¤ï¼š
1. ã€å…³é”®ã€‘URL æ‹¼æ¥ä¸å†åŸºäº BASEï¼Œè€Œæ˜¯åŸºäºåˆ—è¡¨é¡µ URL (current_list_url)ã€‚
   è§£å†³äº† href="c/..." ç›¸å¯¹è·¯å¾„å¯¼è‡´ä¸¢å¤± /shangye/ ç›®å½•çš„é—®é¢˜ã€‚
2. æ—¥æœŸé™å®šï¼šä¸¥æ ¼æŠ“å– 2025-12-07 çš„æ–‡ç« ã€‚
3. å¤´éƒ¨å¢å¼ºï¼šç»§ç»­ä¿æŒé«˜ä»¿çœŸ User-Agentã€‚
"""

import re
import time
import requests
import csv
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime

# --- é…ç½®å‚æ•° ---
BASE = "https://www.fortunechina.com"
# ç¡®ä¿åˆ—è¡¨é¡µ URL ç»“å°¾æœ‰æ–œæ ï¼Œè¿™å¯¹ urljoin å¤„ç†ç›¸å¯¹è·¯å¾„éå¸¸é‡è¦
LIST_URL_BASE = "https://www.fortunechina.com/shangye/" 
MAX_PAGES = 3  
MAX_RETRY = 3 
OUTPUT_FILENAME = "fortunechina_articles.csv"
# é™å®šæ—¥æœŸ (æ ¹æ®ä½ çš„æˆªå›¾ï¼Œç›®æ ‡æ˜¯ 2025-12-07)
TARGET_DATE = "2025-12-07" 
# ----------------

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36" 
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": "no-cache",
}

def fetch_list(page=1):
    """
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ï¼Œä½¿ç”¨æ­£ç¡®çš„ç›¸å¯¹è·¯å¾„æ‹¼æ¥ã€‚
    """
    # æ„é€ å½“å‰åˆ—è¡¨é¡µçš„å®Œæ•´ URL
    if page == 1:
        current_list_url = LIST_URL_BASE
    else:
        current_list_url = f"{LIST_URL_BASE}?page={page}"
        
    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ({current_list_url}) ---")

    try:
        r = requests.get(current_list_url, headers=DEFAULT_HEADERS, timeout=15)
        r.raise_for_status() 
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []
    
    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")
        
        if not (h2 and a and date_div):
            continue
            
        # è·å–åŸå§‹ href (ä¾‹å¦‚: "c/2025-12/07/content_470761.htm")
        href = a["href"].strip()
        pub_date = date_div.get_text(strip=True) if date_div else ""

        # 1. æ—¥æœŸè¿‡æ»¤ï¼šåªå¤„ç† 2025-12-07
        if pub_date != TARGET_DATE:
            continue
            
        # 2. ç®€å•çš„æ­£åˆ™æ£€æŸ¥ï¼Œåªè¦åŒ…å« content_æ•°å­— å³å¯
        if not re.search(r"content_\d+\.htm", href):
            continue
        
        # 3. ã€æ ¸å¿ƒä¿®æ­£ã€‘ä½¿ç”¨ current_list_url è¿›è¡Œæ‹¼æ¥
        # å¦‚æœ href æ˜¯ "c/2025..."ï¼Œlist_url æ˜¯ ".../shangye/"
        # ç»“æœè‡ªåŠ¨å˜ä¸º ".../shangye/c/2025..."
        url_full = urljoin(current_list_url, href)
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œç¡®ä¿è·¯å¾„çœ‹èµ·æ¥æ­£ç¡®
        # print(f"  [è°ƒè¯•] åŸå§‹href: {href} -> æ‹¼æ¥å: {url_full}")

        items.append({
            "title": h2.get_text(strip=True),
            "url": url_full,
            "date": pub_date,
            "content": "",
        })

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°ç›®æ ‡æ—¥æœŸ({TARGET_DATE})æ–‡ç« æ•°ï¼š{len(items)}")
    return items


def fetch_article_content(item):
    """
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹
    """
    url = item["url"]
    headers = DEFAULT_HEADERS.copy()
    # åŠ ä¸Š Refererï¼Œæ¨¡æ‹Ÿä»åˆ—è¡¨é¡µç‚¹è¿‡å»
    headers["Referer"] = LIST_URL_BASE 
    
    for attempt in range(MAX_RETRY):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status() 

            soup = BeautifulSoup(r.text, "html.parser")
            # å°è¯•å¤šç§æ­£æ–‡é€‰æ‹©å™¨ï¼Œä»¥é˜²é¡µé¢ç»“æ„å¾®è°ƒ
            container = soup.select_one("div.article-mod div.word-text-con")
            if not container:
                container = soup.select_one("div.article-content") # å¤‡ç”¨é€‰æ‹©å™¨

            if not container:
                item["content"] = "[æ­£æ–‡å®¹å™¨æœªæ‰¾åˆ°]"
                print(f"  âš ï¸ è­¦å‘Šï¼šURL {url} è®¿é—®æˆåŠŸä½†æœªæ‰¾åˆ°æ­£æ–‡å®¹å™¨")
                return

            paras = [p.get_text(strip=True) for p in container.find_all("p") if p.get_text(strip=True)]
            item["content"] = "\n".join(paras)
            time.sleep(0.5) 
            return

        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRY - 1:
                print(f"  âŒ è¯·æ±‚å¤±è´¥ ({r.status_code if 'r' in locals() else 'Error'}), é‡è¯•ä¸­...: {url}")
                time.sleep(1)
            else:
                print(f"  â›”ï¸ æœ€ç»ˆå¤±è´¥: {url} | é”™è¯¯: {e}")
                item["content"] = f"[è·å–å¤±è´¥: {e}]"


def save_to_csv(data: list, filename: str):
    if not data:
        print("ğŸ’¡ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ã€‚")
        return
    fieldnames = ["title", "date", "url", "content"]
    try:
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"\nğŸ‰ æˆåŠŸä¿å­˜åˆ° CSVï¼š{filename}ï¼Œå…± {len(data)} æ¡ã€‚")
    except Exception as e:
        print(f"\nâŒ CSV ä¿å­˜å¤±è´¥ï¼š{e}")


def main():
    all_articles = []
    print(f"=== ğŸš€ çˆ¬è™«å¯åŠ¨ (ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}) ===")
    print(f"=== ğŸ› ï¸ ä¿®å¤ç­–ç•¥: åŸºäºåˆ—è¡¨é¡µ URL ({LIST_URL_BASE}) è¿›è¡Œç›¸å¯¹è·¯å¾„æ‹¼æ¥ ===")

    # 1. æŠ“å–åˆ—è¡¨
    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)
        if not list_items:
            # å¦‚æœç¬¬ä¸€é¡µå°±æ²¡æ•°æ®ï¼Œå¯èƒ½æ˜¯æ—¥æœŸä¸å¯¹ï¼Œæˆ–è€…æ²¡åŠ è½½å‡ºæ¥
            if page == 1: 
                print(f"âš ï¸ ç¬¬ 1 é¡µæœªæ‰¾åˆ° {TARGET_DATE} çš„æ–‡ç« ï¼Œè¯·ç¡®è®¤ç½‘ç«™ä¸Šç¡®å®æœ‰è¯¥æ—¥æœŸçš„å†…å®¹ã€‚")
            break
        all_articles.extend(list_items)
        time.sleep(1) 
    
    print(f"\n=== ğŸ“¥ é“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡ã€‚å¼€å§‹æŠ“å–æ­£æ–‡... ===")

    # 2. æŠ“å–æ­£æ–‡
    count = 0
    for item in all_articles:
        count += 1
        print(f"ğŸ”¥ ({count}/{len(all_articles)}) å¤„ç†: {item['title']}")
        fetch_article_content(item)
        
    # 3. ç»Ÿè®¡ä¸ä¿å­˜
    success_count = sum(1 for item in all_articles if "è·å–å¤±è´¥" not in item["content"] and item["content"])
    print(f"\n=== ç»Ÿè®¡: æˆåŠŸ {success_count} ç¯‡ï¼Œå¤±è´¥ {len(all_articles) - success_count} ç¯‡ ===")
    save_to_csv(all_articles, OUTPUT_FILENAME)

if __name__ == "__main__":
    main()
