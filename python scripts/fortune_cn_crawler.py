# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆç»“æ„ï¼‰- V6 æ—¥æœŸé™å®šä¸ç»ˆæä¿®å¤ç‰ˆ

åŠŸèƒ½ï¼š
1. ã€æ ¸å¿ƒä¿®å¤ã€‘ä½¿ç”¨åŠ¨æ€ã€é€¼çœŸçš„å¤´éƒ¨è¿›è¡Œæ­£æ–‡è¯·æ±‚ï¼Œè§£å†³ 404 é”™è¯¯ã€‚
2. ã€æ–°å¢åŠŸèƒ½ã€‘åªæŠ“å–å‘å¸ƒæ—¥æœŸä¸ºâ€œå½“å¤©â€çš„æ–‡ç« ã€‚
3. æ”¯æŒå¤šé¡µåˆ—è¡¨æŠ“å–ï¼ˆé»˜è®¤å‰ 3 é¡µï¼‰ã€‚
4. è‡ªåŠ¨è·å–æ¯ç¯‡æ–‡ç« çš„å®Œæ•´æ­£æ–‡ã€‚
5. å°†ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶ã€‚
"""

import re
import time
import requests
import csv
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime

# --- é…ç½®å‚æ•° ---
BASE = "https://www.fortunechina.com"
MAX_PAGES = 3  
MAX_RETRY = 3 
OUTPUT_FILENAME = "fortunechina_articles.csv"
TODAY_DATE = datetime.now().strftime("%Y-%m-%d") # æŠ“å–ä»Šå¤©å‘å¸ƒçš„æ–‡ç« 
# ----------------

# åˆ—è¡¨é¡µè¯·æ±‚å¤´éƒ¨ (ä½¿ç”¨ requests åº“è€Œä¸æ˜¯ session)
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36" 
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
}


def fetch_list(page=1):
    """
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ï¼Œå¹¶é™å®šæ—¥æœŸä¸ºå½“å¤©ã€‚
    """
    url = f"{BASE}/shangye/" if page == 1 else f"{BASE}/shangye/?page={page}"
    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ---")

    try:
        r = requests.get(url, headers=DEFAULT_HEADERS, timeout=15)
        r.raise_for_status() 
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥ ({url}): {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []
    
    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")
        
        if not (h2 and a and date_div):
            continue
            
        href = a["href"].strip()
        pub_date = date_div.get_text(strip=True) if date_div else ""

        # **ã€é™å®šæ—¥æœŸåŠŸèƒ½ã€‘**ï¼šåªæŠ“å–å‘å¸ƒæ—¥æœŸç­‰äºä»Šå¤©çš„æ–‡ç« 
        if pub_date != TODAY_DATE:
            continue
            
        # ä¸¥æ ¼æ ¡éªŒé“¾æ¥æ ¼å¼ï¼š/YYYY-MM/DD/content_ID.htm
        match = re.search(r"(/(\d{4}-\d{2}/\d{2})/content_\d+\.htm)", href)
        if not match:
            continue
        
        # å°è¯•ä¸¤ç§å¯èƒ½çš„æ­£ç¡® URL æ ¼å¼
        # 1. åˆ—è¡¨é¡µç»™å‡ºçš„æ ‡å‡†æ ¼å¼ (å¤§æ¦‚ç‡æ˜¯æ­£ç¡®çš„)
        url_full_standard = urljoin(BASE, href) 
        
        # 2. ä¸å¸¦æ—¥æœŸè·¯å¾„çš„æ ¼å¼ (ä»¥é˜²ä¸‡ä¸€)
        content_id_path = re.search(r"/content_\d+\.htm", href)
        url_full_alternate = urljoin(BASE, content_id_path.group(0)) if content_id_path else ""


        items.append({
            "title": h2.get_text(strip=True),
            "url_standard": url_full_standard,
            "url_alternate": url_full_alternate, # å¤‡é€‰ URL 
            "url": url_full_standard, # é»˜è®¤å…ˆä½¿ç”¨æ ‡å‡† URL
            "date": pub_date,
            "content": "",
        })

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°æ–‡ç« æ•°ï¼š{len(items)} (é™å®šæ—¥æœŸ: {TODAY_DATE})")
    return items


def fetch_article_content(item):
    """
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œå¹¶åŒ…å«å¤±è´¥é‡è¯•æœºåˆ¶å’Œ Referer å¤´éƒ¨ã€‚
    """
    # é’ˆå¯¹æ­£æ–‡è¯·æ±‚ï¼Œä½¿ç”¨æ›´é€¼çœŸçš„å¤´éƒ¨
    headers = DEFAULT_HEADERS.copy()
    headers["Referer"] = f"{BASE}/shangye/" # æ¨¡æ‹Ÿä»åˆ—è¡¨é¡µç‚¹å‡»è¿›å…¥
    headers["Sec-Fetch-Site"] = "same-origin" # å…³é”®å¤´éƒ¨
    headers["Sec-Fetch-Mode"] = "navigate"
    
    # å°è¯•è®¿é—®çš„ URL åˆ—è¡¨ï¼Œå…ˆå°è¯•æ ‡å‡† URLï¼Œå¤±è´¥åå°è¯•å¤‡ç”¨ URL
    urls_to_try = [item["url_standard"], item["url_alternate"]]
    
    for current_url in urls_to_try:
        if not current_url:
            continue
            
        for attempt in range(MAX_RETRY):
            try:
                # ä½¿ç”¨åŒ…å« Referer å’Œ Sec-Fetch-Site ç­‰å¤´éƒ¨çš„è¯·æ±‚
                r = requests.get(current_url, headers=headers, timeout=15)
                r.raise_for_status() 

                soup = BeautifulSoup(r.text, "html.parser")
                container = soup.select_one("div.article-mod div.word-text-con")

                if not container:
                    item["content"] = "[æ­£æ–‡å†…å®¹å®¹å™¨æœªæ‰¾åˆ°]"
                    return
                
                # æˆåŠŸè·å–ï¼Œæ›´æ–° item['url'] ä¸ºæˆåŠŸçš„é“¾æ¥ï¼Œå¹¶è¿”å›
                item["url"] = current_url
                paras = [p.get_text(strip=True) for p in container.find_all("p") if p.get_text(strip=True)]
                item["content"] = "\n".join(paras)
                time.sleep(0.5) 
                return

            except requests.exceptions.HTTPError as e:
                if r.status_code == 404:
                    print(f"  âš ï¸ 404 é“¾æ¥æ— æ•ˆï¼Œå°è¯•ä¸‹ä¸€ä¸ª URL æˆ–é‡è¯•ï¼š{current_url}")
                    # å¦‚æœæ˜¯ 404ï¼Œç«‹åˆ»è·³å‡ºé‡è¯•å¾ªç¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ª URL (å¦‚æœå­˜åœ¨)
                    break 
                
                print(f"  âŒ æ­£æ–‡è¯·æ±‚å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•ç¬¬ {attempt + 1}/{MAX_RETRY} æ¬¡ ({current_url}): {e}")
                time.sleep(2 ** attempt) 
            
            except requests.exceptions.RequestException as e:
                print(f"  âŒ æ­£æ–‡è¯·æ±‚å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•ç¬¬ {attempt + 1}/{MAX_RETRY} æ¬¡ ({current_url}): {e}")
                time.sleep(2 ** attempt) 
                
    # å¦‚æœæ‰€æœ‰ URL å°è¯•å’Œé‡è¯•éƒ½å¤±è´¥äº†
    item["content"] = f"[æ­£æ–‡è·å–å¤±è´¥: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°æˆ–æ‰€æœ‰ URL 404]"
    print(f"  â›”ï¸ æ­£æ–‡è·å–å¤±è´¥ï¼Œæ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼š{item['title']}")


def save_to_csv(data: list, filename: str):
    """
    å°†æ–‡ç« æ•°æ®åˆ—è¡¨ä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­ã€‚
    """
    if not data:
        print("ğŸ’¡ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ã€‚")
        return
        
    # åªå¯¼å‡ºå¿…è¦çš„å­—æ®µ
    fieldnames = ["title", "date", "url", "content"]
    
    try:
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
            
        print(f"\nğŸ‰ æ•°æ®ä¿å­˜æˆåŠŸï¼æ–‡ä»¶åä¸ºï¼š{filename}ï¼Œå…± {len(data)} æ¡è®°å½•ã€‚")
    except Exception as e:
        print(f"\nâŒ CSV æ–‡ä»¶å†™å…¥å¤±è´¥ï¼š{e}")


def main():
    all_articles = []
    
    print(f"=== ğŸš€ è´¢å¯Œä¸­æ–‡ç½‘çˆ¬è™«å¼€å§‹æ‰§è¡Œï¼ˆç›®æ ‡é¡µæ•°ï¼š{MAX_PAGES}ï¼Œé™å®šæ—¥æœŸï¼š{TODAY_DATE}ï¼‰ ===")

    # 1. å®ç°å¤šé¡µæŠ“å–å¾ªç¯
    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)
        
        if not list_items:
            # å¦‚æœæŸä¸€é¡µæ²¡æœ‰æŠ“åˆ°æ–‡ç« ï¼Œæˆ–è€…æ²¡æœ‰ç¬¦åˆæ—¥æœŸçš„æ–‡ç« ï¼Œåœæ­¢ç¿»é¡µ
            if page == 1:
                print("--- ç¬¬ 1 é¡µæœªæŠ“åˆ°ä»»ä½•æ–‡ç« ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µæˆ–å½“å‰æ— å‘å¸ƒ ---")
            else:
                print(f"--- ç¬¬ {page} é¡µæ²¡æœ‰æŠ“åˆ°æ–‡ç« ï¼Œåœæ­¢ç¿»é¡µ ---")
            break
            
        all_articles.extend(list_items)
        time.sleep(1) 
    
    print(f"\n=== ğŸ“¥ åˆ—è¡¨æŠ“å–å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(all_articles)} ç¯‡ç¬¦åˆæ—¥æœŸçš„æ–‡ç« é“¾æ¥ã€‚===")

    # 2. éå†æ‰€æœ‰æ–‡ç« ï¼ŒæŠ“å–æ­£æ–‡
    count = 0
    for item in all_articles:
        count += 1
        print(f"ğŸ”¥ æ­£åœ¨å¤„ç†ç¬¬ {count}/{len(all_articles)} ç¯‡ï¼š{item['title']}")
        fetch_article_content(item)
        
    # 3. ç»Ÿè®¡å¤±è´¥æ–‡ç« æ•°
    failed_count = sum(1 for item in all_articles if item["content"].startswith("[æ­£æ–‡è·å–å¤±è´¥"))
    print(f"\n=== ç»Ÿè®¡ç»“æœï¼šæˆåŠŸè·å– {len(all_articles) - failed_count} ç¯‡ï¼Œå¤±è´¥ {failed_count} ç¯‡ã€‚===")

    # 4. ä¿å­˜ä¸º CSV æ–‡ä»¶
    save_to_csv(all_articles, OUTPUT_FILENAME)
    
    return all_articles


if __name__ == "__main__":
    main()
