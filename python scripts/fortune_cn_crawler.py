# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘ å•†ä¸šé¢‘é“çˆ¬è™«ï¼ˆPC ç‰ˆç»“æ„ï¼‰- V8 + AI æ ‡é¢˜ & Markdown ç‰ˆ

åœ¨ V8ã€Œè·¯å¾„æ‹¼æ¥ç»ˆæä¿®æ­£ç‰ˆã€åŸºç¡€ä¸Šæ–°å¢ï¼š
1. è°ƒç”¨ OpenAI æ¥å£ï¼Œä¸ºæ¯ç¯‡æ–‡ç« ç”Ÿæˆä¸€å¥å®¢è§‚çš„ä¸­æ–‡æ ‡é¢˜ï¼ˆéæ ‡é¢˜å…šï¼‰ï¼›
2. è¾“å‡º CSV æ—¶å¢åŠ  ai_title å­—æ®µï¼›
3. ç”Ÿæˆ Markdown æ–‡æœ¬ï¼Œæ¯ä¸€è¡Œå½¢å¦‚ `[AI æ ‡é¢˜](URL)`ï¼Œå¯ç›´æ¥ç”¨äºé’‰é’‰ Markdown æ¶ˆæ¯ï¼Œæ ‡é¢˜å¯ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…ã€‚

æ³¨æ„ï¼š
- OpenAI Key ä¸å†å†™æ­»åœ¨ä»£ç é‡Œï¼Œè€Œæ˜¯ä»ç¯å¢ƒå˜é‡ OPENAI_API_KEY è¯»å–ï¼Œæ–¹ä¾¿åœ¨ GitHub Secrets é‡Œé…ç½®ï¼›
- ä»ç„¶ä½¿ç”¨å›ºå®š TARGET_DATEï¼ˆä¾‹å¦‚ "2025-12-07"ï¼‰ï¼Œä½ å¯ä»¥æ‰‹åŠ¨ä¿®æ”¹ï¼Œæˆ–ä¹‹åå†æ”¹æˆè‡ªåŠ¨ã€Œæ˜¨å¤©ã€ã€‚
"""

import os
import re
import time
import csv
import json
from datetime import datetime
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# --- é…ç½®å‚æ•° ---
BASE = "https://www.fortunechina.com"
# ç¡®ä¿åˆ—è¡¨é¡µ URL ç»“å°¾æœ‰æ–œæ ï¼Œè¿™å¯¹ urljoin å¤„ç†ç›¸å¯¹è·¯å¾„éå¸¸é‡è¦
LIST_URL_BASE = "https://www.fortunechina.com/shangye/"
MAX_PAGES = 3
MAX_RETRY = 3
OUTPUT_FILENAME = "fortunechina_articles_with_ai_title.csv"
OUTPUT_MD = "fortunechina_articles_with_ai_title.md"

# é™å®šæ—¥æœŸ (æ ¹æ®ä½ çš„æˆªå›¾ï¼Œç›®æ ‡æ˜¯ 2025-12-07)
# ä½ å¯ä»¥æ‰‹åŠ¨æ”¹æˆæƒ³æŠ“çš„é‚£ä¸€å¤©ï¼Œæ¯”å¦‚ "2025-12-08"
TARGET_DATE = "2025-12-07"
# ----------------

# --- OpenAI é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å– Keyï¼Œé€‚é… GitHub Secretsï¼‰ ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL = "gpt-4.1-mini"  # ä½ å¯ä»¥æŒ‰éœ€æ”¹æˆå…¶ä»–æ¨¡å‹
OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"

DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": "no-cache",
}


# === AI æ ‡é¢˜ç”Ÿæˆ ===
def ai_summarize_title(content: str, fallback_title: str) -> str:
    """
    è°ƒç”¨ OpenAIï¼ŒæŠŠæ­£æ–‡å†…å®¹æ¦‚æ‹¬æˆä¸€å¥ã€Œå†…éƒ¨ç”¨æ ‡é¢˜ã€ï¼š
    - ä¸­æ–‡
    - éæ ‡é¢˜å…šï¼Œå®¢è§‚å‡†ç¡®
    - ä¸è¶…è¿‡ 25 ä¸ªå­—
    å¦‚æœæ— æ³•è°ƒç”¨ï¼Œåˆ™è¿”å› fallback_titleï¼ˆåŸå§‹æ ‡é¢˜ï¼‰
    """
    if not OPENAI_API_KEY:
        print("  âš ï¸ æœªé…ç½® OPENAI_API_KEYï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜ã€‚")
        return fallback_title

    if not content or content.startswith("[è·å–å¤±è´¥"):
        return fallback_title

    snippet = content[:2000]

    prompt = (
        "ä½ æ˜¯ä¸€åä¸¥è°¨çš„ä¸­æ–‡æ–°é—»ç¼–è¾‘ï¼Œè¯·æ ¹æ®ä¸‹é¢çš„æ–°é—»æ­£æ–‡ï¼Œ"
        "å†™å‡ºä¸€å¥ä¸è¶…è¿‡ 25 ä¸ªå­—çš„ä¸­æ–‡æ–°é—»æ ‡é¢˜ï¼Œç”¨äºå…¬å¸å†…éƒ¨é˜…è¯»ï¼š\n"
        "è¦æ±‚ï¼šå®¢è§‚å‡†ç¡®ã€éæ ‡é¢˜å…šã€ä¸è¦åŠ å¼•å·ï¼Œåªè¾“å‡ºæ ‡é¢˜æœ¬èº«ã€‚\n\n"
        f"{snippet}"
    )

    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€åä¸¥è°¨çš„ä¸­æ–‡æ–°é—»ç¼–è¾‘ï¼Œåªè¾“å‡ºæ–°é—»æ ‡é¢˜æ–‡æœ¬ã€‚",
            },
            {"role": "user", "content": prompt},
        ],
        "temperature": 0.3,
        "max_tokens": 64,
    }

    try:
        resp = requests.post(
            OPENAI_API_URL, headers=headers, data=json.dumps(payload), timeout=30
        )
        resp.raise_for_status()
        data = resp.json()
        ai_title = data["choices"][0]["message"]["content"].strip()
        # åªå–ç¬¬ä¸€è¡Œï¼Œé˜²æ­¢æ¨¡å‹é¡ºå¸¦è§£é‡Š
        ai_title = ai_title.splitlines()[0].strip()
        if not ai_title:
            return fallback_title
        print(f"  ğŸ§  AI æ ‡é¢˜ï¼š{ai_title}")
        return ai_title
    except Exception as e:
        print(f"  âš ï¸ AI è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜ã€‚é”™è¯¯: {e}")
        return fallback_title


def fetch_list(page=1):
    """
    æŠ“å–æŒ‡å®šé¡µç çš„æ–‡ç« åˆ—è¡¨ï¼Œä½¿ç”¨æ­£ç¡®çš„ç›¸å¯¹è·¯å¾„æ‹¼æ¥ã€‚
    """
    # æ„é€ å½“å‰åˆ—è¡¨é¡µçš„å®Œæ•´ URL
    if page == 1:
        current_list_url = LIST_URL_BASE
    else:
        current_list_url = f"{LIST_URL_BASE}?page={page}"

    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ({current_list_url}) ---")

    try:
        r = requests.get(current_list_url, headers=DEFAULT_HEADERS, timeout=15)
        r.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []

    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")

        if not (h2 and a and date_div):
            continue

        # è·å–åŸå§‹ href (ä¾‹å¦‚: "c/2025-12/07/content_470761.htm")
        href = a["href"].strip()
        pub_date = date_div.get_text(strip=True) if date_div else ""

        # 1. æ—¥æœŸè¿‡æ»¤ï¼šåªå¤„ç† TARGET_DATE
        if pub_date != TARGET_DATE:
            continue

        # 2. ç®€å•çš„æ­£åˆ™æ£€æŸ¥ï¼Œåªè¦åŒ…å« content_æ•°å­— å³å¯
        if not re.search(r"content_\d+\.htm", href):
            continue

        # 3. ã€æ ¸å¿ƒä¿®æ­£ã€‘ä½¿ç”¨ current_list_url è¿›è¡Œæ‹¼æ¥
        # å¦‚æœ href æ˜¯ "c/2025..."ï¼Œlist_url æ˜¯ ".../shangye/"
        # ç»“æœè‡ªåŠ¨å˜ä¸º ".../shangye/c/2025..."
        url_full = urljoin(current_list_url, href)

        items.append(
            {
                "title": h2.get_text(strip=True),
                "url": url_full,
                "date": pub_date,
                "content": "",
                "ai_title": "",
            }
        )

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°ç›®æ ‡æ—¥æœŸ({TARGET_DATE})æ–‡ç« æ•°ï¼š{len(items)}")
    return items


def fetch_article_content(item):
    """
    è¯·æ±‚æ–‡ç« æ­£æ–‡å†…å®¹
    """
    url = item["url"]
    headers = DEFAULT_HEADERS.copy()
    # åŠ ä¸Š Refererï¼Œæ¨¡æ‹Ÿä»åˆ—è¡¨é¡µç‚¹è¿‡å»
    headers["Referer"] = LIST_URL_BASE

    for attempt in range(MAX_RETRY):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()

            soup = BeautifulSoup(r.text, "html.parser")
            # å°è¯•å¤šç§æ­£æ–‡é€‰æ‹©å™¨ï¼Œä»¥é˜²é¡µé¢ç»“æ„å¾®è°ƒ
            container = soup.select_one("div.article-mod div.word-text-con")
            if not container:
                container = soup.select_one("div.article-content")  # å¤‡ç”¨é€‰æ‹©å™¨

            if not container:
                item["content"] = "[æ­£æ–‡å®¹å™¨æœªæ‰¾åˆ°]"
                print(f"  âš ï¸ è­¦å‘Šï¼šURL {url} è®¿é—®æˆåŠŸä½†æœªæ‰¾åˆ°æ­£æ–‡å®¹å™¨")
                return

            paras = [
                p.get_text(strip=True)
                for p in container.find_all("p")
                if p.get_text(strip=True)
            ]
            item["content"] = "\n".join(paras)
            time.sleep(0.5)
            return

        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRY - 1:
                print(
                    f"  âŒ è¯·æ±‚å¤±è´¥ ({r.status_code if 'r' in locals() else 'Error'}), é‡è¯•ä¸­...: {url}"
                )
                time.sleep(1)
            else:
                print(f"  â›”ï¸ æœ€ç»ˆå¤±è´¥: {url} | é”™è¯¯: {e}")
                item["content"] = f"[è·å–å¤±è´¥: {e}]"


def save_to_csv(data: list, filename: str):
    if not data:
        print("ğŸ’¡ æ²¡æœ‰æ•°æ®å¯ä¿å­˜ã€‚")
        return
    fieldnames = ["title", "ai_title", "date", "url", "content"]
    try:
        with open(filename, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"\nğŸ‰ æˆåŠŸä¿å­˜åˆ° CSVï¼š{filename}ï¼Œå…± {len(data)} æ¡ã€‚")
    except Exception as e:
        print(f"\nâŒ CSV ä¿å­˜å¤±è´¥ï¼š{e}")


# === ç”Ÿæˆ Markdownï¼Œå¯ç”¨äºé’‰é’‰ Markdown æ¶ˆæ¯ ===
def build_markdown(items: list) -> str:
    """
    ç”Ÿæˆä¸€ä¸ª Markdown å­—ç¬¦ä¸²ï¼š
    - é¡¶éƒ¨æ˜¯æ ‡é¢˜
    - æ¯ä¸€è¡Œéƒ½æ˜¯ï¼š1. [AI æ ‡é¢˜](URL)
    """
    if not items:
        return f"### è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{TARGET_DATE}ï¼‰\n\nä»Šæ—¥æœªæŠ“åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°é—»ã€‚"

    lines = [f"### è´¢å¯Œä¸­æ–‡ç½‘Â·å•†ä¸šé¢‘é“ç²¾é€‰ï¼ˆ{TARGET_DATE}ï¼‰", ""]

    for idx, item in enumerate(items, start=1):
        title = item.get("ai_title") or item.get("title") or "ï¼ˆæ— æ ‡é¢˜ï¼‰"
        url = item.get("url", "")
        lines.append(f"{idx}. [{title}]({url})")

    return "\n".join(lines)


def save_markdown(content: str, filename: str):
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"\nğŸ“„ å·²ä¿å­˜ Markdown åˆ°æ–‡ä»¶ï¼š{filename}")
    except Exception as e:
        print(f"\nâŒ Markdown æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{e}")


def main():
    all_articles = []
    print(f"=== ğŸš€ çˆ¬è™«å¯åŠ¨ (ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}) ===")
    print(f"=== ğŸ› ï¸ ä¿®å¤ç­–ç•¥: åŸºäºåˆ—è¡¨é¡µ URL ({LIST_URL_BASE}) è¿›è¡Œç›¸å¯¹è·¯å¾„æ‹¼æ¥ ===")

    # 1. æŠ“å–åˆ—è¡¨
    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)
        if not list_items:
            # å¦‚æœç¬¬ä¸€é¡µå°±æ²¡æ•°æ®ï¼Œå¯èƒ½æ˜¯æ—¥æœŸä¸å¯¹ï¼Œæˆ–è€…æ²¡åŠ è½½å‡ºæ¥
            if page == 1:
                print(
                    f"âš ï¸ ç¬¬ 1 é¡µæœªæ‰¾åˆ° {TARGET_DATE} çš„æ–‡ç« ï¼Œè¯·ç¡®è®¤ç½‘ç«™ä¸Šç¡®å®æœ‰è¯¥æ—¥æœŸçš„å†…å®¹ã€‚"
                )
            break
        all_articles.extend(list_items)
        time.sleep(1)

    print(f"\n=== ğŸ“¥ é“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡ã€‚å¼€å§‹æŠ“å–æ­£æ–‡ + ç”Ÿæˆ AI æ ‡é¢˜... ===")

    # 2. æŠ“å–æ­£æ–‡ + ç”Ÿæˆ AI æ ‡é¢˜
    count = 0
    for item in all_articles:
        count += 1
        print(f"\nğŸ”¥ ({count}/{len(all_articles)}) å¤„ç†: {item['title']}")
        fetch_article_content(item)
        item["ai_title"] = ai_summarize_title(item["content"], item["title"])

    # 3. ç»Ÿè®¡ä¸ä¿å­˜ CSV
    success_count = sum(
        1
        for item in all_articles
        if "è·å–å¤±è´¥" not in item["content"] and item["content"]
    )
    print(f"\n=== ç»Ÿè®¡: æˆåŠŸ {success_count} ç¯‡ï¼Œå¤±è´¥ {len(all_articles) - success_count} ç¯‡ ===")
    save_to_csv(all_articles, OUTPUT_FILENAME)

    # 4. ç”Ÿæˆ Markdown é¢„è§ˆ & ä¿å­˜
    md_content = build_markdown(all_articles)
    print("\n=== Markdown é¢„è§ˆï¼ˆå¯ç”¨äºé’‰é’‰ Markdown æ¶ˆæ¯ï¼‰ ===\n")
    print(md_content)
    save_markdown(md_content, OUTPUT_MD)


if __name__ == "__main__":
    main()
