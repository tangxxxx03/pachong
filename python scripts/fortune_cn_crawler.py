# -*- coding: utf-8 -*-
"""
è´¢å¯Œä¸­æ–‡ç½‘çˆ¬è™« V10 - æ‘˜è¦æ€»ç»“ç‰ˆ

æ›´æ–°å†…å®¹ï¼š
1. ã€æ ¸å¿ƒä¿®æ­£ã€‘è°ƒæ•´ AI Promptï¼šè¦æ±‚ç”Ÿæˆä¸€å¥å®¢è§‚ã€å®Œæ•´çš„é™ˆè¿°å¥ï¼Œç”¨äºæ€»ç»“æ–°é—»æ ¸å¿ƒå†…å®¹ï¼Œå¸®åŠ©çœç•¥é˜…è¯»è¿‡ç¨‹ã€‚
2. å…¶ä»–ä»£ç é€»è¾‘ï¼ˆæŠ“å–ã€è·¯å¾„ä¿®å¤ã€æ—¥æœŸé™å®šï¼‰ä¿æŒä¸å˜ã€‚
"""

import re
import time
import requests
import csv
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
from openai import OpenAI

# --- é…ç½®å‚æ•° ---
BASE = "https://www.fortunechina.com"
LIST_URL_BASE = "https://www.fortunechina.com/shangye/" 
MAX_PAGES = 3
MAX_RETRY = 3
OUTPUT_FILENAME = "fortunechina_ai_summary_v10.csv"
TARGET_DATE = "2025-12-07" 

# --- AI é…ç½® (å·²å¡«å…¥ä½ çš„ Key) ---
API_KEY = "sk-lTg1L3LAYY1rGfWH21QgK7bkCoe4SIQZJIYiW0c9W2Gg4Zlq"
API_BASE_URL = None  
AI_MODEL = "gpt-3.5-turbo" 

client = OpenAI(
    api_key=API_KEY,
    base_url=API_BASE_URL
)

DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Cache-Control": "no-cache",
}

def get_ai_summary(content):
    """
    è°ƒç”¨ AI æ¥å£ç”Ÿæˆå®Œæ•´å†…å®¹æ€»ç»“ï¼ˆä¸€å¥è¯é™ˆè¿°å¥ï¼‰
    """
    if not content or len(content) < 50:
        return "å†…å®¹å¤ªçŸ­ï¼Œæ— æ³•æ¦‚æ‹¬"

    print("  ğŸ¤– AI æ­£åœ¨ç”Ÿæˆæ€»ç»“...")
    try:
        # **ã€æ ¸å¿ƒä¿®æ”¹ã€‘**ï¼šæç¤ºè¯è°ƒæ•´ä¸ºè¦æ±‚ç”Ÿæˆå®¢è§‚ã€å®Œæ•´çš„é™ˆè¿°å¥æ€»ç»“
        response = client.chat.completions.create(
            model=AI_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å•†åŠ¡åˆ†æå¸ˆï¼Œè´Ÿè´£å°†é•¿ç¯‡æ–°é—»å¿«é€Ÿæç‚¼ã€‚"},
                {"role": "user", "content": f"è¯·é˜…è¯»ä»¥ä¸‹æ–°é—»æ­£æ–‡ï¼Œå°†å…¶æ ¸å¿ƒå†…å®¹æç‚¼æ¦‚æ‹¬ä¸º**ä¸€å¥å®Œæ•´çš„é™ˆè¿°å¥æ€»ç»“**ï¼Œç”¨äºå†…éƒ¨æ²Ÿé€šï¼Œè¦æ±‚å®¢è§‚ã€ä¿¡æ¯å®Œæ•´ã€ä¸è¶…è¿‡50ä¸ªå­—ï¼š\n\n{content[:2000]}"}
            ],
            temperature=0.3, # é™ä½æ¸©åº¦ï¼Œè·å–æ›´å®¢è§‚çš„è¾“å‡º
            max_tokens=150 # é€‚å½“å¢åŠ æœ€å¤§ tokenï¼Œç¡®ä¿å¥å­å®Œæ•´
        )
        summary = response.choices[0].message.content.strip()
        print(f"  âœ¨ AI æ€»ç»“: {summary}")
        return summary
    except Exception as e:
        print(f"  âš ï¸ AI æ¥å£è°ƒç”¨å¤±è´¥: {e}")
        return f"[AI æ¦‚æ‹¬å¤±è´¥: {e}]"

# --- åˆ—è¡¨æŠ“å–å‡½æ•° (fetch_list) ---
def fetch_list(page=1):
    if page == 1:
        current_list_url = LIST_URL_BASE
    else:
        current_list_url = f"{LIST_URL_BASE}?page={page}"
        
    print(f"\n--- æ­£åœ¨è¯·æ±‚åˆ—è¡¨é¡µ: ç¬¬ {page} é¡µ ({current_list_url}) ---")

    try:
        r = requests.get(current_list_url, headers=DEFAULT_HEADERS, timeout=15)
        r.raise_for_status() 
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ åˆ—è¡¨é¡µè¯·æ±‚å¤±è´¥: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    items = []
    
    for li in soup.select("ul.news-list li.news-item"):
        h2 = li.find("h2")
        a = li.find("a", href=True)
        date_div = li.find("div", class_="date")
        
        if not (h2 and a and date_div): continue
            
        href = a["href"].strip()
        pub_date = date_div.get_text(strip=True) if date_div else ""

        if pub_date != TARGET_DATE: continue
        if not re.search(r"content_\d+\.htm", href): continue
        
        url_full = urljoin(current_list_url, href)
        items.append({
            "title": h2.get_text(strip=True),
            "url": url_full,
            "date": pub_date,
            "content": "",
            "ai_summary": "" 
        })

    print(f"  âœ… ç¬¬ {page} é¡µæŠ“åˆ°ç›®æ ‡æ—¥æœŸ({TARGET_DATE})æ–‡ç« æ•°ï¼š{len(items)}")
    return items

# --- æ­£æ–‡æŠ“å–å‡½æ•° (fetch_article_content) ---
def fetch_article_content(item):
    url = item["url"]
    headers = DEFAULT_HEADERS.copy()
    headers["Referer"] = LIST_URL_BASE 
    
    for attempt in range(MAX_RETRY):
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status() 
            soup = BeautifulSoup(r.text, "html.parser")
            container = soup.select_one("div.article-mod div.word-text-con")
            if not container:
                container = soup.select_one("div.article-content")

            if not container:
                item["content"] = "[æ­£æ–‡å®¹å™¨æœªæ‰¾åˆ°]"
                print(f"  âš ï¸ æœªæ‰¾åˆ°æ­£æ–‡: {url}")
                return

            paras = [p.get_text(strip=True) for p in container.find_all("p") if p.get_text(strip=True)]
            full_text = "\n".join(paras)
            item["content"] = full_text
            
            if full_text:
                item["ai_summary"] = get_ai_summary(full_text)
            
            time.sleep(0.5) 
            return

        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRY - 1:
                time.sleep(1)
            else:
                print(f"  â›”ï¸ æœ€ç»ˆå¤±è´¥: {url} | é”™è¯¯: {e}")
                item["content"] = f"[è·å–å¤±è´¥: {e}]"

# --- CSV ä¿å­˜å‡½æ•° (save_to_csv) ---
def save_to_csv(data: list, filename: str):
    if not data: return
    fieldnames = ["title", "date", "url", "ai_summary", "content"]
    try:
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"\nğŸ‰ æˆåŠŸä¿å­˜åˆ° CSVï¼š{filename}ï¼ŒåŒ…å« AI æ€»ç»“æ•°æ®ï¼")
    except Exception as e:
        print(f"\nâŒ CSV ä¿å­˜å¤±è´¥ï¼š{e}")

# --- ä¸»å‡½æ•° (main) ---
def main():
    all_articles = []
    print(f"=== ğŸš€ çˆ¬è™«å¯åŠ¨ + AI æ€»ç»“ (ç›®æ ‡æ—¥æœŸ: {TARGET_DATE}) ===")

    for page in range(1, MAX_PAGES + 1):
        list_items = fetch_list(page)
        if not list_items: 
            if page == 1:
                print(f"âš ï¸ ç¬¬ 1 é¡µæœªæ‰¾åˆ° {TARGET_DATE} çš„æ–‡ç« ã€‚")
            break
        all_articles.extend(list_items)
        time.sleep(1) 
    
    print(f"\n=== ğŸ“¥ é“¾æ¥æ”¶é›†å®Œæˆï¼Œå…± {len(all_articles)} ç¯‡ã€‚å¼€å§‹æŠ“å–æ­£æ–‡å¹¶ç”Ÿæˆæ€»ç»“... ===")

    count = 0
    for item in all_articles:
        count += 1
        print(f"ğŸ”¥ ({count}/{len(all_articles)}) å¤„ç†: {item['title']}")
        fetch_article_content(item)
        
    save_to_csv(all_articles, OUTPUT_FILENAME)

if __name__ == "__main__":
    main()
