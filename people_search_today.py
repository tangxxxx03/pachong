# -*- coding: utf-8 -*-
"""
hr_search_24h_dingtalk.py  â€” æ˜¨å¤©ä¸“æŠ“ç‰ˆ
æ–°å¢ï¼š
  - --date å‚æ•°ï¼šæ”¯æŒ yesterday / YYYY-MM-DD
  - ä»…æ˜¨å¤©æ¨¡å¼ï¼šè¿‡æ»¤æ—¶é—´èŒƒå›´ä¸º [æ˜¨å¤©00:00, æ˜¨å¤©23:59:59]
  - ä¸ä¼  --date æ—¶ï¼Œä¿æŒåŸæœ‰ --window-hours è¡Œä¸ºï¼ˆå‘åå…¼å®¹ï¼‰
"""

import re
import os
import time
import hmac
import base64
import hashlib
import argparse
from dataclasses import dataclass
from typing import List, Tuple, Optional
from urllib.parse import urljoin, urlencode, urlparse, quote
from datetime import datetime, timedelta

try:
    from zoneinfo import ZoneInfo  # Py3.9+
except Exception:  # pragma: no cover
    from backports.zoneinfo import ZoneInfo

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup


# ===================== å…¨å±€é…ç½® =====================
TZ = ZoneInfo("Asia/Shanghai")
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
      "AppleWebKit/537.36 (KHTML, like Gecko) "
      "Chrome/123.0.0.0 Safari/537.36")

# â€”â€” é»˜è®¤é’‰é’‰ï¼ˆå¯è¢«ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
DEFAULT_WEBHOOK = (
    "https://oapi.dingtalk.com/robot/send?"
    "access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
)
DEFAULT_SECRET = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"


def _first_env(*keys: str, default: str = "") -> str:
    for k in keys:
        v = os.getenv(k, "").strip()
        if v:
            return v
    return default


DINGTALK_WEBHOOK = _first_env("DINGTALK_WEBHOOK", "DINGTALK_BASE", "WEBHOOK", default=DEFAULT_WEBHOOK)
DINGTALK_SECRET  = _first_env("DINGTALK_SECRET",  "SECRET",        default=DEFAULT_SECRET)


def _mask_tail(s: str, keep: int = 6) -> str:
    if not s:
        return ""
    if len(s) <= keep:
        return "*" * len(s)
    return "*" * (len(s) - keep) + s[-keep:]


# ===================== HTTP å·¥å…· =====================
def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": UA, "Accept-Language": "zh-CN,zh;q=0.9"})
    retries = Retry(
        total=3,
        backoff_factor=0.6,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "POST"]),
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    s.mount("https://", adapter)
    s.mount("http://", adapter)
    s.trust_env = False
    return s


# ===================== DingTalk æ¨é€ =====================
def _sign_webhook(base_webhook: str, secret: str) -> str:
    if not base_webhook:
        return ""
    if not secret:
        return base_webhook
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = quote(base64.b64encode(hmac_code))
    sep = "&" if "?" in base_webhook else "?"
    return f"{base_webhook}{sep}timestamp={ts}&sign={sign}"


def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
    if not webhook:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    try:
        host = urlparse(webhook).netloc
        print(f"[DingTalk] host={host}  token~{_mask_tail(DINGTALK_WEBHOOK)}  secret~{_mask_tail(DINGTALK_SECRET)}")
    except Exception:
        pass

    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    try:
        r = requests.post(webhook, json=payload, timeout=20)
        ok = (r.status_code == 200 and isinstance(r.json(), dict) and r.json().get("errcode") == 0)
        print("DingTalk resp:", r.status_code, r.text[:200])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False


# ===================== è§£æä¸æ—¶é—´è¿‡æ»¤ =====================
DATE_PATS = [
    r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",  # 2025-09-16 08:30
    r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})",                        # 2025-09-16
    r"(\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",               # 09-16 08:30
]

def parse_dt(text: str) -> Optional[datetime]:
    if not text:
        return None
    t = re.sub(r"\s+", " ", text.strip())
    # æ˜ç¡®æ—¥æœŸåŒ¹é…
    for pat in DATE_PATS:
        m = re.search(pat, t)
        if not m:
            continue
        if len(m.groups()) == 5:
            y, mo, d, hh, mm = map(int, m.groups())
            return datetime(y, mo, d, hh, mm, tzinfo=TZ)
        if len(m.groups()) == 3:
            y, mo, d = map(int, m.groups())
            # åªæœ‰æ—¥æœŸæ—¶è®¾ä¸ºä¸­åˆ12:00ï¼Œé¿å…è¢«24hçª—å£è¯¯æ€
            return datetime(y, mo, d, 12, 0, tzinfo=TZ)
        if len(m.groups()) == 4:
            mo, d, hh, mm = map(int, m.groups())
            y = datetime.now(TZ).year
            return datetime(y, mo, d, hh, mm, tzinfo=TZ)
    # ç›¸å¯¹æ—¶é—´
    if re.search(r"(åˆšåˆš|åˆ†é’Ÿ|å°æ—¶å‰|ä»Šå¤©|ä»Šæ—¥)", t):
        return datetime.now(TZ)
    return None


def within_last_hours(dt: Optional[datetime], hours: int) -> bool:
    if not dt:
        return False
    now = datetime.now(TZ)
    return (now - timedelta(hours=hours)) <= dt <= now


def day_range(date_str: str) -> Tuple[datetime, datetime]:
    """è¿”å›æŸå¤©åœ¨æœ¬åœ°æ—¶åŒºçš„ [start, end]"""
    if date_str.lower() == "yesterday":
        base = datetime.now(TZ).date() - timedelta(days=1)
    else:
        base = datetime.strptime(date_str, "%Y-%m-%d").date()
    start = datetime(base.year, base.month, base.day, 0, 0, 0, tzinfo=TZ)
    end   = datetime(base.year, base.month, base.day, 23, 59, 59, tzinfo=TZ)
    return start, end


# ===================== æ•°æ®ç»“æ„ =====================
@dataclass
class Item:
    title: str
    url: str
    dt: Optional[datetime]
    content: str
    source: str


# ===================== ç«™ç‚¹ 1ï¼šmohrss.gov.cn/hsearch =====================
class MohrssHSearch:
    BASE = "https://www.mohrss.gov.cn"
    PATH = "/hsearch/"

    def __init__(self, session: requests.Session, q: str, delay: float = 1.0):
        self.session = session
        self.q = q
        self.delay = delay

    def _fetch_page(self, page: int) -> str:
        params = {"searchword": self.q}
        if page > 1:
            params["page"] = page
        url = self.BASE + self.PATH + "?" + urlencode(params)
        r = self.session.get(url, timeout=20)
        r.encoding = r.apparent_encoding or "utf-8"
        time.sleep(self.delay)
        return r.text

    def parse_list(self, html: str) -> Tuple[List[Item], Optional[str]]:
        soup = BeautifulSoup(html, "html.parser")
        nodes = []
        for sel in ["ul.search-list li", "div.search-list li", "div.list li", "ul li", "div.result", "div.row"]:
            tmp = soup.select(sel)
            if tmp:
                nodes = tmp
                break
        if not nodes:
            nodes = soup.select("a")  # å…œåº•

        items: List[Item] = []
        for node in nodes:
            a = node if node.name == "a" else node.find("a")
            if not a or not a.get("href"):
                continue
            title = a.get_text(" ", strip=True)
            href = a.get("href").strip()
            url = urljoin(self.BASE, href)

            # å†…å®¹/æ‘˜è¦
            abs_el = None
            for sel in [".summary", ".abs", ".intro", "p"]:
                abs_el = node.select_one(sel)
                if abs_el:
                    break
            content = abs_el.get_text(" ", strip=True) if abs_el else ""

            # æ—¶é—´
            ttxt = ""
            for sel in [".date", ".time", ".pubtime", ".f-date", ".info time", ".post-time"]:
                sub = node.select_one(sel)
                if sub:
                    ttxt = sub.get_text(" ", strip=True)
                    break
            if not ttxt:
                ttxt = node.get_text(" ", strip=True)
            dt = parse_dt(ttxt)

            items.append(Item(title=title, url=url, dt=dt, content=content, source="äººç¤¾éƒ¨ç«™å†…æœç´¢"))

        # ä¸‹ä¸€é¡µ
        next_link = None
        for a in soup.select("a"):
            txt = a.get_text(strip=True)
            if txt in ("ä¸‹ä¸€é¡µ", "ä¸‹é¡µ", "â€º", ">") or a.get("rel") == ["next"]:
                href = a.get("href") or ""
                if href and href != "javascript:;" and not href.startswith("#"):
                    next_link = urljoin(self.BASE, href)
                    break

        return items, next_link

    def run(self, max_pages: int) -> List[Item]:
        all_items: List[Item] = []
        next_url = None
        for p in range(1, max_pages + 1):
            if p == 1 or not next_url:
                html = self._fetch_page(p)
            else:
                r = self.session.get(next_url, timeout=20)
                r.encoding = r.apparent_encoding or "utf-8"
                time.sleep(self.delay)
                html = r.text
            items, next_url = self.parse_list(html)
            if not items and p == 1:
                break
            all_items.extend(items)
            if not next_url:
                break
        return all_items


# ===================== ç«™ç‚¹ 2ï¼šjob.mohrss.gov.cn/zxss =====================
class JobMohrssSearch:
    BASE = "http://job.mohrss.gov.cn"
    PATH = "/zxss/index.jhtml"

    def __init__(self, session: requests.Session, q: str, delay: float = 1.0):
        self.session = session
        self.q = q
        self.delay = delay

    def _fetch_page(self, page: int, last_next: Optional[str]) -> str:
        if last_next:
            url = last_next
        else:
            params = {"textfield": self.q}
            if page > 1:
                params["pageNo"] = page
            url = self.BASE + self.PATH + "?" + urlencode(params)
        r = self.session.get(url, timeout=20)
        r.encoding = r.apparent_encoding or "utf-8"
        time.sleep(self.delay)
        return r.text

    def parse_list(self, html: str) -> Tuple[List[Item], Optional[str]]:
        soup = BeautifulSoup(html, "html.parser")
        nodes = []
        for sel in [
            ".list li", ".news-list li", ".content-list li", ".box-list li",
            "ul.list li", "ul.news li", "ul li", "li"
        ]:
            tmp = soup.select(sel)
            if tmp:
                nodes = tmp
                break
        if not nodes:
            nodes = soup.select("a")

        items: List[Item] = []
        for node in nodes:
            a = node if node.name == "a" else node.find("a")
            if not a or not a.get("href"):
                continue
            title = a.get_text(" ", strip=True)
            href = a.get("href").strip()
            url = urljoin(self.BASE, href)

            # è¿‡æ»¤éæœ¬ç«™
            host = urlparse(url).netloc.lower()
            if not host.endswith("mohrss.gov.cn"):
                continue

            # æ‘˜è¦
            abs_el = None
            for sel in [".summary", ".abs", ".intro", "p"]:
                abs_el = node.select_one(sel)
                if abs_el:
                    break
            content = abs_el.get_text(" ", strip=True) if abs_el else ""

            # æ—¶é—´
            ttxt = ""
            for sel in [".date", ".time", ".pubtime", ".f-date", ".info time", ".post-time", "em", "span"]:
                sub = node.select_one(sel)
                if sub:
                    maybe = sub.get_text(" ", strip=True)
                    if re.search(r"\d{2,4}[^\d]\d{1,2}[^\d]\d{1,2}", maybe) or re.search(r"(åˆšåˆš|åˆ†é’Ÿ|å°æ—¶å‰|ä»Šå¤©|ä»Šæ—¥)", maybe):
                        ttxt = maybe
                        break
            if not ttxt:
                ttxt = node.get_text(" ", strip=True)
            dt = parse_dt(ttxt)

            items.append(Item(title=title, url=url, dt=dt, content=content, source="å…¬å…±æ‹›è˜ç½‘æœç´¢"))

        # ä¸‹ä¸€é¡µ
        next_link = None
        for a in soup.select("a"):
            txt = a.get_text(strip=True)
            if txt in ("ä¸‹ä¸€é¡µ", "ä¸‹é¡µ", "â€º", ">") or a.get("rel") == ["next"]:
                href = a.get("href") or ""
                if href and href != "javascript:;" and not href.startswith("#"):
                    next_link = urljoin(self.BASE, href)
                    break

        return items, next_link

    def run(self, max_pages: int) -> List[Item]:
        all_items: List[Item] = []
        next_url = None
        for p in range(1, max_pages + 1):
            html = self._fetch_page(p, last_next=next_url)
            items, next_url = self.parse_list(html)
            if not items and p == 1:
                break
            all_items.extend(items)
            if not next_url:
                break
        return all_items


# ===================== æ±‡æ€»ã€è¿‡æ»¤ã€è¾“å‡º =====================
def dedup_by_url(items: List[Item]) -> List[Item]:
    seen = set()
    out: List[Item] = []
    for it in items:
        if it.url and it.url not in seen:
            seen.add(it.url)
            out.append(it)
    return out


def filter_by_range(items: List[Item], start: datetime, end: datetime) -> List[Item]:
    out: List[Item] = []
    for it in items:
        if it.dt and start <= it.dt <= end:
            out.append(it)
    return out


def build_markdown(items: List[Item], keyword: str, title_prefix: str) -> str:
    now_dt = datetime.now(TZ)
    wd = ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][now_dt.weekday()]
    lines = [
        f"**æ—¥æœŸï¼š{now_dt.strftime('%Y-%m-%d')}ï¼ˆ{wd}ï¼‰**",
        "",
        f"**æ ‡é¢˜ï¼š{title_prefix}ï½œäººç¤¾éƒ¨ & å…¬å…±æ‹›è˜ç½‘æœç´¢ï½œ{keyword}**",
        "",
        "**ä¸»è¦å†…å®¹**",
    ]
    if not items:
        lines.append("> æš‚æ— æ›´æ–°ã€‚")
        return "\n".join(lines)

    for i, it in enumerate(items, 1):
        dt_str = it.dt.strftime("%Y-%m-%d %H:%M") if it.dt else ""
        title_line = f"{i}. [{it.title}]({it.url})"
        if it.source:
            title_line += f"ã€€â€”ã€€*{it.source}*"
        if dt_str:
            title_line += f"ã€€`{dt_str}`"
        lines.append(title_line)
        if it.content:
            snippet = re.sub(r"\s+", " ", it.content).strip()[:120]
            lines.append(f"> {snippet}")
        lines.append("")
    return "\n".join(lines)


# ===================== ä¸»æµç¨‹ =====================
def main():
    ap = argparse.ArgumentParser(description="äººç¤¾éƒ¨ & å…¬å…±æ‹›è˜ç½‘ ç«™å†…æœç´¢ â†’ é’‰é’‰æ¨é€")
    ap.add_argument("--q", default=os.getenv("QUERY", "äººåŠ›èµ„æº"), help="æœç´¢å…³é”®è¯ï¼ˆé»˜è®¤ï¼šäººåŠ›èµ„æºï¼›ä¹Ÿå¯ç”¨ç¯å¢ƒå˜é‡ QUERY è¦†ç›–ï¼‰")
    ap.add_argument("--pages", type=int, default=int(os.getenv("PAGES", "2")), help="æ¯ç«™æœ€å¤šç¿»é¡µæ•°ï¼ˆé»˜è®¤2ï¼Œå¯ç”¨ env PAGESï¼‰")
    ap.add_argument("--window-hours", type=int, default=int(os.getenv("WINDOW_HOURS", "24")), help="æ»šåŠ¨çª—å£å°æ—¶æ•°ï¼ˆé»˜è®¤24ï¼‰")
    ap.add_argument("--delay", type=float, default=float(os.getenv("DELAY", "1.0")), help="æ¯æ¬¡è¯·æ±‚é—´éš”ç§’ï¼ˆé»˜è®¤1.0ï¼‰")
    ap.add_argument("--limit", type=int, default=int(os.getenv("LIMIT", "20")), help="æ­£æ–‡æœ€å¤šå±•ç¤ºæ¡æ•°ï¼ˆé»˜è®¤20ï¼‰")
    ap.add_argument("--date", default=os.getenv("DATE", "yesterday"),
                    help="æŠ“å–æŒ‡å®šæ—¥æœŸï¼ˆyesterday æˆ– YYYY-MM-DDï¼‰ã€‚è‹¥ä¸ºç©ºåˆ™ä½¿ç”¨ --window-hours æ»šåŠ¨çª—å£")
    ap.add_argument("--no-push", action="store_true", help="åªæ‰“å°ä¸æ¨é€é’‰é’‰")
    args = ap.parse_args()

    session = make_session()

    # ç«™ç‚¹æŠ“å–
    a = MohrssHSearch(session, args.q, delay=args.delay).run(max_pages=args.pages)
    b = JobMohrssSearch(session, args.q, delay=args.delay).run(max_pages=args.pages)
    all_items = dedup_by_url(a + b)

    title_prefix = "æ—©å®‰èµ„è®¯"
    # æ—¶é—´è¿‡æ»¤ï¼šä¼˜å…ˆä½¿ç”¨ --dateï¼ˆé»˜è®¤ yesterdayï¼‰ï¼Œå¦åˆ™å›é€€ --window-hours
    if args.date:
        start, end = day_range(args.date)
        all_items = filter_by_range(all_items, start, end)
        title_prefix = f"{args.date} ä¸“é¢˜"
    else:
        all_items = [it for it in all_items if within_last_hours(it.dt, args.window_hours)]

    # æ’åº+æˆªæ–­
    all_items.sort(key=lambda x: x.dt or datetime(1970, 1, 1, tzinfo=TZ), reverse=True)
    show = all_items[:args.limit] if args.limit and args.limit > 0 else all_items

    print(f"âœ… åˆè®¡å€™é€‰ {len(a)+len(b)} æ¡ï¼›ç­›é€‰å {len(all_items)} æ¡ï¼›å±•ç¤º {len(show)} æ¡ã€‚")

    md = build_markdown(show, args.q, title_prefix)
    print("\n--- Markdown Preview ---\n")
    print(md)

    if not args.no_push:
        ok = send_dingtalk_markdown(f"{title_prefix}ï½œéƒ¨ç½‘&æ‹›è˜ç½‘æœç´¢ï½œ{args.q}", md)
        print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥/æœªæ¨é€ âŒ")


if __name__ == "__main__":
    main()
