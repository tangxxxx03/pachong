# -*- coding: utf-8 -*-
"""
hr_search_24h_dingtalk.py  â€” ç¨³å¥ç‰ˆ
æ”¹åŠ¨äº®ç‚¹ï¼š
  1) --q ä¸å† requiredï¼Œé»˜è®¤ "äººåŠ›èµ„æº"ï¼ˆå†ä¹Ÿä¸ä¼šå› å¿˜ä¼ å‚æ•°é€€å‡ºï¼‰
  2) é’‰é’‰ webhook/secret æ”¯æŒå¤šåˆ«åç¯å¢ƒå˜é‡ä¼˜å…ˆï¼š
       DINGTALK_WEBHOOK / DINGTALK_BASE / WEBHOOK
       DINGTALK_SECRET  / SECRET
     â†’ è§£å†³â€œæ”¹äº†å´è¿˜å‘åˆ°æ—§ç¾¤â€çš„å¸¸è§é—®é¢˜
  3) è¿è¡Œæ—¶æ‰“å°æ©ç ä¿¡æ¯ï¼ˆhost + token/secret æœ«å°¾6ä½ï¼‰ï¼Œæ–¹ä¾¿ä½ ç¡®è®¤æ¨é€ç›®æ ‡ï¼ˆä¸æ³„éœ²æ˜æ–‡ï¼‰
  4) çˆ¬å–ã€æ—¶é—´è§£æã€åˆ†é¡µé€»è¾‘ä¸åŸç‰ˆä¸€è‡´ï¼Œå¢åŠ å°‘é‡å…¼å®¹ä¸æ³¨é‡Š
ç”¨æ³•ç¤ºä¾‹ï¼š
  python hr_search_24h_dingtalk.py --q "äººåŠ›èµ„æº" --pages 3 --window-hours 24 --limit 20
  # åªæ‰“å°ä¸æ¨é€
  python hr_search_24h_dingtalk.py --no-push
ä¾èµ–ï¼š
  pip install requests beautifulsoup4
  # è‹¥ Python < 3.9ï¼špip install backports.zoneinfo
"""

import re
import os
import time
import hmac
import json
import base64
import hashlib
import argparse
from dataclasses import dataclass
from typing import List, Tuple, Optional
from urllib.parse import urljoin, urlencode, urlparse, quote
from datetime import datetime, timedelta

try:
    from zoneinfo import ZoneInfo  # Py3.9+
except Exception:  # pragma: no cover
    from backports.zoneinfo import ZoneInfo

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup


# ===================== å…¨å±€é…ç½® =====================
TZ = ZoneInfo("Asia/Shanghai")
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
      "AppleWebKit/537.36 (KHTML, like Gecko) "
      "Chrome/123.0.0.0 Safari/537.36")

# â€”â€” é»˜è®¤é’‰é’‰ï¼ˆå¯è¢«ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
DEFAULT_WEBHOOK = (
    "https://oapi.dingtalk.com/robot/send?"
    "access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
)
DEFAULT_SECRET = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"


def _first_env(*keys: str, default: str = "") -> str:
    """å–ç¬¬ä¸€ä¸ªéç©ºç¯å¢ƒå˜é‡å€¼"""
    for k in keys:
        v = os.getenv(k, "").strip()
        if v:
            return v
    return default


DINGTALK_WEBHOOK = _first_env("DINGTALK_WEBHOOK", "DINGTALK_BASE", "WEBHOOK", default=DEFAULT_WEBHOOK)
DINGTALK_SECRET  = _first_env("DINGTALK_SECRET",  "SECRET",        default=DEFAULT_SECRET)


def _mask_tail(s: str, keep: int = 6) -> str:
    """æ©ç æ˜¾ç¤ºï¼Œä»…ä¿ç•™æœ«å°¾ keep ä½"""
    if not s:
        return ""
    if len(s) <= keep:
        return "*" * len(s)
    return "*" * (len(s) - keep) + s[-keep:]


# ===================== HTTP å·¥å…· =====================
def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({"User-Agent": UA, "Accept-Language": "zh-CN,zh;q=0.9"})
    retries = Retry(
        total=3,
        backoff_factor=0.6,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "POST"]),
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    s.mount("https://", adapter)
    s.mount("http://", adapter)
    s.trust_env = False  # å¿½ç•¥ç³»ç»Ÿä»£ç†ï¼Œä¿è¯å¯æ§
    return s


# ===================== DingTalk æ¨é€ =====================
def _sign_webhook(base_webhook: str, secret: str) -> str:
    if not base_webhook:
        return ""
    if not secret:
        return base_webhook
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = quote(base64.b64encode(hmac_code))
    sep = "&" if "?" in base_webhook else "?"
    return f"{base_webhook}{sep}timestamp={ts}&sign={sign}"


def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
    if not webhook:
        print("ğŸ”• æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡æ¨é€ã€‚")
        return False

    # è¿è¡Œæ—¶æç¤ºï¼ˆæ©ç ï¼‰ï¼Œé¿å…â€œå‘åˆ°æ—§ç¾¤â€
    try:
        host = urlparse(webhook).netloc
        print(f"[DingTalk] host={host}  token~{_mask_tail(DINGTALK_WEBHOOK, 6)}  secret~{_mask_tail(DINGTALK_SECRET, 6)}")
    except Exception:
        pass

    payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
    try:
        r = requests.post(webhook, json=payload, timeout=20)
        ok = (r.status_code == 200 and isinstance(r.json(), dict) and r.json().get("errcode") == 0)
        print("DingTalk resp:", r.status_code, r.text[:200])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False


# ===================== è§£æä¸æ—¶é—´è¿‡æ»¤ =====================
DATE_PATS = [
    r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",  # 2025-09-16 08:30 / 2025/09/16 08:30
    r"(20\d{2})[^\d](\d{1,2})[^\d](\d{1,2})",                        # 2025-09-16
    r"(\d{1,2})[^\d](\d{1,2})\s+(\d{1,2}):(\d{1,2})",               # 09-16 08:30
]

def parse_dt(text: str) -> Optional[datetime]:
    if not text:
        return None
    t = re.sub(r"\s+", " ", text.strip())
    # æ˜ç¡®æ—¥æœŸåŒ¹é…
    for pat in DATE_PATS:
        m = re.search(pat, t)
        if not m:
            continue
        if len(m.groups()) == 5:
            y, mo, d, hh, mm = map(int, m.groups())
            return datetime(y, mo, d, hh, mm, tzinfo=TZ)
        if len(m.groups()) == 3:
            y, mo, d = map(int, m.groups())
            return datetime(y, mo, d, 0, 0, tzinfo=TZ)
        if len(m.groups()) == 4:
            mo, d, hh, mm = map(int, m.groups())
            y = datetime.now(TZ).year
            return datetime(y, mo, d, hh, mm, tzinfo=TZ)
    # ç›¸å¯¹æ—¶é—´ï¼ˆç²—ç•¥å…œåº•ï¼‰
    if re.search(r"(åˆšåˆš|åˆ†é’Ÿ|å°æ—¶å‰|ä»Šå¤©|ä»Šæ—¥)", t):
        return datetime.now(TZ)
    return None


def within_last_hours(dt: Optional[datetime], hours: int) -> bool:
    if not dt:
        return False
    now = datetime.now(TZ)
    return (now - timedelta(hours=hours)) <= dt <= now


# ===================== æ•°æ®ç»“æ„ =====================
@dataclass
class Item:
    title: str
    url: str
    dt: Optional[datetime]
    content: str
    source: str


# ===================== ç«™ç‚¹ 1ï¼šmohrss.gov.cn/hsearch =====================
class MohrssHSearch:
    BASE = "https://www.mohrss.gov.cn"
    PATH = "/hsearch/"

    def __init__(self, session: requests.Session, q: str, delay: float = 1.0):
        self.session = session
        self.q = q
        self.delay = delay

    def _fetch_page(self, page: int) -> str:
        params = {"searchword": self.q}
        if page > 1:
            params["page"] = page  # è‹¥ç«™ç‚¹ç”¨å…¶ä»–åˆ†é¡µåä¹Ÿèƒ½å…¼å®¹â€œä¸‹ä¸€é¡µâ€æŠ“å–
        url = self.BASE + self.PATH + "?" + urlencode(params)
        r = self.session.get(url, timeout=20)
        r.encoding = r.apparent_encoding or "utf-8"
        time.sleep(self.delay)
        return r.text

    def parse_list(self, html: str) -> Tuple[List[Item], Optional[str]]:
        soup = BeautifulSoup(html, "html.parser")
        nodes = []
        for sel in ["ul.search-list li", "div.search-list li", "div.list li", "ul li", "div.result", "div.row"]:
            tmp = soup.select(sel)
            if tmp:
                nodes = tmp
                break
        if not nodes:
            nodes = soup.select("a")  # å…œåº•

        items: List[Item] = []
        for node in nodes:
            a = node if node.name == "a" else node.find("a")
            if not a or not a.get("href"):
                continue
            title = a.get_text(" ", strip=True)
            href = a.get("href").strip()
            url = urljoin(self.BASE, href)

            # å†…å®¹/æ‘˜è¦
            abs_el = None
            for sel in [".summary", ".abs", ".intro", "p"]:
                abs_el = node.select_one(sel)
                if abs_el:
                    break
            content = abs_el.get_text(" ", strip=True) if abs_el else ""

            # æ—¶é—´
            ttxt = ""
            for sel in [".date", ".time", ".pubtime", ".f-date", ".info time", ".post-time"]:
                sub = node.select_one(sel)
                if sub:
                    ttxt = sub.get_text(" ", strip=True)
                    break
            if not ttxt:
                ttxt = node.get_text(" ", strip=True)
            dt = parse_dt(ttxt)

            items.append(Item(title=title, url=url, dt=dt, content=content, source="äººç¤¾éƒ¨ç«™å†…æœç´¢"))

        # ä¸‹ä¸€é¡µ
        next_link = None
        for a in soup.select("a"):
            txt = a.get_text(strip=True)
            if txt in ("ä¸‹ä¸€é¡µ", "ä¸‹é¡µ", "â€º", ">") or a.get("rel") == ["next"]:
                href = a.get("href") or ""
                if href and href != "javascript:;" and not href.startswith("#"):
                    next_link = urljoin(self.BASE, href)
                    break

        return items, next_link

    def run(self, max_pages: int) -> List[Item]:
        all_items: List[Item] = []
        next_url = None
        for p in range(1, max_pages + 1):
            if p == 1 or not next_url:
                html = self._fetch_page(p)
            else:
                r = self.session.get(next_url, timeout=20)
                r.encoding = r.apparent_encoding or "utf-8"
                time.sleep(self.delay)
                html = r.text
            items, next_url = self.parse_list(html)
            if not items and p == 1:
                break
            all_items.extend(items)
            if not next_url:
                break
        return all_items


# ===================== ç«™ç‚¹ 2ï¼šjob.mohrss.gov.cn/zxss =====================
class JobMohrssSearch:
    BASE = "http://job.mohrss.gov.cn"
    PATH = "/zxss/index.jhtml"

    def __init__(self, session: requests.Session, q: str, delay: float = 1.0):
        self.session = session
        self.q = q
        self.delay = delay

    def _fetch_page(self, page: int, last_next: Optional[str]) -> str:
        # ä¼˜å…ˆä½¿ç”¨â€œä¸‹ä¸€é¡µâ€é“¾æ¥ï¼ˆé€‚åº”ç«™ç‚¹çœŸå®åˆ†é¡µå‚æ•°ï¼‰ï¼Œå¦åˆ™å›é€€ textfield + pageNo
        if last_next:
            url = last_next
        else:
            params = {"textfield": self.q}
            if page > 1:
                params["pageNo"] = page  # å¸¸è§åˆ†é¡µå
            url = self.BASE + self.PATH + "?" + urlencode(params)
        r = self.session.get(url, timeout=20)
        r.encoding = r.apparent_encoding or "utf-8"
        time.sleep(self.delay)
        return r.text

    def parse_list(self, html: str) -> Tuple[List[Item], Optional[str]]:
        soup = BeautifulSoup(html, "html.parser")
        nodes = []
        for sel in [
            ".list li", ".news-list li", ".content-list li", ".box-list li",
            "ul.list li", "ul.news li", "ul li", "li"
        ]:
            tmp = soup.select(sel)
            if tmp:
                nodes = tmp
                break
        if not nodes:
            nodes = soup.select("a")  # å…œåº•

        items: List[Item] = []
        for node in nodes:
            a = node if node.name == "a" else node.find("a")
            if not a or not a.get("href"):
                continue
            title = a.get_text(" ", strip=True)
            href = a.get("href").strip()
            url = urljoin(self.BASE, href)

            # è¿‡æ»¤éæœ¬ç«™ã€æˆ–æ— æ„ä¹‰é“¾æ¥
            host = urlparse(url).netloc.lower()
            if not host.endswith("mohrss.gov.cn"):
                continue

            # æ‘˜è¦
            abs_el = None
            for sel in [".summary", ".abs", ".intro", "p"]:
                abs_el = node.select_one(sel)
                if abs_el:
                    break
            content = abs_el.get_text(" ", strip=True) if abs_el else ""

            # æ—¶é—´
            ttxt = ""
            for sel in [".date", ".time", ".pubtime", ".f-date", ".info time", ".post-time", "em", "span"]:
                sub = node.select_one(sel)
                if sub:
                    maybe = sub.get_text(" ", strip=True)
                    if re.search(r"\d{2,4}[^\d]\d{1,2}[^\d]\d{1,2}", maybe) or re.search(r"(åˆšåˆš|åˆ†é’Ÿ|å°æ—¶å‰|ä»Šå¤©|ä»Šæ—¥)", maybe):
                        ttxt = maybe
                        break
            if not ttxt:
                ttxt = node.get_text(" ", strip=True)
            dt = parse_dt(ttxt)

            items.append(Item(title=title, url=url, dt=dt, content=content, source="å…¬å…±æ‹›è˜ç½‘æœç´¢"))

        # ä¸‹ä¸€é¡µ
        next_link = None
        for a in soup.select("a"):
            txt = a.get_text(strip=True)
            if txt in ("ä¸‹ä¸€é¡µ", "ä¸‹é¡µ", "â€º", ">") or a.get("rel") == ["next"]:
                href = a.get("href") or ""
                if href and href != "javascript:;" and not href.startswith("#"):
                    next_link = urljoin(self.BASE, href)
                    break

        return items, next_link

    def run(self, max_pages: int) -> List[Item]:
        all_items: List[Item] = []
        next_url = None
        for p in range(1, max_pages + 1):
            html = self._fetch_page(p, last_next=next_url)
            items, next_url = self.parse_list(html)
            if not items and p == 1:
                break
            all_items.extend(items)
            if not next_url:
                break
        return all_items


# ===================== æ±‡æ€»ã€è¿‡æ»¤ã€è¾“å‡º =====================
def dedup_by_url(items: List[Item]) -> List[Item]:
    seen = set()
    out: List[Item] = []
    for it in items:
        if it.url and it.url not in seen:
            seen.add(it.url)
            out.append(it)
    return out


def filter_24h(items: List[Item], window_hours: int) -> List[Item]:
    return [it for it in items if within_last_hours(it.dt, window_hours)]


def build_markdown(items: List[Item], keyword: str) -> str:
    now_dt = datetime.now(TZ)
    wd = ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][now_dt.weekday()]
    lines = [
        f"**æ—¥æœŸï¼š{now_dt.strftime('%Y-%m-%d')}ï¼ˆ{wd}ï¼‰**",
        "",
        f"**æ ‡é¢˜ï¼šæ—©å®‰èµ„è®¯ï½œäººç¤¾éƒ¨ & å…¬å…±æ‹›è˜ç½‘æœç´¢ï½œ{keyword}**",
        "",
        "**ä¸»è¦å†…å®¹**",
    ]
    if not items:
        lines.append("> æš‚æ— æ›´æ–°ã€‚")
        return "\n".join(lines)

    for i, it in enumerate(items, 1):
        dt_str = it.dt.strftime("%Y-%m-%d %H:%M") if it.dt else ""
        title_line = f"{i}. [{it.title}]({it.url})"
        if it.source:
            title_line += f"ã€€â€”ã€€*{it.source}*"
        if dt_str:
            title_line += f"ã€€`{dt_str}`"
        lines.append(title_line)
        if it.content:
            snippet = re.sub(r"\s+", " ", it.content).strip()[:120]
            lines.append(f"> {snippet}")
        lines.append("")
    return "\n".join(lines)


# ===================== ä¸»æµç¨‹ =====================
def main():
    ap = argparse.ArgumentParser(description="äººç¤¾éƒ¨ & å…¬å…±æ‹›è˜ç½‘ ç«™å†…æœç´¢ï¼ˆä»…æœ€è¿‘Nå°æ—¶ï¼‰â†’ é’‰é’‰æ¨é€")
    ap.add_argument("--q", default=os.getenv("QUERY", "äººåŠ›èµ„æº"), help="æœç´¢å…³é”®è¯ï¼ˆé»˜è®¤ï¼šäººåŠ›èµ„æºï¼›ä¹Ÿå¯ç”¨ç¯å¢ƒå˜é‡ QUERY è¦†ç›–ï¼‰")
    ap.add_argument("--pages", type=int, default=int(os.getenv("PAGES", "2")), help="æ¯ç«™æœ€å¤šç¿»é¡µæ•°ï¼ˆé»˜è®¤2ï¼Œå¯ç”¨ env PAGESï¼‰")
    ap.add_argument("--window-hours", type=int, default=int(os.getenv("WINDOW_HOURS", "24")), help="æœ€è¿‘Nå°æ—¶ï¼ˆé»˜è®¤24ï¼Œå¯ç”¨ env WINDOW_HOURSï¼‰")
    ap.add_argument("--delay", type=float, default=float(os.getenv("DELAY", "1.0")), help="æ¯æ¬¡è¯·æ±‚é—´éš”ç§’ï¼ˆé»˜è®¤1.0ï¼Œå¯ç”¨ env DELAYï¼‰")
    ap.add_argument("--limit", type=int, default=int(os.getenv("LIMIT", "20")), help="æ­£æ–‡æœ€å¤šå±•ç¤ºæ¡æ•°ï¼ˆé»˜è®¤20ï¼Œå¯ç”¨ env LIMITï¼‰")
    ap.add_argument("--no-push", action="store_true", help="åªæ‰“å°ä¸æ¨é€é’‰é’‰")
    args = ap.parse_args()

    session = make_session()

    # ç«™ç‚¹ 1
    hsearch = MohrssHSearch(session, args.q, delay=args.delay)
    a = hsearch.run(max_pages=args.pages)

    # ç«™ç‚¹ 2
    jsearch = JobMohrssSearch(session, args.q, delay=args.delay)
    b = jsearch.run(max_pages=args.pages)

    all_items = dedup_by_url(a + b)

    # ä»…æœ€è¿‘ N å°æ—¶
    all_items = filter_24h(all_items, args.window_hours)

    # æŒ‰æ—¶é—´é™åºï¼ˆè§£æä¸åˆ°æ—¶é—´çš„æ’æœ€åï¼‰
    all_items.sort(key=lambda x: x.dt or datetime(1970, 1, 1, tzinfo=TZ), reverse=True)

    # æˆªæ–­å±•ç¤ºæ¡æ•°
    show = all_items[:args.limit] if args.limit and args.limit > 0 else all_items

    print(f"âœ… åˆè®¡å€™é€‰ {len(a)+len(b)} æ¡ï¼›çª—å£å†…ï¼ˆæœ€è¿‘ {args.window_hours}hï¼‰å‘½ä¸­ {len(all_items)} æ¡ï¼›å±•ç¤º {len(show)} æ¡ã€‚")

    md = build_markdown(show, args.q)
    print("\n--- Markdown Preview ---\n")
    print(md)

    if not args.no_push:
        ok = send_dingtalk_markdown(f"æ—©å®‰èµ„è®¯ï½œéƒ¨ç½‘&æ‹›è˜ç½‘æœç´¢ï½œ{args.q}", md)
        print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥/æœªæ¨é€ âŒ")


if __name__ == "__main__":
    main()
