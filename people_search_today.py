# -*- coding: utf-8 -*-
"""
People.cn ç«™å†…æœç´¢ï¼ˆä»…å½“å¤© + ç¿»é¡µï¼‰â†’ è‡ªåŠ¨æ¨é€é’‰é’‰ï¼ˆåŠ ç­¾ï¼‰
ä¿®å¤ç‚¹ï¼š
- æ›´ç¨³çš„ç»“æœåŒºé€‰æ‹©å™¨
- å‘ä¸Š/é‚»è¿‘èŠ‚ç‚¹å›æº¯æå–æ—¥æœŸ
- æ›´å®½æ¾çš„æ—¥æœŸè¯†åˆ«
- ä¸å†å› ä¸ºç¬¬ä¸€é¡µ 0 æ¡å°±æå‰é€€å‡º

ç”¨æ³•ï¼š
  python people_search_today.py --keyword å¤–åŒ… --pages 1 --delay 120
"""
import re
import time
import csv
import json
import argparse
from urllib.parse import urlencode, urljoin, urlparse
from collections import defaultdict
from datetime import datetime
from zoneinfo import ZoneInfo

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import hmac, hashlib, base64, urllib.parse

# ====== é’‰é’‰ï¼ˆç¡¬ç¼–ç ï¼‰======
DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
DINGTALK_SECRET  = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"

def _sign_webhook(base_webhook: str, secret: str) -> str:
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    return f"{base_webhook}&timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    try:
        webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
        payload = {"msgtype":"markdown","markdown":{"title":title,"text":md_text}}
        r = requests.post(webhook, json=payload, timeout=20)
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print("DingTalk resp:", r.status_code, r.text[:200])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

# ====== HTTP ä¼šè¯ ======
def make_session():
    s = requests.Session()
    s.trust_env = False
    s.headers.update({
        "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/123.0.0.0 Safari/537.36")
    })
    retries = Retry(total=3, backoff_factor=0.6,
                    status_forcelist=(429, 500, 502, 503, 504),
                    allowed_methods=["GET"])
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    s.mount("http://", adapter); s.mount("https://", adapter)
    return s

# ====== å·¥å…· ======
DATE_PATTERNS = [
    r"(20\d{2})[-/.](\d{1,2})[-/.](\d{1,2})\s+\d{2}:\d{2}:\d{2}",
    r"(20\d{2})[-/.](\d{1,2})[-/.](\d{1,2})",
    r"(20\d{2})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",
]

def find_date_in_text(text: str) -> str:
    t = text.replace("\u3000", " ")
    for pat in DATE_PATTERNS:
        m = re.search(pat, t)
        if m:
            y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
            return f"{y:04d}-{mo:02d}-{d:02d}"
    return ""

def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())

class PeopleSearch:
    def __init__(self, keyword="å¤–åŒ…", max_pages=1, delay=120, tz="Asia/Shanghai"):
        self.keyword = keyword
        self.max_pages = max_pages
        self.tz = ZoneInfo(tz)
        self.today = datetime.now(self.tz).strftime("%Y-%m-%d")
        self.session = make_session()
        self._next_allowed_time = defaultdict(float)
        self._domain_delay = {
            "search.people.cn": delay,
            "www.people.com.cn": delay,
        }
        self.results = []
        self._seen = set()

    def _get_with_throttle(self, url, timeout=25):
        host = urlparse(url).netloc
        delay = self._domain_delay.get(host, 0)
        now = time.time()
        if delay > 0 and self._next_allowed_time[host] > now:
            time.sleep(self._next_allowed_time[host] - now)
        resp = self.session.get(url, timeout=timeout)
        if delay > 0:
            self._next_allowed_time[host] = time.time() + delay
        return resp

    def _build_url(self, page: int) -> str:
        base = "https://search.people.cn/s/"
        qs = {"keyword": self.keyword, "page": page}
        return base + "?" + urlencode(qs, doseq=True)

    def _push_if_new(self, item):
        key = item["url"]
        if key in self._seen:
            return False
        self._seen.add(key); self.results.append(item); return True

    # â€”â€”ä»æ ‡é¢˜ a å‡ºå‘ï¼Œå‘ä¸Šå›æº¯å‡ å±‚å¹¶æ£€æŸ¥å…„å¼ŸèŠ‚ç‚¹ï¼Œå°½å¯èƒ½æ‰¾â€œæ¥æº/æ—¥æœŸâ€è¡Œ
    def _locate_block_and_date(self, a_tag):
        # 1) å…ˆçœ‹ a_tag è‡ªèº«å’Œçˆ¶èŠ‚ç‚¹
        for up in [a_tag, a_tag.parent, getattr(a_tag.parent, "parent", None),
                   getattr(getattr(a_tag, "parent", None), "parent", None)]:
            if not up: continue
            txt = up.get_text(" ", strip=True)
            d = find_date_in_text(txt)
            if d: return up, d
            # å†çœ‹å…„å¼Ÿ
            for sib in list(up.previous_siblings)[:3] + list(up.next_siblings)[:3]:
                try:
                    if not hasattr(sib, "get_text"): continue
                    d2 = find_date_in_text(sib.get_text(" ", strip=True))
                    if d2: return up, d2
                except Exception:
                    pass
        return a_tag, ""

    def run(self):
        print(f"å¼€å§‹æŠ“å–ï¼šå…³é”®è¯='{self.keyword}'ï¼Œä»…å½“å¤©={self.today}ï¼Œæœ€å¤š {self.max_pages} é¡µï¼ˆæ³¨æ„ people éœ€å»¶è¿Ÿï¼‰")
        added_total = 0
        for page in range(1, self.max_pages + 1):
            url = self._build_url(page)
            try:
                resp = self._get_with_throttle(url, timeout=25)
                resp.encoding = resp.apparent_encoding or "utf-8"
                if resp.status_code != 200:
                    print(f"âš ï¸ ç¬¬{page}é¡µè®¿é—®å¤±è´¥ {resp.status_code}: {url}")
                    continue

                soup = BeautifulSoup(resp.text, "html.parser")

                # æ›´ç²¾å‡†ï¼šç»“æœåŒºä¸€èˆ¬åœ¨ .content æˆ– .search å®¹å™¨ä¸‹
                result_root = None
                for sel in ["div.content", "div.search", "div.main-container", "div.module-common"]:
                    result_root = soup.select_one(sel)
                    if result_root: break
                scan_scope = result_root or soup

                # ç»“æœæ¡ç›®ï¼ša æ ‡ç­¾ï¼ˆæ’é™¤å¯¼èˆª/ç¿»é¡µï¼‰
                anchors = []
                for sel in [
                    "div.content a",
                    "div.search a",
                    "a"
                ]:
                    anchors = scan_scope.select(sel)
                    if anchors: break

                added_page = 0
                for a in anchors:
                    href = a.get("href") or ""
                    title = norm(a.get_text())
                    if not href or not title: continue
                    # è¿‡æ»¤æ˜æ˜¾çš„æ— æ•ˆé“¾æ¥ï¼ˆç¿»é¡µé”šç‚¹ã€javascript ç­‰ï¼‰
                    if href.startswith("#") or href.startswith("javascript"): 
                        continue
                    full_url = urljoin(url, href)

                    block, d = self._locate_block_and_date(a)
                    if d != self.today:
                        continue  # åªè¦å½“å¤©

                    # æ‘˜è¦ï¼šä¼˜å…ˆæ‰¾å—å†…çš„ pï¼›æ²¡æœ‰å°±å–å—æ–‡æœ¬
                    p = block.find("p") if hasattr(block, "find") else None
                    digest = norm(p.get_text(" ", strip=True)) if p else norm(block.get_text(" ", strip=True))
                    digest = digest[:160]

                    item = {
                        "title": title,
                        "url": full_url,
                        "source": "äººæ°‘ç½‘ï¼ˆæœç´¢ï¼‰",
                        "date": d,
                        "content": digest
                    }
                    if self._push_if_new(item):
                        added_page += 1
                        print(f" + {title} | {d}")

                added_total += added_page
                print(f"ç¬¬{page}é¡µï¼šå½“å¤©å‘½ä¸­ {added_page} æ¡ã€‚")

                # ä¸å†å› ä¸ºç¬¬ä¸€é¡µ 0 æ¡å°±æå‰é€€å‡ºï¼›å¦‚æœä½ æƒ³å¿«äº›ï¼Œå¯æ”¾å¼€ä»¥ä¸‹é€»è¾‘ï¼š
                # if page == 1 and added_page == 0:
                #     break

            except Exception as e:
                print(f"âš ï¸ æŠ“å–å¼‚å¸¸ page={page}: {e}")
                continue

        print(f"å®Œæˆï¼šå…±æŠ“åˆ° {added_total} æ¡å½“å¤©ç»“æœã€‚")
        return self.results

    def save(self, fmt="both"):
        if not self.results:
            print("æ— ç»“æœå¯ä¿å­˜ã€‚"); return []
        ts = datetime.now(self.tz).strftime("%Y%m%d_%H%M%S")
        out = []
        if fmt in ("csv", "both"):
            fn = f"people_search_{ts}.csv"
            with open(fn, "w", newline="", encoding="utf-8-sig") as f:
                w = csv.DictWriter(f, fieldnames=["title","url","source","date","content"])
                w.writeheader(); w.writerows(self.results)
            out.append(fn); print("CSV:", fn)
        if fmt in ("json", "both"):
            fn = f"people_search_{ts}.json"
            with open(fn, "w", encoding="utf-8") as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            out.append(fn); print("JSON:", fn)
        return out

    def to_markdown(self, limit=12):
        if not self.results:
            return f"### äººæ°‘ç½‘æœç´¢ï¼ˆä»…å½“å¤©ï¼‰\n**å…³é”®è¯ï¼š{self.keyword}**\n**æ—¶é—´ï¼š{self.today}**\n> ä»Šå¤©æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»“æœã€‚"
        lines = [
            f"### äººæ°‘ç½‘æœç´¢ï¼ˆä»…å½“å¤©ï¼‰",
            f"**å…³é”®è¯ï¼š{self.keyword}**",
            f"**æ—¶é—´ï¼š{self.today}**",
            "",
            "#### ç»“æœ"
        ]
        for i, it in enumerate(self.results[:limit], 1):
            lines.append(f"{i}. [{it['title']}]({it['url']})")
            lines.append(f"> ğŸ“… {it['date']} | ğŸ›ï¸ {it['source']}")
            if it.get("content"): lines.append(f"> {it['content'][:120]}")
            lines.append("")
        return "\n".join(lines)

def main():
    ap = argparse.ArgumentParser(description="People.cn æœç´¢ï¼šä»…å½“å¤© + ç¿»é¡µ â†’ é’‰é’‰æ¨é€")
    ap.add_argument("--keyword", default="å¤–åŒ…", help="æœç´¢å…³é”®è¯ï¼ˆé»˜è®¤ï¼šå¤–åŒ…ï¼‰")
    ap.add_argument("--pages", type=int, default=1, help="æœ€å¤šç¿»é¡µæ•°ï¼ˆé»˜è®¤ï¼š1ï¼‰")
    ap.add_argument("--delay", type=int, default=120, help="åŒåŸŸè¯·æ±‚é—´éš”ç§’ï¼ˆé»˜è®¤ï¼š120ï¼Œéµå®ˆ robotsï¼‰")
    ap.add_argument("--tz", default="Asia/Shanghai", help="æ—¶åŒºï¼ˆé»˜è®¤ï¼šAsia/Shanghaiï¼‰")
    ap.add_argument("--save", default="both", choices=["csv","json","both","none"], help="ä¿å­˜æ ¼å¼ï¼ˆé»˜è®¤ï¼šbothï¼‰")
    args = ap.parse_args()

    spider = PeopleSearch(keyword=args.keyword, max_pages=args.pages, delay=args.delay, tz=args.tz)
    spider.run()
    if args.save != "none":
        spider.save(args.save)

    md = spider.to_markdown()
    ok = send_dingtalk_markdown(f"äººæ°‘ç½‘æœç´¢ï¼ˆ{args.keyword}ï¼‰å½“å¤©ç»“æœ", md)
    print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥ âŒ")

if __name__ == "__main__":
    main()
