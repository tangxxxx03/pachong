# -*- coding: utf-8 -*-
"""
People.cn ç«™å†…æœç´¢ï¼ˆä»…å½“å¤© + ç¿»é¡µï¼‰â†’ è‡ªåŠ¨æ¨é€é’‰é’‰ï¼ˆåŠ ç­¾ï¼‰
- æŒ‰ <li> è§£æï¼Œç›´æ¥è¯»å– .tip-pubtime æ—¥æœŸï¼Œå‘½ä¸­ç‡æ›´é«˜
- ä»…ä¿ç•™â€œä»Šå¤©â€çš„ç»“æœï¼ˆAsia/Shanghaiï¼‰
- éµå®ˆ robotsï¼ˆé»˜è®¤ï¼šsearch.people.cn / www.people.com.cn 120s/æ¬¡ï¼‰
- è¿è¡Œå®Œæˆåå°†ç»“æœä»¥ Markdown æ¨é€åˆ°é’‰é’‰ï¼ˆå·²ç¡¬ç¼–ç  webhook/secretï¼‰

ç”¨æ³•ï¼ˆå»ºè®®å…ˆè·‘ 1 é¡µéªŒè¯ï¼‰ï¼š
  python people_search_today.py --keyword å¤–åŒ… --pages 1 --delay 120
"""

import re
import time
import csv
import json
import argparse
import hmac
import hashlib
import base64
import urllib.parse
from urllib.parse import urlencode, urljoin, urlparse
from collections import defaultdict
from datetime import datetime

# å…¼å®¹ Py<3.9 çš„ zoneinfo
try:
    from zoneinfo import ZoneInfo  # Py3.9+
except Exception:  # pragma: no cover
    from backports.zoneinfo import ZoneInfo  # pip install backports.zoneinfo

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ====== é’‰é’‰ï¼ˆç¡¬ç¼–ç ï¼‰======
DINGTALK_WEBHOOK = (
    "https://oapi.dingtalk.com/robot/send?"
    "access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
)
DINGTALK_SECRET = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"


def _sign_webhook(base_webhook: str, secret: str) -> str:
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    return f"{base_webhook}&timestamp={ts}&sign={sign}"


def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    try:
        webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
        payload = {"msgtype": "markdown", "markdown": {"title": title, "text": md_text}}
        r = requests.post(webhook, json=payload, timeout=20)
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print("DingTalk resp:", r.status_code, r.text[:200])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False


# ====== HTTP ä¼šè¯ ======
def make_session():
    s = requests.Session()
    s.trust_env = False  # ä¸ç»§æ‰¿ runner çš„ä»£ç†ï¼Œé¿å…è«åå…¶å¦™çš„ 407/è¶…æ—¶
    s.headers.update({
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0.0.0 Safari/537.36"
        )
    })
    retries = Retry(
        total=3,
        backoff_factor=0.6,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=["GET"],
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s


# ====== å·¥å…· ======
DATE_PATTERNS = [
    r"(20\d{2})[-/.](\d{1,2})[-/.](\d{1,2})\s+\d{2}:\d{2}:\d{2}",
    r"(20\d{2})[-/.](\d{1,2})[-/.](\d{1,2})",
    r"(20\d{2})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",
]


def find_date_in_text(text: str) -> str:
    t = (text or "").replace("\u3000", " ")
    for pat in DATE_PATTERNS:
        m = re.search(pat, t)
        if m:
            y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
            return f"{y:04d}-{mo:02d}-{d:02d}"
    return ""


def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


class PeopleSearch:
    def __init__(self, keyword="å¤–åŒ…", max_pages=1, delay=120, tz="Asia/Shanghai"):
        self.keyword = keyword
        self.max_pages = max_pages
        self.tz = ZoneInfo(tz)
        self.today = datetime.now(self.tz).strftime("%Y-%m-%d")
        self.session = make_session()
        # æŒ‰åŸŸåèŠ‚æµï¼ˆéµå®ˆ robotsï¼‰
        self._next_allowed_time = defaultdict(float)
        self._domain_delay = {
            "search.people.cn": delay,
            "www.people.com.cn": delay,
        }
        self.results = []
        self._seen = set()

    def _get_with_throttle(self, url, timeout=25):
        host = urlparse(url).netloc
        delay = self._domain_delay.get(host, 0)
        now = time.time()
        next_at = self._next_allowed_time.get(host, 0.0)
        if delay > 0 and next_at > now:
            time.sleep(max(0.0, next_at - now))
        resp = self.session.get(url, timeout=timeout)
        if delay > 0:
            self._next_allowed_time[host] = time.time() + delay
        return resp

    def _build_url(self, page: int) -> str:
        base = "https://search.people.cn/s/"
        qs = {"keyword": self.keyword, "page": page}
        return base + "?" + urlencode(qs, doseq=True)

    def _push_if_new(self, item):
        key = item["url"]
        if key in self._seen:
            return False
        self._seen.add(key)
        self.results.append(item)
        return True

    def run(self):
        print(
            f"å¼€å§‹æŠ“å–ï¼šå…³é”®è¯='{self.keyword}'ï¼Œä»…å½“å¤©={self.today}ï¼Œ"
            f"æœ€å¤š {self.max_pages} é¡µï¼ˆpeople éœ€å»¶è¿Ÿï¼‰"
        )
        added_total = 0

        for page in range(1, self.max_pages + 1):
            url = self._build_url(page)
            try:
                resp = self._get_with_throttle(url, timeout=25)
                resp.encoding = resp.apparent_encoding or "utf-8"
                if resp.status_code != 200:
                    print(f"âš ï¸ ç¬¬{page}é¡µè®¿é—®å¤±è´¥ {resp.status_code}: {url}")
                    continue

                soup = BeautifulSoup(resp.text, "html.parser")

                # é”å®šç»“æœåŒºåŸŸï¼ˆé¡µé¢æ˜¯ SSR + Nuxtï¼Œé€šå¸¸è¿™äº›å®¹å™¨é‡Œæœ‰åˆ—è¡¨ï¼‰
                root = None
                for sel in ["div.article", "div.content", "div.search", "div.main-container", "div.module-common"]:
                    root = soup.select_one(sel)
                    if root:
                        break
                scope = root or soup

                # æŒ‰â€œæ¡ç›® liâ€è§£æï¼Œè¦æ±‚å…·å¤‡ .tip-pubtime æ—¥æœŸ
                items = []
                for sel in ["div.article li", "ul li", "li"]:
                    items = scope.select(sel)
                    if items:
                        break

                added_page = 0
                for li in items:
                    # è·³è¿‡åˆ†é¡µ/å¯¼èˆªç±» li
                    classes = " ".join(li.get("class") or [])
                    if "page" in classes:
                        continue

                    pub = li.select_one(".tip-pubtime")
                    a = li.select_one("a[href]")
                    if not pub or not a:
                        continue

                    d = find_date_in_text(pub.get_text(" ", strip=True))
                    if d != self.today:
                        continue  # ä»…å½“å¤©

                    title = norm(a.get_text())
                    if not title:
                        continue
                    href = (a.get("href") or "").strip()
                    if not href or href.startswith("#") or href.startswith("javascript"):
                        continue
                    full_url = urljoin(url, href)

                    # æ‘˜è¦ä¼˜å…ˆ .absï¼Œå…¶æ¬¡ç¬¬ä¸€ä¸ª <p>ï¼Œæœ€å li æ–‡æœ¬
                    abs_el = li.select_one(".abs")
                    if abs_el:
                        digest = norm(abs_el.get_text(" ", strip=True))
                    else:
                        p = li.find("p")
                        digest = norm(p.get_text(" ", strip=True)) if p else norm(li.get_text(" ", strip=True))
                    digest = digest[:160]

                    item = {
                        "title": title,
                        "url": full_url,
                        "source": "äººæ°‘ç½‘ï¼ˆæœç´¢ï¼‰",
                        "date": d,
                        "content": digest,
                    }
                    if self._push_if_new(item):
                        added_page += 1
                        print(f" + {title} | {d}")

                added_total += added_page
                print(f"ç¬¬{page}é¡µï¼šå½“å¤©å‘½ä¸­ {added_page} æ¡ã€‚")

            except Exception as e:
                print(f"âš ï¸ æŠ“å–å¼‚å¸¸ page={page}: {e}")
                continue

        print(f"å®Œæˆï¼šå…±æŠ“åˆ° {added_total} æ¡å½“å¤©ç»“æœã€‚")
        return self.results

    def save(self, fmt="both"):
        if not self.results:
            print("æ— ç»“æœå¯ä¿å­˜ã€‚")
            return []
        ts = datetime.now(self.tz).strftime("%Y%m%d_%H%M%S")
        out = []
        if fmt in ("csv", "both"):
            fn = f"people_search_{ts}.csv"
            with open(fn, "w", newline="", encoding="utf-8-sig") as f:
                w = csv.DictWriter(f, fieldnames=["title", "url", "source", "date", "content"])
                w.writeheader()
                w.writerows(self.results)
            out.append(fn)
            print("CSV:", fn)
        if fmt in ("json", "both"):
            fn = f"people_search_{ts}.json"
            with open(fn, "w", encoding="utf-8") as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            out.append(fn)
            print("JSON:", fn)
        return out

    def to_markdown(self, limit=12):
        if not self.results:
            return (
                f"### äººæ°‘ç½‘æœç´¢ï¼ˆä»…å½“å¤©ï¼‰\n"
                f"**å…³é”®è¯ï¼š{self.keyword}**\n"
                f"**æ—¶é—´ï¼š{self.today}**\n"
                f"> ä»Šå¤©æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»“æœã€‚"
            )
        lines = [
            "### äººæ°‘ç½‘æœç´¢ï¼ˆä»…å½“å¤©ï¼‰",
            f"**å…³é”®è¯ï¼š{self.keyword}**",
            f"**æ—¶é—´ï¼š{self.today}**",
            "",
            "#### ç»“æœ",
        ]
        for i, it in enumerate(self.results[:limit], 1):
            lines.append(f"{i}. [{it['title']}]({it['url']})")
            lines.append(f"> ğŸ“… {it['date']} | ğŸ›ï¸ {it['source']}")
            if it.get("content"):
                lines.append(f"> {it['content'][:120]}")
            lines.append("")
        return "\n".join(lines)


def main():
    ap = argparse.ArgumentParser(description="People.cn æœç´¢ï¼šä»…å½“å¤© + ç¿»é¡µ â†’ é’‰é’‰æ¨é€")
    ap.add_argument("--keyword", default="å¤–åŒ…", help="æœç´¢å…³é”®è¯ï¼ˆé»˜è®¤ï¼šå¤–åŒ…ï¼‰")
    ap.add_argument("--pages", type=int, default=1, help="æœ€å¤šç¿»é¡µæ•°ï¼ˆé»˜è®¤ï¼š1ï¼‰")
    ap.add_argument("--delay", type=int, default=120, help="åŒåŸŸè¯·æ±‚é—´éš”ç§’ï¼ˆé»˜è®¤ï¼š120ï¼Œéµå®ˆ robotsï¼‰")
    ap.add_argument("--tz", default="Asia/Shanghai", help="æ—¶åŒºï¼ˆé»˜è®¤ï¼šAsia/Shanghaiï¼‰")
    ap.add_argument("--save", default="both", choices=["csv", "json", "both", "none"], help="ä¿å­˜æ ¼å¼ï¼ˆé»˜è®¤ï¼šbothï¼‰")
    args = ap.parse_args()

    spider = PeopleSearch(keyword=args.keyword, max_pages=args.pages, delay=args.delay, tz=args.tz)
    spider.run()
    if args.save != "none":
        spider.save(args.save)

    md = spider.to_markdown()
    ok = send_dingtalk_markdown(f"äººæ°‘ç½‘æœç´¢ï¼ˆ{args.keyword}ï¼‰å½“å¤©ç»“æœ", md)
    print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥ âŒ")


if __name__ == "__main__":
    main()
