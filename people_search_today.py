# -*- coding: utf-8 -*-
"""
People.cn ç«™å†…æœç´¢ï¼ˆä»…å½“å¤© + ç¿»é¡µï¼‰â†’ è‡ªåŠ¨æ¨é€é’‰é’‰ï¼ˆåŠ ç­¾ï¼‰

- å…³é”®è¯é»˜è®¤ï¼šå¤–åŒ…ï¼ˆ--keyword å¯æ”¹ï¼‰
- ç¿»é¡µé»˜è®¤ï¼š1 é¡µï¼ˆ--pages å¯æ”¹ï¼›éµå®ˆ robots å»ºè®® 120s/æ¬¡ï¼Œç¿»å¤šé¡µä¼šæ…¢ï¼‰
- ä»…å½“å¤©ï¼ˆAsia/Shanghaiï¼‰
- å‘é€é’‰é’‰ï¼šç¡¬ç¼–ç  webhook/secretï¼ˆä½ æä¾›çš„é‚£ç»„ï¼‰

ä¾èµ–ï¼šrequests, beautifulsoup4, urllib3
ç”¨æ³•ï¼š
  python people_search_today.py --keyword å¤–åŒ… --pages 1 --delay 120
"""

import re
import time
import csv
import json
import argparse
from urllib.parse import urlencode, urljoin, urlparse
from collections import defaultdict
from datetime import datetime
from zoneinfo import ZoneInfo

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import hmac, hashlib, base64, urllib.parse

# ============ é’‰é’‰æœºå™¨äººï¼ˆå·²ç¡¬ç¼–ç ï¼‰ ============
DINGTALK_WEBHOOK = "https://oapi.dingtalk.com/robot/send?access_token=0d9943129de109072430567e03689e8c7d9012ec160e023cfa94cf6cdc703e49"
DINGTALK_SECRET  = "SEC820601d706f1894100cbfc500114a1c0977a62cfe72f9ea2b5ac2909781753d0"

def _sign_webhook(base_webhook: str, secret: str) -> str:
    ts = str(round(time.time() * 1000))
    string_to_sign = f"{ts}\n{secret}".encode("utf-8")
    hmac_code = hmac.new(secret.encode("utf-8"), string_to_sign, digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    return f"{base_webhook}&timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title: str, md_text: str) -> bool:
    try:
        webhook = _sign_webhook(DINGTALK_WEBHOOK, DINGTALK_SECRET)
        payload = {"msgtype":"markdown","markdown":{"title":title,"text":md_text}}
        r = requests.post(webhook, json=payload, timeout=20)
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print("DingTalk resp:", r.status_code, r.text[:200])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

# ============ HTTP ä¼šè¯ ============
def make_session():
    s = requests.Session()
    s.trust_env = False
    s.headers.update({
        "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/123.0.0.0 Safari/537.36")
    })
    retries = Retry(total=3, backoff_factor=0.6,
                    status_forcelist=(429, 500, 502, 503, 504),
                    allowed_methods=["GET"])
    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=20)
    s.mount("http://", adapter); s.mount("https://", adapter)
    return s

class PeopleSearch:
    def __init__(self, keyword="å¤–åŒ…", max_pages=1, delay=120, tz="Asia/Shanghai"):
        self.keyword = keyword
        self.max_pages = max_pages
        self.tz = ZoneInfo(tz)
        self.today = datetime.now(self.tz).strftime("%Y-%m-%d")
        self.session = make_session()
        # people åŸŸåæŒ‰ robots èŠ‚æµ
        self._next_allowed_time = defaultdict(float)
        self._domain_delay = {
            "search.people.cn": delay,
            "www.people.com.cn": delay,
        }
        self.results = []
        self._seen = set()

    def _get_with_throttle(self, url, timeout=25):
        host = urlparse(url).netloc
        delay = self._domain_delay.get(host, 0)
        now = time.time()
        if delay > 0 and self._next_allowed_time[host] > now:
            time.sleep(self._next_allowed_time[host] - now)
        resp = self.session.get(url, timeout=timeout)
        if delay > 0:
            self._next_allowed_time[host] = time.time() + delay
        return resp

    def _build_url(self, page: int) -> str:
        base = "https://search.people.cn/s/"
        qs = {"keyword": self.keyword, "page": page}
        return base + "?" + urlencode(qs, doseq=True)

    def _push_if_new(self, item):
        key = item["url"]
        if key in self._seen:
            return False
        self._seen.add(key); self.results.append(item); return True

    @staticmethod
    def _extract_date(text: str) -> str:
        m = re.search(r"(20\d{2})[-/.](\d{1,2})[-/.](\d{1,2})\s+\d{2}:\d{2}:\d{2}", text)
        if m:
            return f"{int(m.group(1)):04d}-{int(m.group(2)):02d}-{int(m.group(3)):02d}"
        m2 = re.search(r"(20\d{2})[-/.](\d{1,2})[-/.](\d{1,2})", text)
        if m2:
            return f"{int(m2.group(1)):04d}-{int(m2.group(2)):02d}-{int(m2.group(3)):02d}"
        return ""

    def run(self):
        print(f"å¼€å§‹æŠ“å–ï¼šå…³é”®è¯='{self.keyword}'ï¼Œä»…å½“å¤©={self.today}ï¼Œæœ€å¤š {self.max_pages} é¡µ")
        added_total = 0
        for page in range(1, self.max_pages + 1):
            url = self._build_url(page)
            try:
                resp = self._get_with_throttle(url, timeout=25)
                resp.encoding = resp.apparent_encoding or "utf-8"
                if resp.status_code != 200:
                    print(f"âš ï¸ ç¬¬{page}é¡µè®¿é—®å¤±è´¥ {resp.status_code}: {url}")
                    break

                soup = BeautifulSoup(resp.text, "html.parser")

                candidates = []
                for css in [
                    "div.content div[class*='news']",
                    "div.content div[class*='result']",
                    "div.content div",
                    "div.search div",
                    "div.article div"
                ]:
                    candidates = soup.select(css)
                    if candidates: break
                if not candidates:
                    candidates = soup.select("div")

                added_page = 0
                for node in candidates:
                    a = node.find("a", href=True)
                    if not a: continue
                    title = re.sub(r"\s+", " ", a.get_text(strip=True))
                    if not title: continue
                    href = a["href"].strip()
                    full_url = urljoin(url, href)

                    raw = node.get_text(" ", strip=True)
                    d = self._extract_date(raw)
                    if d != self.today:
                        continue

                    p = node.find("p")
                    digest = ""
                    if p:
                        digest = re.sub(r"\s+", " ", p.get_text(" ", strip=True))
                    if not digest:
                        digest = re.sub(r"\s+", " ", raw)[:160]

                    item = {
                        "title": title,
                        "url": full_url,
                        "source": "äººæ°‘ç½‘ï¼ˆæœç´¢ï¼‰",
                        "date": d,
                        "content": digest
                    }
                    if self._push_if_new(item):
                        added_page += 1
                        print(f" + {title} | {d}")

                added_total += added_page
                if page == 1 and added_page == 0:
                    print("ç¬¬ä¸€é¡µæœªæ‰¾åˆ°å½“å¤©ç»“æœï¼Œæå‰ç»“æŸã€‚")
                    break

            except Exception as e:
                print(f"âš ï¸ æŠ“å–å¼‚å¸¸ page={page}: {e}")
                break

        print(f"å®Œæˆï¼šå…±æŠ“åˆ° {added_total} æ¡å½“å¤©ç»“æœã€‚")
        return self.results

    def save(self, fmt="both"):
        if not self.results:
            print("æ— ç»“æœå¯ä¿å­˜ã€‚"); return []
        ts = datetime.now(self.tz).strftime("%Y%m%d_%H%M%S")
        out = []
        if fmt in ("csv", "both"):
            fn = f"people_search_{ts}.csv"
            with open(fn, "w", newline="", encoding="utf-8-sig") as f:
                w = csv.DictWriter(f, fieldnames=["title","url","source","date","content"])
                w.writeheader(); w.writerows(self.results)
            out.append(fn); print("CSV:", fn)
        if fmt in ("json", "both"):
            fn = f"people_search_{ts}.json"
            with open(fn, "w", encoding="utf-8") as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            out.append(fn); print("JSON:", fn)
        return out

    def to_markdown(self, limit=12):
        if not self.results:
            return f"### äººæ°‘ç½‘æœç´¢ï¼ˆä»…å½“å¤©ï¼‰\n**å…³é”®è¯ï¼š{self.keyword}**\n**æ—¶é—´ï¼š{self.today}**\n> ä»Šå¤©æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»“æœã€‚"
        lines = [
            f"### äººæ°‘ç½‘æœç´¢ï¼ˆä»…å½“å¤©ï¼‰",
            f"**å…³é”®è¯ï¼š{self.keyword}**",
            f"**æ—¶é—´ï¼š{self.today}**",
            "",
            "#### ç»“æœ"
        ]
        for i, it in enumerate(self.results[:limit], 1):
            lines.append(f"{i}. [{it['title']}]({it['url']})")
            lines.append(f"> ğŸ“… {it['date']} | ğŸ›ï¸ {it['source']}")
            if it.get("content"): lines.append(f"> {it['content'][:120]}")
            lines.append("")
        return "\n".join(lines)

def main():
    ap = argparse.ArgumentParser(description="People.cn æœç´¢ï¼šä»…å½“å¤© + ç¿»é¡µ â†’ é’‰é’‰æ¨é€")
    ap.add_argument("--keyword", default="å¤–åŒ…", help="æœç´¢å…³é”®è¯ï¼ˆé»˜è®¤ï¼šå¤–åŒ…ï¼‰")
    ap.add_argument("--pages", type=int, default=1, help="æœ€å¤šç¿»é¡µæ•°ï¼ˆé»˜è®¤ï¼š1ï¼‰")
    ap.add_argument("--delay", type=int, default=120, help="åŒåŸŸè¯·æ±‚é—´éš”ç§’ï¼ˆé»˜è®¤ï¼š120ï¼Œéµå®ˆ robotsï¼‰")
    ap.add_argument("--tz", default="Asia/Shanghai", help="æ—¶åŒºï¼ˆé»˜è®¤ï¼šAsia/Shanghaiï¼‰")
    ap.add_argument("--save", default="both", choices=["csv","json","both","none"], help="ä¿å­˜æ ¼å¼ï¼ˆé»˜è®¤ï¼šbothï¼‰")
    args = ap.parse_args()

    spider = PeopleSearch(keyword=args.keyword, max_pages=args.pages, delay=args.delay, tz=args.tz)
    spider.run()
    if args.save != "none":
        spider.save(args.save)

    md = spider.to_markdown()
    ok = send_dingtalk_markdown(f"äººæ°‘ç½‘æœç´¢ï¼ˆ{args.keyword}ï¼‰å½“å¤©ç»“æœ", md)
    print("é’‰é’‰æ¨é€ï¼š", "æˆåŠŸ âœ…" if ok else "å¤±è´¥ âŒ")

if __name__ == "__main__":
    main()
