# -*- coding: utf-8 -*-

import os, re, time, hmac, ssl, base64, hashlib, urllib.parse, requests
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin
from datetime import datetime, date
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

try:
    from zoneinfo import ZoneInfo
except:
    from backports.zoneinfo import ZoneInfo

def _tz(): return ZoneInfo("Asia/Shanghai")
def now_tz(): return datetime.now(_tz())
def norm(s): return re.sub(r"\s+", " ", (s or "").strip())
def zh_weekday(dt): return ["å‘¨ä¸€","å‘¨äºŒ","å‘¨ä¸‰","å‘¨å››","å‘¨äº”","å‘¨å…­","å‘¨æ—¥"][dt.weekday()]

# ----------------------------------------
# å›ºå®šé’‰é’‰ Webhook & Secretï¼ˆä½ ç»™æˆ‘çš„ç‰ˆæœ¬ï¼‰
# ----------------------------------------
DINGTALK_BASE = "https://oapi.dingtalk.com/robot/send?access_token=00c49f5d9aab4b8c86d60ef9bc0a25d46d9669b1b1d94645671062c4b845dced"
DINGTALK_SECRET = "SEC2431e95f7bca3b419185a0fbd80530829c45c94977ba338022400433f064c6ad"

def _sign_webhook(base, secret):
    if not base: return ""
    if not secret: return base
    ts = str(round(time.time() * 1000))
    s = f"{ts}\n{secret}".encode("utf-8")
    sign = urllib.parse.quote_plus(
        base64.b64encode(hmac.new(secret.encode("utf-8"), s, hashlib.sha256).digest())
    )
    sep = "&" if "?" in base else "?"
    return f"{base}{sep}timestamp={ts}&sign={sign}"

def send_dingtalk_markdown(title, md):
    base = DINGTALK_BASE
    secret = DINGTALK_SECRET
    if not base:
        print("ğŸ”• æœªé…ç½® webhookï¼Œè·³è¿‡æ¨é€ã€‚")
        return False
    try:
        r = requests.post(
            _sign_webhook(base, secret),
            json={"msgtype":"markdown","markdown":{"title":title,"text":md}},
            timeout=20
        )
        ok = (r.status_code == 200 and r.json().get("errcode") == 0)
        print(f"DingTalk push={ok} code={r.status_code}")
        if not ok:
            print("resp:", r.text[:300])
        return ok
    except Exception as e:
        print("DingTalk error:", e)
        return False

class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *a, **kw):
        ctx = ssl.create_default_context()
        if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
            ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
        kw["ssl_context"] = ctx
        return super().init_poolmanager(*a, **kw)

def make_session():
    s = requests.Session()
    s.headers.update({
        "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
        "Accept-Language":"zh-CN,zh;q=0.9"
    })
    r = Retry(total=3, backoff_factor=0.6, status_forcelist=[500,502,503,504])
    s.mount("https://", LegacyTLSAdapter(max_retries=r))
    return s

CN_TITLE_DATE = re.compile(r"[ï¼ˆ(]\s*(20\d{2})\s*[å¹´\-/.]\s*(\d{1,2})\s*[æœˆ\-/.]\s*(\d{1,2})\s*[)ï¼‰]")
def date_from_bracket_title(text:str):
    m = CN_TITLE_DATE.search(text or "")
    if not m: return None
    try:
        y, mo, d = int(m[1]), int(m[2]), int(m[3])
        return date(y, mo, d)
    except:
        return None

def looks_like_numbered(text: str) -> bool:
    # 1ã€xxx  1.xxx  ï¼ˆ1ï¼‰xxx  1ï¼‰xxx  ç­‰
    return bool(re.match(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*\S+", text or ""))

CIRCLED = "â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©"
def strip_leading_num(t: str) -> str:
    t = re.sub(r"^\s*[ï¼ˆ(]?\s*\d{1,2}\s*[)ï¼‰]?\s*[ã€.ï¼]\s*", "", t)
    t = re.sub(r"^\s*[" + CIRCLED + r"]\s*", "", t)
    t = re.sub(r"^\s*[ï¼-ï¼™]+\s*[ã€.ï¼]\s*", "", t)
    return t.strip()

class HRLooCrawler:
    def __init__(self):
        self.session = make_session()
        self.results = []
        self.max_items = 1

        t = (os.getenv("HR_TARGET_DATE") or "").strip()
        if t:
            try:
                y,m,d = map(int, re.split(r"[-/\.]", t))
                self.target_date = date(y,m,d)
            except:
                print("âš ï¸ HR_TARGET_DATE è§£æå¤±è´¥ï¼Œä½¿ç”¨ä»Šæ—¥ã€‚")
                self.target_date = now_tz().date()
        else:
            self.target_date = now_tz().date()

        self.daily_title_pat = re.compile(r"ä¸‰èŒ…æ—¥[æŠ¥å ±]")
        self.sources = [u.strip() for u in os.getenv(
            "SRC_HRLOO_URLS",
            "https://www.hrloo.com/,https://www.hrloo.com/news/hr"
        ).split(",") if u.strip()]

        print(f"[CFG] target_date={self.target_date} {zh_weekday(now_tz())}  sources={self.sources}")

    def crawl(self):
        for base in self.sources:
            if self._crawl_source(base):
                break

    def _crawl_source(self, base):
        try:
            r = self.session.get(base, timeout=20)
        except Exception as e:
            print("é¦–é¡µè¯·æ±‚å¼‚å¸¸ï¼š", base, e)
            return False

        if r.status_code != 200:
            print("é¦–é¡µè¯·æ±‚å¤±è´¥ï¼š", base, r.status_code)
            return False

        soup = BeautifulSoup(r.text, "html.parser")

        # é€šé“ 1ï¼šåˆ—è¡¨å®¹å™¨ï¼ˆè€æ ·å¼å¯èƒ½è¿˜åœ¨ï¼‰
        items = soup.select("div.dwxfd-list-items div.dwxfd-list-content-left")
        if items:
            for div in items:
                dts = (div.get("dwdata-time") or "").strip()
                if dts:
                    try:
                        pub_d = datetime.strptime(dts.split()[0], "%Y-%m-%d").date()
                        if pub_d != self.target_date:
                            continue
                    except:
                        pass

                a = div.find("a", href=True)
                if not a:
                    continue

                title_text = norm(a.get_text())
                if not self.daily_title_pat.search(title_text):
                    continue

                t2 = date_from_bracket_title(title_text)
                if t2 and t2 != self.target_date:
                    continue

                abs_url = urljoin(base, a["href"])
                if self._try_detail(abs_url):
                    return True

            print("[MISS] å®¹å™¨é€šé“æœªå‘½ä¸­ï¼š", base)

        # é€šé“ 2ï¼šå…œåº•æ‰« linksï¼ˆ/news/æ•°å­—.html ä¸”æ ‡é¢˜å«ä¸‰èŒ…æ—¥æŠ¥ï¼‰
        links = []
        for a in soup.select("a[href*='/news/']"):
            href = a.get("href","")
            if not re.search(r"/news/\d+\.html$", href):
                continue
            text = norm(a.get_text())
            if not self.daily_title_pat.search(text):
                continue

            t2 = date_from_bracket_title(text)
            if t2 and t2 != self.target_date:
                continue

            links.append(urljoin(base, href))

        seen = set()
        for url in links:
            if url in seen:
                continue
            seen.add(url)
            if self._try_detail(url):
                return True

        print("[MISS] æœ¬æºæœªå‘½ä¸­ç›®æ ‡æ—¥æœŸï¼š", base)
        return False

    def _try_detail(self, abs_url):
        pub_dt, titles, page_title = self._fetch_detail_clean(abs_url)

        if not page_title or not self.daily_title_pat.search(page_title):
            return False

        t3 = date_from_bracket_title(page_title)
        if t3 and t3 != self.target_date:
            return False

        # æ²¡æ‹¬å·æ—¥æœŸæ—¶ï¼Œç”¨å‘å¸ƒæ—¶é—´å…œåº•æ ¡éªŒ
        if pub_dt and pub_dt.date() != self.target_date and not t3:
            return False

        if not titles:
            return False

        self.results.append({
            "title": page_title,
            "url": abs_url,
            "date": (pub_dt.strftime("%Y-%m-%d %H:%M") if pub_dt else f"{self.target_date} 09:00"),
            "titles": titles
        })
        print(f"[HIT] {abs_url} -> {len(titles)} æ¡")
        return True

    def _extract_pub_time(self, soup: BeautifulSoup):
        cand = []
        for t in soup.select("time[datetime]"):
            cand.append(t.get("datetime",""))
        for m in soup.select("meta[property='article:published_time'],meta[name='pubdate'],meta[name='publishdate']"):
            cand.append(m.get("content",""))
        for sel in [".time",".date",".pubtime",".publish-time",".post-time",".info","meta[itemprop='datePublished']"]:
            for x in soup.select(sel):
                if isinstance(x, Tag):
                    cand.append(x.get_text(" ", strip=True))

        pat = re.compile(r"(20\d{2})[./\-å¹´](\d{1,2})[./\-æœˆ](\d{1,2})(?:\D+(\d{1,2}):(\d{1,2}))?")
        def parse_one(s):
            m = pat.search(s or "")
            if not m:
                return None
            try:
                y,mo,d = int(m[1]),int(m[2]),int(m[3])
                hh = int(m[4]) if m[4] else 9
                mm = int(m[5]) if m[5] else 0
                return datetime(y,mo,d,hh,mm,tzinfo=_tz())
            except:
                return None

        dts = [dt for dt in map(parse_one, cand) if dt]
        if dts:
            now = now_tz()
            past = [dt for dt in dts if dt <= now]
            return min(past or dts, key=lambda dt: abs((now - dt).total_seconds()))
        return None

    # ========== âœ… æ–°ç‰ˆè¦ç‚¹æå–ï¼šä¼˜å…ˆ h2.style-h2 ==========
    def _extract_h2_titles(self, root: Tag):
        """
        æ–°ç‰ˆè¯¦æƒ…é¡µçš„è¦ç‚¹æ ‡é¢˜æ˜¯ï¼š
        <h2 class="... style-h2">1ã€xxx</h2>
        """
        out = []
        for h2 in root.select("h2.style-h2, h2[class*='style-h2']"):
            text = norm(h2.get_text())
            if not text:
                continue
            text = strip_leading_num(text)
            # å¸¸è§ï¼šæ ‡é¢˜åé¢å¯èƒ½å¸¦æ‹¬å·è¡¥å……è¯´æ˜ï¼ŒæŒ‰ä½ ä¹‹å‰é€»è¾‘ç æ‰
            text = re.split(r"[ï¼ˆ(]", text)[0].strip()
            if text and len(text) >= 4:
                out.append(text)

        # å»é‡ä¿åº
        seen, final = set(), []
        for t in out:
            if t in seen:
                continue
            seen.add(t)
            final.append(t)
        return final

    def _extract_strong_titles(self, root: Tag):
        keep = []
        for st in root.select("strong"):
            text = norm(st.get_text())
            if not text:
                continue
            if len(text) < 4:
                continue
            text = re.split(r"[ï¼ˆ(]?(é˜…è¯»|é˜…è¯»é‡|æµè§ˆ|æ¥æº)[:ï¼š]\s*\d+.*$", text)[0].strip()
            if not text:
                continue
            text = strip_leading_num(text)
            if text:
                keep.append(text)

        seen, out = set(), []
        for t in keep:
            if t in seen:
                continue
            seen.add(t)
            out.append(t)
        return out

    def _extract_numbered_titles(self, root: Tag):
        out = []
        for p in root.find_all(["p","h2","h3","div","span","li"]):
            text = norm(p.get_text())
            if looks_like_numbered(text):
                text = strip_leading_num(text)
                text = re.split(r"[ï¼ˆ(]", text)[0].strip()
                if text and len(text) >= 4:
                    out.append(text)

        seen, final = set(), []
        for t in out:
            if t in seen:
                continue
            seen.add(t)
            final.append(t)
        return final

    def _pick_container(self, soup: BeautifulSoup):
        # è¯¦æƒ…æ­£æ–‡å®¹å™¨ï¼ˆæŒ‰ä½ æˆªå›¾çš„ class åšå¤šå€™é€‰ï¼‰
        selectors = [
            ".content-con.fn-wenda-detail-infomation",
            ".fn-wenda-detail-infomation",
            ".content-con.hr-rich-text.fn-wenda-detail-infomation",
            ".hr-rich-text.fn-wenda-detail-infomation",
            ".fn-hr-rich-text.custom-style-warp",
            ".custom-style-warp",
            ".content-wrap-con",  # å…œåº•
        ]
        for sel in selectors:
            node = soup.select_one(sel)
            if node:
                return node
        return soup

    def _fetch_detail_clean(self, url):
        try:
            r = self.session.get(url, timeout=(6, 20))
            if r.status_code != 200:
                print("[DetailFail]", url, r.status_code)
                return None, [], ""

            r.encoding = r.apparent_encoding or "utf-8"
            soup = BeautifulSoup(r.text, "html.parser")

            # âœ… æ ‡é¢˜ï¼šä¼˜å…ˆ h1ï¼ˆé¿å…è¯¯æ‹¿åˆ° h2 è¦ç‚¹ï¼‰
            h1 = soup.find("h1")
            if h1:
                page_title = norm(h1.get_text())
            else:
                title_tag = soup.find(["h1","h2"])
                page_title = norm(title_tag.get_text()) if title_tag else ""

            pub_dt = self._extract_pub_time(soup)

            container = self._pick_container(soup)

            # æ¸…ç†æ— å…³åŒºå—ï¼ˆå°½é‡åˆ«æŠŠæ­£æ–‡ç æ‰ï¼‰
            for sel in [
                ".other-wrap",
                ".txt",
                "a.prev.fn-dataStatistics-btn",
                "a.next.fn-dataStatistics-btn",
                ".footer",
                ".bottom",
            ]:
                for bad in container.select(sel):
                    bad.decompose()

            # âœ… æ–°ç‰ˆï¼šå…ˆæŠ“ h2.style-h2
            titles = self._extract_h2_titles(container)

            # é€€å›ï¼šstrong
            if not titles:
                titles = self._extract_strong_titles(container)

            # å†é€€å›ï¼šç¼–å·æ®µè½
            if not titles:
                titles = self._extract_numbered_titles(container)

            return pub_dt, titles, page_title

        except Exception as e:
            print("[DetailError]", url, e)
            return None, [], ""

def build_md(items):
    n = now_tz()
    out = [
        f"**æ—¥æœŸï¼š{n.strftime('%Y-%m-%d')}ï¼ˆ{zh_weekday(n)}ï¼‰**  ",
        "",
        "**æ ‡é¢˜ï¼šäººèµ„æ—¥æŠ¥ï½œæ¯æ—¥è¦ç‚¹**  ",
        ""
    ]
    if not items:
        out.append("> æœªå‘ç°å½“å¤©çš„â€œä¸‰èŒ…æ—¥æŠ¥â€ã€‚")
        return "\n".join(out)

    it = items[0]
    for idx, t in enumerate(it["titles"], 1):
        out.append(f"{idx}. {t}  ")
    out.append(f"[æŸ¥çœ‹è¯¦ç»†]({it['url']})  ")
    return "\n".join(out)

if __name__ == "__main__":
    print("æ‰§è¡Œ hr-news-huabei.pyï¼ˆå½“å¤©ä¸€æ¡ Â· ä¸‰é‡æ—¥æœŸæ ¡éªŒ Â· h2/strong/ç¼–å·æ ‡é¢˜æå–ï¼‰")
    c = HRLooCrawler()
    c.crawl()
    md = build_md(c.results)
    print("\n===== Markdown Preview =====\n")
    print(md)
    send_dingtalk_markdown("äººèµ„æ—¥æŠ¥ï½œæ¯æ—¥è¦ç‚¹", md)
